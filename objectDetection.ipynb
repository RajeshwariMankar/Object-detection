{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "objectDetection.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RajeshwariMankar/Object-detection/blob/master/objectDetection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7EnC8RZ3xTNz",
        "colab_type": "text"
      },
      "source": [
        "**Install PyDrive**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4aamiFkGE-8b",
        "colab_type": "code",
        "outputId": "313d677b-f67e-40d0-b526-2ed098bfdc68",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        }
      },
      "source": [
        "!pip install PyDrive"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting PyDrive\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/52/e0/0e64788e5dd58ce2d6934549676243dc69d982f198524be9b99e9c2a4fd5/PyDrive-1.3.1.tar.gz (987kB)\n",
            "\u001b[K     |████████████████████████████████| 993kB 3.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-api-python-client>=1.2 in /usr/local/lib/python3.6/dist-packages (from PyDrive) (1.7.9)\n",
            "Requirement already satisfied: oauth2client>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from PyDrive) (4.1.3)\n",
            "Requirement already satisfied: PyYAML>=3.0 in /usr/local/lib/python3.6/dist-packages (from PyDrive) (3.13)\n",
            "Requirement already satisfied: httplib2<1dev,>=0.9.2 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->PyDrive) (0.11.3)\n",
            "Requirement already satisfied: google-auth>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->PyDrive) (1.4.2)\n",
            "Requirement already satisfied: six<2dev,>=1.6.1 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->PyDrive) (1.12.0)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->PyDrive) (3.0.0)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->PyDrive) (0.0.3)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->PyDrive) (4.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->PyDrive) (0.2.5)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->PyDrive) (0.4.5)\n",
            "Requirement already satisfied: cachetools>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.4.1->google-api-python-client>=1.2->PyDrive) (3.1.1)\n",
            "Building wheels for collected packages: PyDrive\n",
            "  Building wheel for PyDrive (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/fa/d2/9a/d3b6b506c2da98289e5d417215ce34b696db856643bad779f4\n",
            "Successfully built PyDrive\n",
            "Installing collected packages: PyDrive\n",
            "Successfully installed PyDrive-1.3.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P5-TTLAnxpq0",
        "colab_type": "text"
      },
      "source": [
        "**Import libraries and create client**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ImbBYUSWFkVx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Authenticate and create the PyDrive client.\n",
        "# This only needs to be done once per notebook.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eBjUArwxx1Nl",
        "colab_type": "text"
      },
      "source": [
        "**Download file based on its file id**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NFaj7RYQGLtr",
        "colab_type": "code",
        "outputId": "987de14c-3457-4ba4-aa41-148434075f76",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "downloaded = drive.CreateFile({'id': '1W6DFpimMcqDflJMqUw2Z67llxygMP7h4'})\n",
        "print(downloaded['title'])\n",
        "downloaded.GetContentFile('darkflow-master.zip')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "darkflow-master.zip\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZQJmBlU8GRzQ",
        "colab_type": "code",
        "outputId": "4bf512cb-cd71-4a44-aa34-eef03430a3eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 18341
        }
      },
      "source": [
        "# Unzip the file\n",
        "!unzip darkflow-master.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  darkflow-master.zip\n",
            "   creating: darkflow-master/\n",
            "   creating: darkflow-master/ckpt/\n",
            "  inflating: darkflow-master/ckpt/checkpoint  \n",
            "  inflating: darkflow-master/ckpt/tiny-yolo-voc1-125.profile  \n",
            "  inflating: darkflow-master/ckpt/tiny-yolo-voc1-250.profile  \n",
            "  inflating: darkflow-master/ckpt/tiny-yolo-voc1-375.profile  \n",
            "  inflating: darkflow-master/ckpt/tiny-yolo-voc1-500.profile  \n",
            "  inflating: darkflow-master/ckpt/tiny-yolo-voc1-625.profile  \n",
            "  inflating: darkflow-master/ckpt/tiny-yolo-voc1-750.profile  \n",
            "  inflating: darkflow-master/ckpt/tiny-yolo-voc1-875.profile  \n",
            "  inflating: darkflow-master/ckpt/tiny-yolo-voc1-1000.profile  \n",
            "  inflating: darkflow-master/ckpt/tiny-yolo-voc1-1125.data-00000-of-00001  \n",
            "  inflating: darkflow-master/ckpt/tiny-yolo-voc1-1125.index  \n",
            "  inflating: darkflow-master/ckpt/tiny-yolo-voc1-1125.meta  \n",
            "  inflating: darkflow-master/ckpt/tiny-yolo-voc1-1125.profile  \n",
            "  inflating: darkflow-master/ckpt/tiny-yolo-voc1-1250.data-00000-of-00001  \n",
            "  inflating: darkflow-master/ckpt/tiny-yolo-voc1-1250.index  \n",
            "  inflating: darkflow-master/ckpt/tiny-yolo-voc1-1250.meta  \n",
            "  inflating: darkflow-master/ckpt/tiny-yolo-voc1-1250.profile  \n",
            "  inflating: darkflow-master/ckpt/tiny-yolo-voc1-1375.data-00000-of-00001  \n",
            "  inflating: darkflow-master/ckpt/tiny-yolo-voc1-1375.index  \n",
            "  inflating: darkflow-master/ckpt/tiny-yolo-voc1-1375.meta  \n",
            "  inflating: darkflow-master/ckpt/tiny-yolo-voc1-1375.profile  \n",
            "  inflating: darkflow-master/ckpt/tiny-yolo-voc1-1500.data-00000-of-00001  \n",
            "  inflating: darkflow-master/ckpt/tiny-yolo-voc1-1500.index  \n",
            "  inflating: darkflow-master/ckpt/tiny-yolo-voc1-1500.meta  \n",
            "  inflating: darkflow-master/ckpt/tiny-yolo-voc1-1500.profile  \n",
            "  inflating: darkflow-master/ckpt/tiny-yolo-voc1-1625.data-00000-of-00001  \n",
            "  inflating: darkflow-master/ckpt/tiny-yolo-voc1-1625.index  \n",
            "  inflating: darkflow-master/ckpt/tiny-yolo-voc1-1625.meta  \n",
            "  inflating: darkflow-master/ckpt/tiny-yolo-voc1-1625.profile  \n",
            "  inflating: darkflow-master/ckpt/tiny-yolo-voc1-1750.data-00000-of-00001  \n",
            "  inflating: darkflow-master/ckpt/tiny-yolo-voc1-1750.index  \n",
            "  inflating: darkflow-master/ckpt/tiny-yolo-voc1-1750.meta  \n",
            "  inflating: darkflow-master/ckpt/tiny-yolo-voc1-1750.profile  \n",
            "  inflating: darkflow-master/ckpt/tiny-yolo-voc1-1875.data-00000-of-00001  \n",
            "  inflating: darkflow-master/ckpt/tiny-yolo-voc1-1875.index  \n",
            "  inflating: darkflow-master/ckpt/tiny-yolo-voc1-1875.meta  \n",
            "  inflating: darkflow-master/ckpt/tiny-yolo-voc1-1875.profile  \n",
            "  inflating: darkflow-master/ckpt/tiny-yolo-voc1-2000.data-00000-of-00001  \n",
            "  inflating: darkflow-master/ckpt/tiny-yolo-voc1-2000.index  \n",
            "  inflating: darkflow-master/ckpt/tiny-yolo-voc1-2000.meta  \n",
            "  inflating: darkflow-master/ckpt/tiny-yolo-voc1-2000.profile  \n",
            "  inflating: darkflow-master/ckpt/tiny-yolo-voc1-2125.data-00000-of-00001  \n",
            "  inflating: darkflow-master/ckpt/tiny-yolo-voc1-2125.index  \n",
            "  inflating: darkflow-master/ckpt/tiny-yolo-voc1-2125.meta  \n",
            "  inflating: darkflow-master/ckpt/tiny-yolo-voc1-2125.profile  \n",
            "  inflating: darkflow-master/ckpt/tiny-yolo-voc1-2250.data-00000-of-00001  \n",
            "  inflating: darkflow-master/ckpt/tiny-yolo-voc1-2250.index  \n",
            "  inflating: darkflow-master/ckpt/tiny-yolo-voc1-2250.meta  \n",
            "  inflating: darkflow-master/ckpt/tiny-yolo-voc1-2250.profile  \n",
            "  inflating: darkflow-master/ckpt/tiny-yolo-voc1-2375.data-00000-of-00001  \n",
            "  inflating: darkflow-master/ckpt/tiny-yolo-voc1-2375.index  \n",
            "  inflating: darkflow-master/ckpt/tiny-yolo-voc1-2375.meta  \n",
            "  inflating: darkflow-master/ckpt/tiny-yolo-voc1-2375.profile  \n",
            "  inflating: darkflow-master/ckpt/tiny-yolo-voc1-2500.data-00000-of-00001  \n",
            "  inflating: darkflow-master/ckpt/tiny-yolo-voc1-2500.index  \n",
            "  inflating: darkflow-master/ckpt/tiny-yolo-voc1-2500.meta  \n",
            "  inflating: darkflow-master/ckpt/tiny-yolo-voc1-2500.profile  \n",
            "  inflating: darkflow-master/ckpt/tiny-yolo-voc1-2625.data-00000-of-00001  \n",
            "  inflating: darkflow-master/ckpt/tiny-yolo-voc1-2625.index  \n",
            "  inflating: darkflow-master/ckpt/tiny-yolo-voc1-2625.meta  \n",
            "  inflating: darkflow-master/ckpt/tiny-yolo-voc1-2625.profile  \n",
            "  inflating: darkflow-master/ckpt/tiny-yolo-voc1-2750.data-00000-of-00001  \n",
            "  inflating: darkflow-master/ckpt/tiny-yolo-voc1-2750.index  \n",
            "  inflating: darkflow-master/ckpt/tiny-yolo-voc1-2750.meta  \n",
            "  inflating: darkflow-master/ckpt/tiny-yolo-voc1-2750.profile  \n",
            "  inflating: darkflow-master/ckpt/tiny-yolo-voc1-2875.data-00000-of-00001  \n",
            "  inflating: darkflow-master/ckpt/tiny-yolo-voc1-2875.index  \n",
            "  inflating: darkflow-master/ckpt/tiny-yolo-voc1-2875.meta  \n",
            "  inflating: darkflow-master/ckpt/tiny-yolo-voc1-2875.profile  \n",
            "  inflating: darkflow-master/ckpt/tiny-yolo-voc1-3000.data-00000-of-00001  \n",
            "  inflating: darkflow-master/ckpt/tiny-yolo-voc1-3000.index  \n",
            "  inflating: darkflow-master/ckpt/tiny-yolo-voc1-3000.meta  \n",
            "  inflating: darkflow-master/ckpt/tiny-yolo-voc1-3000.profile  \n",
            "  inflating: darkflow-master/ckpt/tiny-yolo-voc1-3125.data-00000-of-00001  \n",
            "  inflating: darkflow-master/ckpt/tiny-yolo-voc1-3125.index  \n",
            "  inflating: darkflow-master/ckpt/tiny-yolo-voc1-3125.meta  \n",
            "  inflating: darkflow-master/ckpt/tiny-yolo-voc1-3125.profile  \n",
            "  inflating: darkflow-master/ckpt/tiny-yolo-voc1-3250.data-00000-of-00001  \n",
            "  inflating: darkflow-master/ckpt/tiny-yolo-voc1-3250.index  \n",
            "  inflating: darkflow-master/ckpt/tiny-yolo-voc1-3250.meta  \n",
            "  inflating: darkflow-master/ckpt/tiny-yolo-voc1-3250.profile  \n",
            "  inflating: darkflow-master/ckpt/tiny-yolo-voc1-3375.data-00000-of-00001  \n",
            "  inflating: darkflow-master/ckpt/tiny-yolo-voc1-3375.index  \n",
            "  inflating: darkflow-master/ckpt/tiny-yolo-voc1-3375.meta  \n",
            "  inflating: darkflow-master/ckpt/tiny-yolo-voc1-3375.profile  \n",
            "  inflating: darkflow-master/ckpt/tiny-yolo-voc1-3450.data-00000-of-00001  \n",
            "  inflating: darkflow-master/ckpt/tiny-yolo-voc1-3450.index  \n",
            "  inflating: darkflow-master/ckpt/tiny-yolo-voc1-3450.meta  \n",
            "  inflating: darkflow-master/ckpt/tiny-yolo-voc1-3450.profile  \n",
            "   creating: darkflow-master/train/\n",
            "   creating: darkflow-master/train/Images/\n",
            "  inflating: darkflow-master/train/Images/ankle_boot1.jpg  \n",
            "  inflating: darkflow-master/train/Images/ankle_boot2.jpg  \n",
            "  inflating: darkflow-master/train/Images/ankle_boot3.jpg  \n",
            "  inflating: darkflow-master/train/Images/ankle_boot4.jpg  \n",
            "  inflating: darkflow-master/train/Images/ankle_boot5.jpg  \n",
            "  inflating: darkflow-master/train/Images/ankle_boot6.jpg  \n",
            "  inflating: darkflow-master/train/Images/ankle_boot7.jpg  \n",
            "  inflating: darkflow-master/train/Images/ankle_boot8.jpg  \n",
            "  inflating: darkflow-master/train/Images/ankle_boot9.jpg  \n",
            "  inflating: darkflow-master/train/Images/ankle_boot10.jpg  \n",
            "  inflating: darkflow-master/train/Images/ankle_boot11.jpg  \n",
            "  inflating: darkflow-master/train/Images/ankle_boot12.jpg  \n",
            "  inflating: darkflow-master/train/Images/ankle_boot13.jpg  \n",
            "  inflating: darkflow-master/train/Images/ankle_boot14.jpg  \n",
            "  inflating: darkflow-master/train/Images/ankle_boot15.jpg  \n",
            "  inflating: darkflow-master/train/Images/ankle_boot16.jpg  \n",
            "  inflating: darkflow-master/train/Images/ankle_boot17.jpg  \n",
            "  inflating: darkflow-master/train/Images/ankle_boot18.jpg  \n",
            "  inflating: darkflow-master/train/Images/ankle_boot19.jpg  \n",
            "  inflating: darkflow-master/train/Images/ankle_boot20.jpg  \n",
            "  inflating: darkflow-master/train/Images/ankle_boot21.jpg  \n",
            "  inflating: darkflow-master/train/Images/ankle_boot22.jpg  \n",
            "  inflating: darkflow-master/train/Images/ankle_boot23.jpg  \n",
            "  inflating: darkflow-master/train/Images/ankle_boot24.jpg  \n",
            "  inflating: darkflow-master/train/Images/ankle_boot25.jpg  \n",
            "  inflating: darkflow-master/train/Images/ankle_boot26.jpg  \n",
            "  inflating: darkflow-master/train/Images/ankle_boot27.jpg  \n",
            "  inflating: darkflow-master/train/Images/ankle_boot28.jpg  \n",
            "  inflating: darkflow-master/train/Images/ankle_boot29.jpg  \n",
            "  inflating: darkflow-master/train/Images/ankle_boot30.jpg  \n",
            "  inflating: darkflow-master/train/Images/ankle_boot31.jpg  \n",
            "  inflating: darkflow-master/train/Images/ankle_boot32.jpg  \n",
            "  inflating: darkflow-master/train/Images/ankle_boot33.jpg  \n",
            "  inflating: darkflow-master/train/Images/ankle_boot34.jpg  \n",
            "  inflating: darkflow-master/train/Images/ankle_boot35.jpg  \n",
            "  inflating: darkflow-master/train/Images/ankle_boot36.jpg  \n",
            "  inflating: darkflow-master/train/Images/ankle_boot37.jpg  \n",
            "  inflating: darkflow-master/train/Images/ankle_boot38.jpg  \n",
            "  inflating: darkflow-master/train/Images/ankle_boot39.jpg  \n",
            "  inflating: darkflow-master/train/Images/ankle_boot40.jpg  \n",
            "  inflating: darkflow-master/train/Images/ankle_boot41.jpg  \n",
            "  inflating: darkflow-master/train/Images/ankle_boot42.jpg  \n",
            "  inflating: darkflow-master/train/Images/ankle_boot43.jpg  \n",
            "  inflating: darkflow-master/train/Images/ankle_boot44.jpg  \n",
            "  inflating: darkflow-master/train/Images/ankle_boot45.jpg  \n",
            "  inflating: darkflow-master/train/Images/ankle_boot46.jpg  \n",
            "  inflating: darkflow-master/train/Images/ankle_boot47.jpg  \n",
            "  inflating: darkflow-master/train/Images/ankle_boot48.jpg  \n",
            "  inflating: darkflow-master/train/Images/ankle_boot49.jpg  \n",
            "  inflating: darkflow-master/train/Images/ankle_boot50.jpg  \n",
            "  inflating: darkflow-master/train/Images/ankle_boot51.jpg  \n",
            "  inflating: darkflow-master/train/Images/ankle_boot52.jpg  \n",
            "  inflating: darkflow-master/train/Images/ankle_boot53.jpg  \n",
            "  inflating: darkflow-master/train/Images/ankle_boot54.jpg  \n",
            "  inflating: darkflow-master/train/Images/ankle_boot55.jpg  \n",
            "  inflating: darkflow-master/train/Images/ankle_boot56.jpg  \n",
            "  inflating: darkflow-master/train/Images/ankle_boot57.jpg  \n",
            "  inflating: darkflow-master/train/Images/ankle_boot58.jpg  \n",
            "  inflating: darkflow-master/train/Images/ankle_boot59.jpg  \n",
            "  inflating: darkflow-master/train/Images/ankle_boot60.jpg  \n",
            "  inflating: darkflow-master/train/Images/ankle_boot61.jpg  \n",
            "  inflating: darkflow-master/train/Images/ankle_boot62.jpg  \n",
            "  inflating: darkflow-master/train/Images/ankle_boot63.jpg  \n",
            "  inflating: darkflow-master/train/Images/ankle_boot64.jpg  \n",
            "  inflating: darkflow-master/train/Images/ankle_boot65.jpg  \n",
            "  inflating: darkflow-master/train/Images/ankle_boot66.jpg  \n",
            "  inflating: darkflow-master/train/Images/ankle_boot67.jpg  \n",
            "  inflating: darkflow-master/train/Images/ankle_boot68.jpg  \n",
            "  inflating: darkflow-master/train/Images/ankle_boot69.jpg  \n",
            "  inflating: darkflow-master/train/Images/ankle_boot70.jpg  \n",
            "  inflating: darkflow-master/train/Images/formal1.jpg  \n",
            "  inflating: darkflow-master/train/Images/formal2.jpg  \n",
            "  inflating: darkflow-master/train/Images/formal3.jpg  \n",
            "  inflating: darkflow-master/train/Images/formal4.jpg  \n",
            "  inflating: darkflow-master/train/Images/formal5.jpg  \n",
            "  inflating: darkflow-master/train/Images/formal6.jpg  \n",
            "  inflating: darkflow-master/train/Images/formal7.jpg  \n",
            "  inflating: darkflow-master/train/Images/formal8.jpg  \n",
            "  inflating: darkflow-master/train/Images/formal9.jpg  \n",
            "  inflating: darkflow-master/train/Images/formal10.jpg  \n",
            "  inflating: darkflow-master/train/Images/formal11.jpg  \n",
            "  inflating: darkflow-master/train/Images/formal12.jpg  \n",
            "  inflating: darkflow-master/train/Images/formal13.jpg  \n",
            "  inflating: darkflow-master/train/Images/formal14.jpg  \n",
            "  inflating: darkflow-master/train/Images/formal15.jpg  \n",
            "  inflating: darkflow-master/train/Images/formal16.jpg  \n",
            "  inflating: darkflow-master/train/Images/formal17.jpg  \n",
            "  inflating: darkflow-master/train/Images/formal18.jpg  \n",
            "  inflating: darkflow-master/train/Images/formal19.jpg  \n",
            "  inflating: darkflow-master/train/Images/formal20.jpg  \n",
            "  inflating: darkflow-master/train/Images/formal21.jpg  \n",
            "  inflating: darkflow-master/train/Images/formal22.jpg  \n",
            "  inflating: darkflow-master/train/Images/formal23.jpg  \n",
            "  inflating: darkflow-master/train/Images/formal24.jpg  \n",
            "  inflating: darkflow-master/train/Images/formal25.jpg  \n",
            "  inflating: darkflow-master/train/Images/formal26.jpg  \n",
            "  inflating: darkflow-master/train/Images/formal27.jpg  \n",
            "  inflating: darkflow-master/train/Images/formal28.jpg  \n",
            "  inflating: darkflow-master/train/Images/formal29.jpg  \n",
            "  inflating: darkflow-master/train/Images/formal30.jpg  \n",
            "  inflating: darkflow-master/train/Images/formal31.jpg  \n",
            "  inflating: darkflow-master/train/Images/formal32.jpg  \n",
            "  inflating: darkflow-master/train/Images/formal33.jpg  \n",
            "  inflating: darkflow-master/train/Images/formal34.jpg  \n",
            "  inflating: darkflow-master/train/Images/formal35.jpg  \n",
            "  inflating: darkflow-master/train/Images/formal36.jpg  \n",
            "  inflating: darkflow-master/train/Images/formal37.jpg  \n",
            "  inflating: darkflow-master/train/Images/formal38.jpg  \n",
            "  inflating: darkflow-master/train/Images/formal39.jpg  \n",
            "  inflating: darkflow-master/train/Images/formal40.jpg  \n",
            "  inflating: darkflow-master/train/Images/formal41.jpg  \n",
            "  inflating: darkflow-master/train/Images/formal42.jpg  \n",
            "  inflating: darkflow-master/train/Images/formal43.jpg  \n",
            "  inflating: darkflow-master/train/Images/formal44.jpg  \n",
            "  inflating: darkflow-master/train/Images/formal45.jpg  \n",
            "  inflating: darkflow-master/train/Images/formal46.jpg  \n",
            "  inflating: darkflow-master/train/Images/formal47.jpg  \n",
            "  inflating: darkflow-master/train/Images/formal48.jpg  \n",
            "  inflating: darkflow-master/train/Images/formal49.jpg  \n",
            "  inflating: darkflow-master/train/Images/formal50.jpg  \n",
            "  inflating: darkflow-master/train/Images/formal51.jpg  \n",
            "  inflating: darkflow-master/train/Images/formal52.jpg  \n",
            "  inflating: darkflow-master/train/Images/formal53.jpg  \n",
            "  inflating: darkflow-master/train/Images/formal54.jpg  \n",
            "  inflating: darkflow-master/train/Images/formal55.jpg  \n",
            "  inflating: darkflow-master/train/Images/formal56.jpg  \n",
            "  inflating: darkflow-master/train/Images/formal57.jpg  \n",
            "  inflating: darkflow-master/train/Images/formal58.jpg  \n",
            "  inflating: darkflow-master/train/Images/formal59.jpg  \n",
            "  inflating: darkflow-master/train/Images/formal60.jpg  \n",
            "  inflating: darkflow-master/train/Images/formal61.jpg  \n",
            "  inflating: darkflow-master/train/Images/formal62.jpg  \n",
            "  inflating: darkflow-master/train/Images/formal63.jpg  \n",
            "  inflating: darkflow-master/train/Images/formal64.jpg  \n",
            "  inflating: darkflow-master/train/Images/formal65.jpg  \n",
            "  inflating: darkflow-master/train/Images/formal66.jpg  \n",
            "  inflating: darkflow-master/train/Images/formal67.jpg  \n",
            "  inflating: darkflow-master/train/Images/formal68.jpg  \n",
            "  inflating: darkflow-master/train/Images/formal69.jpg  \n",
            "  inflating: darkflow-master/train/Images/formal70.jpg  \n",
            "  inflating: darkflow-master/train/Images/formal71.jpg  \n",
            "  inflating: darkflow-master/train/Images/formal72.jpg  \n",
            "  inflating: darkflow-master/train/Images/formal73.jpg  \n",
            "  inflating: darkflow-master/train/Images/formal74.jpg  \n",
            "  inflating: darkflow-master/train/Images/formal75.jpg  \n",
            "  inflating: darkflow-master/train/Images/formal76.jpg  \n",
            "  inflating: darkflow-master/train/Images/formal77.jpg  \n",
            "  inflating: darkflow-master/train/Images/formal78.jpg  \n",
            "  inflating: darkflow-master/train/Images/formal79.jpg  \n",
            "  inflating: darkflow-master/train/Images/formal80.jpg  \n",
            "  inflating: darkflow-master/train/Images/formal81.jpg  \n",
            "  inflating: darkflow-master/train/Images/formal82.jpg  \n",
            "  inflating: darkflow-master/train/Images/formal83.jpg  \n",
            "  inflating: darkflow-master/train/Images/formal84.jpg  \n",
            "  inflating: darkflow-master/train/Images/formal85.jpg  \n",
            "  inflating: darkflow-master/train/Images/formal86.jpg  \n",
            "  inflating: darkflow-master/train/Images/formal87.jpg  \n",
            "  inflating: darkflow-master/train/Images/formal88.jpg  \n",
            "  inflating: darkflow-master/train/Images/formal89.jpg  \n",
            "  inflating: darkflow-master/train/Images/formal90.jpg  \n",
            "  inflating: darkflow-master/train/Images/formal91.jpg  \n",
            "  inflating: darkflow-master/train/Images/formal92.jpg  \n",
            "  inflating: darkflow-master/train/Images/formal93.jpg  \n",
            "  inflating: darkflow-master/train/Images/formal94.jpg  \n",
            "  inflating: darkflow-master/train/Images/formal95.jpg  \n",
            "  inflating: darkflow-master/train/Images/formal96.jpg  \n",
            "  inflating: darkflow-master/train/Images/formal97.jpg  \n",
            "  inflating: darkflow-master/train/Images/formal98.jpg  \n",
            "  inflating: darkflow-master/train/Images/formal99.jpg  \n",
            "  inflating: darkflow-master/train/Images/formal100.jpg  \n",
            "  inflating: darkflow-master/train/Images/sneakers1.jpg  \n",
            "  inflating: darkflow-master/train/Images/sneakers2.jpg  \n",
            "  inflating: darkflow-master/train/Images/sneakers3.jpg  \n",
            "  inflating: darkflow-master/train/Images/sneakers4.jpg  \n",
            "  inflating: darkflow-master/train/Images/sneakers5.jpg  \n",
            "  inflating: darkflow-master/train/Images/sneakers6.jpg  \n",
            "  inflating: darkflow-master/train/Images/sneakers7.jpg  \n",
            "  inflating: darkflow-master/train/Images/sneakers8.jpg  \n",
            "  inflating: darkflow-master/train/Images/sneakers9.jpg  \n",
            "  inflating: darkflow-master/train/Images/sneakers10.jpg  \n",
            "  inflating: darkflow-master/train/Images/sneakers11.jpg  \n",
            "  inflating: darkflow-master/train/Images/sneakers12.jpg  \n",
            "  inflating: darkflow-master/train/Images/sneakers13.jpg  \n",
            "  inflating: darkflow-master/train/Images/sneakers14.jpg  \n",
            "  inflating: darkflow-master/train/Images/sneakers15.jpg  \n",
            "  inflating: darkflow-master/train/Images/sneakers16.jpg  \n",
            "  inflating: darkflow-master/train/Images/sneakers17.jpg  \n",
            "  inflating: darkflow-master/train/Images/sneakers18.jpg  \n",
            "  inflating: darkflow-master/train/Images/sneakers19.jpg  \n",
            "  inflating: darkflow-master/train/Images/sneakers20.jpg  \n",
            "  inflating: darkflow-master/train/Images/sneakers21.jpg  \n",
            "  inflating: darkflow-master/train/Images/sneakers22.jpg  \n",
            "  inflating: darkflow-master/train/Images/sneakers23.jpg  \n",
            "  inflating: darkflow-master/train/Images/sneakers24.jpg  \n",
            "  inflating: darkflow-master/train/Images/sneakers25.jpg  \n",
            "  inflating: darkflow-master/train/Images/sneakers26.jpg  \n",
            "  inflating: darkflow-master/train/Images/sneakers27.jpg  \n",
            "  inflating: darkflow-master/train/Images/sneakers28.jpg  \n",
            "  inflating: darkflow-master/train/Images/sneakers29.jpg  \n",
            "  inflating: darkflow-master/train/Images/sneakers30.jpg  \n",
            "  inflating: darkflow-master/train/Images/sneakers31.jpg  \n",
            "  inflating: darkflow-master/train/Images/sneakers32.jpg  \n",
            "  inflating: darkflow-master/train/Images/sneakers33.jpg  \n",
            "  inflating: darkflow-master/train/Images/sneakers34.jpg  \n",
            "  inflating: darkflow-master/train/Images/sneakers35.jpg  \n",
            "  inflating: darkflow-master/train/Images/sneakers36.jpg  \n",
            "  inflating: darkflow-master/train/Images/sneakers37.jpg  \n",
            "  inflating: darkflow-master/train/Images/sneakers38.jpg  \n",
            "  inflating: darkflow-master/train/Images/sneakers39.jpg  \n",
            "  inflating: darkflow-master/train/Images/sneakers40.jpg  \n",
            "  inflating: darkflow-master/train/Images/sneakers41.jpg  \n",
            "  inflating: darkflow-master/train/Images/sneakers42.jpg  \n",
            "  inflating: darkflow-master/train/Images/sneakers43.jpg  \n",
            "  inflating: darkflow-master/train/Images/sneakers44.jpg  \n",
            "  inflating: darkflow-master/train/Images/sneakers45.jpg  \n",
            "  inflating: darkflow-master/train/Images/sneakers46.jpg  \n",
            "  inflating: darkflow-master/train/Images/sneakers47.jpg  \n",
            "  inflating: darkflow-master/train/Images/sneakers48.jpg  \n",
            "  inflating: darkflow-master/train/Images/sneakers49.jpg  \n",
            "  inflating: darkflow-master/train/Images/sneakers50.jpg  \n",
            "  inflating: darkflow-master/train/Images/sneakers51.jpg  \n",
            "  inflating: darkflow-master/train/Images/sneakers52.jpg  \n",
            "  inflating: darkflow-master/train/Images/sneakers53.jpg  \n",
            "  inflating: darkflow-master/train/Images/sneakers54.jpg  \n",
            "  inflating: darkflow-master/train/Images/sneakers55.jpg  \n",
            "  inflating: darkflow-master/train/Images/sneakers56.jpg  \n",
            "  inflating: darkflow-master/train/Images/sneakers57.jpg  \n",
            "  inflating: darkflow-master/train/Images/sneakers58.jpg  \n",
            "  inflating: darkflow-master/train/Images/sneakers59.jpg  \n",
            "  inflating: darkflow-master/train/Images/sneakers60.jpg  \n",
            "  inflating: darkflow-master/train/Images/sneakers61.jpg  \n",
            "  inflating: darkflow-master/train/Images/sneakers62.jpg  \n",
            "  inflating: darkflow-master/train/Images/sneakers63.jpg  \n",
            "  inflating: darkflow-master/train/Images/sneakers64.jpg  \n",
            "  inflating: darkflow-master/train/Images/sneakers65.jpg  \n",
            "  inflating: darkflow-master/train/Images/sneakers66.jpg  \n",
            "  inflating: darkflow-master/train/Images/sneakers67.jpg  \n",
            "  inflating: darkflow-master/train/Images/sneakers68.jpg  \n",
            "  inflating: darkflow-master/train/Images/sneakers69.jpg  \n",
            "  inflating: darkflow-master/train/Images/sneakers70.jpg  \n",
            "  inflating: darkflow-master/train/Images/sneakers71.jpg  \n",
            "  inflating: darkflow-master/train/Images/sneakers72.jpg  \n",
            "  inflating: darkflow-master/train/Images/sneakers73.jpg  \n",
            "  inflating: darkflow-master/train/Images/sneakers74.jpg  \n",
            "  inflating: darkflow-master/train/Images/sneakers75.jpg  \n",
            "  inflating: darkflow-master/train/Images/sneakers76.jpg  \n",
            "  inflating: darkflow-master/train/Images/sneakers77.jpg  \n",
            "  inflating: darkflow-master/train/Images/sneakers78.jpg  \n",
            "  inflating: darkflow-master/train/Images/sneakers79.jpg  \n",
            "  inflating: darkflow-master/train/Images/sneakers80.jpg  \n",
            "  inflating: darkflow-master/train/Images/sneakers81.jpg  \n",
            "  inflating: darkflow-master/train/Images/sneakers82.jpg  \n",
            "  inflating: darkflow-master/train/Images/sneakers83.jpg  \n",
            "  inflating: darkflow-master/train/Images/sneakers84.jpg  \n",
            "  inflating: darkflow-master/train/Images/sneakers85.jpg  \n",
            "  inflating: darkflow-master/train/Images/sneakers86.jpg  \n",
            "  inflating: darkflow-master/train/Images/sneakers87.jpg  \n",
            "  inflating: darkflow-master/train/Images/sneakers88.jpg  \n",
            "  inflating: darkflow-master/train/Images/sneakers89.jpg  \n",
            "  inflating: darkflow-master/train/Images/sneakers90.jpg  \n",
            "  inflating: darkflow-master/train/Images/sneakers91.jpg  \n",
            "  inflating: darkflow-master/train/Images/sneakers92.jpg  \n",
            "  inflating: darkflow-master/train/Images/sneakers93.jpg  \n",
            "  inflating: darkflow-master/train/Images/sneakers94.jpg  \n",
            "  inflating: darkflow-master/train/Images/sneakers95.jpg  \n",
            "  inflating: darkflow-master/train/Images/sneakers96.jpg  \n",
            "  inflating: darkflow-master/train/Images/sneakers97.jpg  \n",
            "  inflating: darkflow-master/train/Images/sneakers98.jpg  \n",
            "  inflating: darkflow-master/train/Images/sneakers99.jpg  \n",
            "  inflating: darkflow-master/train/Images/sneakers100.jpg  \n",
            "  inflating: darkflow-master/train/Images/sneakers101.jpg  \n",
            "  inflating: darkflow-master/train/Images/sneakers102.jpg  \n",
            "  inflating: darkflow-master/train/Images/sneakers103.jpg  \n",
            "  inflating: darkflow-master/train/Images/sneakers104.jpg  \n",
            "  inflating: darkflow-master/train/Images/sneakers105.jpg  \n",
            "  inflating: darkflow-master/train/Images/sneakers106.jpg  \n",
            "  inflating: darkflow-master/train/Images/sneakers107.jpg  \n",
            "  inflating: darkflow-master/train/Images/sneakers108.jpg  \n",
            "  inflating: darkflow-master/train/Images/sneakers109.jpg  \n",
            "  inflating: darkflow-master/train/Images/sneakers110.jpg  \n",
            "  inflating: darkflow-master/train/Images/sneakers111.jpg  \n",
            "  inflating: darkflow-master/train/Images/sneakers112.jpg  \n",
            "  inflating: darkflow-master/train/Images/sneakers113.jpg  \n",
            "  inflating: darkflow-master/train/Images/sneakers114.jpg  \n",
            "  inflating: darkflow-master/train/Images/sneakers115.jpg  \n",
            "  inflating: darkflow-master/train/Images/sneakers116.jpg  \n",
            "  inflating: darkflow-master/train/Images/sneakers117.jpg  \n",
            "  inflating: darkflow-master/train/Images/sneakers118.jpg  \n",
            "  inflating: darkflow-master/train/Images/sneakers119.jpg  \n",
            "  inflating: darkflow-master/train/Images/sneakers120.jpg  \n",
            "  inflating: darkflow-master/train/Images/sneakers121.jpg  \n",
            "  inflating: darkflow-master/train/Images/sneakers122.jpg  \n",
            "  inflating: darkflow-master/train/Images/sneakers123.jpg  \n",
            "  inflating: darkflow-master/train/Images/sneakers124.jpg  \n",
            "  inflating: darkflow-master/train/Images/sneakers125.jpg  \n",
            "  inflating: darkflow-master/train/Images/sneakers126.jpg  \n",
            "  inflating: darkflow-master/train/Images/sneakers127.jpg  \n",
            "  inflating: darkflow-master/train/Images/sneakers128.jpg  \n",
            "  inflating: darkflow-master/train/Images/sneakers129.jpg  \n",
            "  inflating: darkflow-master/train/Images/sneakers130.jpg  \n",
            "  inflating: darkflow-master/train/Images/sneakers131.jpg  \n",
            "  inflating: darkflow-master/train/Images/sneakers132.jpg  \n",
            "  inflating: darkflow-master/train/Images/sneakers133.jpg  \n",
            "  inflating: darkflow-master/train/Images/sneakers134.jpg  \n",
            "  inflating: darkflow-master/train/Images/sneakers135.jpg  \n",
            "  inflating: darkflow-master/train/Images/sneakers136.jpg  \n",
            "  inflating: darkflow-master/train/Images/sneakers137.jpg  \n",
            "  inflating: darkflow-master/train/Images/sneakers138.jpg  \n",
            "  inflating: darkflow-master/train/Images/sneakers139.jpg  \n",
            "  inflating: darkflow-master/train/Images/sneakers140.jpg  \n",
            "  inflating: darkflow-master/train/Images/sneakers141.jpg  \n",
            "  inflating: darkflow-master/train/Images/sneakers142.jpg  \n",
            "  inflating: darkflow-master/train/Images/sneakers143.jpg  \n",
            "  inflating: darkflow-master/train/Images/sneakers144.jpg  \n",
            "  inflating: darkflow-master/train/Images/sneakers145.jpg  \n",
            "  inflating: darkflow-master/train/Images/sneakers146.jpg  \n",
            "  inflating: darkflow-master/train/Images/sneakers147.jpg  \n",
            "  inflating: darkflow-master/train/Images/sneakers148.jpg  \n",
            "  inflating: darkflow-master/train/Images/sneakers149.jpg  \n",
            "  inflating: darkflow-master/train/Images/sneakers150.jpg  \n",
            "  inflating: darkflow-master/train/Images/sneakers151.jpg  \n",
            "  inflating: darkflow-master/train/Images/sneakers152.jpg  \n",
            "  inflating: darkflow-master/train/Images/sneakers153.jpg  \n",
            "  inflating: darkflow-master/train/Images/sneakers154.jpg  \n",
            "  inflating: darkflow-master/train/Images/sneakers155.jpg  \n",
            "  inflating: darkflow-master/train/Images/sneakers156.jpg  \n",
            "  inflating: darkflow-master/train/Images/sneakers157.jpg  \n",
            "  inflating: darkflow-master/train/Images/sneakers158.jpg  \n",
            "  inflating: darkflow-master/train/Images/sneakers159.jpg  \n",
            "  inflating: darkflow-master/train/Images/sneakers160.jpg  \n",
            "  inflating: darkflow-master/train/Images/sneakers161.jpg  \n",
            "  inflating: darkflow-master/train/Images/sneakers162.jpg  \n",
            "  inflating: darkflow-master/train/Images/sneakers163.jpg  \n",
            "  inflating: darkflow-master/train/Images/sneakers164.jpg  \n",
            "  inflating: darkflow-master/train/Images/sneakers165.jpg  \n",
            "  inflating: darkflow-master/train/Images/sneakers166.jpg  \n",
            "  inflating: darkflow-master/train/Images/sneakers167.jpg  \n",
            "  inflating: darkflow-master/train/Images/sneakers168.jpg  \n",
            "  inflating: darkflow-master/train/Images/sneakers169.jpg  \n",
            "  inflating: darkflow-master/train/Images/sneakers170.jpg  \n",
            "  inflating: darkflow-master/train/Images/sneakers171.jpg  \n",
            "  inflating: darkflow-master/train/Images/sneakers172.jpg  \n",
            "  inflating: darkflow-master/train/Images/sneakers173.jpg  \n",
            "  inflating: darkflow-master/train/Images/sneakers174.jpg  \n",
            "  inflating: darkflow-master/train/Images/sneakers175.jpg  \n",
            "  inflating: darkflow-master/train/Images/sneakers176.jpg  \n",
            "  inflating: darkflow-master/train/Images/sneakers177.jpg  \n",
            "  inflating: darkflow-master/train/Images/sneakers178.jpg  \n",
            "  inflating: darkflow-master/train/Images/sneakers179.jpg  \n",
            "  inflating: darkflow-master/train/Images/sneakers180.jpg  \n",
            "  inflating: darkflow-master/train/Images/sneakers181.jpg  \n",
            "  inflating: darkflow-master/train/Images/sneakers182.jpg  \n",
            "  inflating: darkflow-master/train/Images/sneakers183.jpg  \n",
            "  inflating: darkflow-master/train/Images/sneakers184.jpg  \n",
            "  inflating: darkflow-master/train/Images/sneakers185.jpg  \n",
            "  inflating: darkflow-master/train/Images/sneakers186.jpg  \n",
            "  inflating: darkflow-master/train/Images/sneakers187.jpg  \n",
            "  inflating: darkflow-master/train/Images/sneakers188.jpg  \n",
            "  inflating: darkflow-master/train/Images/sneakers189.jpg  \n",
            "  inflating: darkflow-master/train/Images/sneakers190.jpg  \n",
            "  inflating: darkflow-master/train/Images/sneakers191.jpg  \n",
            "  inflating: darkflow-master/train/Images/sneakers192.jpg  \n",
            "  inflating: darkflow-master/train/Images/sneakers193.jpg  \n",
            "  inflating: darkflow-master/train/Images/sneakers194.jpg  \n",
            "  inflating: darkflow-master/train/Images/sneakers195.jpg  \n",
            "  inflating: darkflow-master/train/Images/sneakers196.jpg  \n",
            "  inflating: darkflow-master/train/Images/sneakers197.jpg  \n",
            "  inflating: darkflow-master/train/Images/sneakers198.jpg  \n",
            "  inflating: darkflow-master/train/Images/sneakers199.jpg  \n",
            "  inflating: darkflow-master/train/Images/sneakers200.jpg  \n",
            "   creating: darkflow-master/train/Annotations/\n",
            "  inflating: darkflow-master/train/Annotations/ankle_boot1.xml  \n",
            "  inflating: darkflow-master/train/Annotations/ankle_boot2.xml  \n",
            "  inflating: darkflow-master/train/Annotations/ankle_boot3.xml  \n",
            "  inflating: darkflow-master/train/Annotations/ankle_boot4.xml  \n",
            "  inflating: darkflow-master/train/Annotations/ankle_boot5.xml  \n",
            "  inflating: darkflow-master/train/Annotations/ankle_boot6.xml  \n",
            "  inflating: darkflow-master/train/Annotations/ankle_boot7.xml  \n",
            "  inflating: darkflow-master/train/Annotations/ankle_boot8.xml  \n",
            "  inflating: darkflow-master/train/Annotations/ankle_boot9.xml  \n",
            "  inflating: darkflow-master/train/Annotations/ankle_boot10.xml  \n",
            "  inflating: darkflow-master/train/Annotations/ankle_boot11.xml  \n",
            "  inflating: darkflow-master/train/Annotations/ankle_boot12.xml  \n",
            "  inflating: darkflow-master/train/Annotations/ankle_boot13.xml  \n",
            "  inflating: darkflow-master/train/Annotations/ankle_boot14.xml  \n",
            "  inflating: darkflow-master/train/Annotations/ankle_boot15.xml  \n",
            "  inflating: darkflow-master/train/Annotations/ankle_boot16.xml  \n",
            "  inflating: darkflow-master/train/Annotations/ankle_boot17.xml  \n",
            "  inflating: darkflow-master/train/Annotations/ankle_boot18.xml  \n",
            "  inflating: darkflow-master/train/Annotations/ankle_boot19.xml  \n",
            "  inflating: darkflow-master/train/Annotations/ankle_boot20.xml  \n",
            "  inflating: darkflow-master/train/Annotations/ankle_boot21.xml  \n",
            "  inflating: darkflow-master/train/Annotations/ankle_boot22.xml  \n",
            "  inflating: darkflow-master/train/Annotations/ankle_boot23.xml  \n",
            "  inflating: darkflow-master/train/Annotations/ankle_boot24.xml  \n",
            "  inflating: darkflow-master/train/Annotations/ankle_boot25.xml  \n",
            "  inflating: darkflow-master/train/Annotations/ankle_boot26.xml  \n",
            "  inflating: darkflow-master/train/Annotations/ankle_boot27.xml  \n",
            "  inflating: darkflow-master/train/Annotations/ankle_boot28.xml  \n",
            "  inflating: darkflow-master/train/Annotations/ankle_boot29.xml  \n",
            "  inflating: darkflow-master/train/Annotations/ankle_boot30.xml  \n",
            "  inflating: darkflow-master/train/Annotations/ankle_boot31.xml  \n",
            "  inflating: darkflow-master/train/Annotations/ankle_boot32.xml  \n",
            "  inflating: darkflow-master/train/Annotations/ankle_boot33.xml  \n",
            "  inflating: darkflow-master/train/Annotations/ankle_boot34.xml  \n",
            "  inflating: darkflow-master/train/Annotations/ankle_boot35.xml  \n",
            "  inflating: darkflow-master/train/Annotations/ankle_boot36.xml  \n",
            "  inflating: darkflow-master/train/Annotations/ankle_boot37.xml  \n",
            "  inflating: darkflow-master/train/Annotations/ankle_boot38.xml  \n",
            "  inflating: darkflow-master/train/Annotations/ankle_boot39.xml  \n",
            "  inflating: darkflow-master/train/Annotations/ankle_boot40.xml  \n",
            "  inflating: darkflow-master/train/Annotations/ankle_boot41.xml  \n",
            "  inflating: darkflow-master/train/Annotations/ankle_boot42.xml  \n",
            "  inflating: darkflow-master/train/Annotations/ankle_boot43.xml  \n",
            "  inflating: darkflow-master/train/Annotations/ankle_boot44.xml  \n",
            "  inflating: darkflow-master/train/Annotations/ankle_boot45.xml  \n",
            "  inflating: darkflow-master/train/Annotations/ankle_boot46.xml  \n",
            "  inflating: darkflow-master/train/Annotations/ankle_boot47.xml  \n",
            "  inflating: darkflow-master/train/Annotations/ankle_boot48.xml  \n",
            "  inflating: darkflow-master/train/Annotations/ankle_boot49.xml  \n",
            "  inflating: darkflow-master/train/Annotations/ankle_boot50.xml  \n",
            "  inflating: darkflow-master/train/Annotations/ankle_boot51.xml  \n",
            "  inflating: darkflow-master/train/Annotations/ankle_boot52.xml  \n",
            "  inflating: darkflow-master/train/Annotations/ankle_boot53.xml  \n",
            "  inflating: darkflow-master/train/Annotations/ankle_boot54.xml  \n",
            "  inflating: darkflow-master/train/Annotations/ankle_boot55.xml  \n",
            "  inflating: darkflow-master/train/Annotations/ankle_boot56.xml  \n",
            "  inflating: darkflow-master/train/Annotations/ankle_boot57.xml  \n",
            "  inflating: darkflow-master/train/Annotations/ankle_boot58.xml  \n",
            "  inflating: darkflow-master/train/Annotations/ankle_boot59.xml  \n",
            "  inflating: darkflow-master/train/Annotations/ankle_boot60.xml  \n",
            "  inflating: darkflow-master/train/Annotations/ankle_boot61.xml  \n",
            "  inflating: darkflow-master/train/Annotations/ankle_boot62.xml  \n",
            "  inflating: darkflow-master/train/Annotations/ankle_boot63.xml  \n",
            "  inflating: darkflow-master/train/Annotations/ankle_boot64.xml  \n",
            "  inflating: darkflow-master/train/Annotations/ankle_boot65.xml  \n",
            "  inflating: darkflow-master/train/Annotations/ankle_boot66.xml  \n",
            "  inflating: darkflow-master/train/Annotations/ankle_boot67.xml  \n",
            "  inflating: darkflow-master/train/Annotations/ankle_boot68.xml  \n",
            "  inflating: darkflow-master/train/Annotations/ankle_boot69.xml  \n",
            "  inflating: darkflow-master/train/Annotations/ankle_boot70.xml  \n",
            "  inflating: darkflow-master/train/Annotations/formal1.xml  \n",
            "  inflating: darkflow-master/train/Annotations/formal2.xml  \n",
            "  inflating: darkflow-master/train/Annotations/formal3.xml  \n",
            "  inflating: darkflow-master/train/Annotations/formal4.xml  \n",
            "  inflating: darkflow-master/train/Annotations/formal5.xml  \n",
            "  inflating: darkflow-master/train/Annotations/formal6.xml  \n",
            "  inflating: darkflow-master/train/Annotations/formal7.xml  \n",
            "  inflating: darkflow-master/train/Annotations/formal8.xml  \n",
            "  inflating: darkflow-master/train/Annotations/formal9.xml  \n",
            "  inflating: darkflow-master/train/Annotations/formal10.xml  \n",
            "  inflating: darkflow-master/train/Annotations/formal11.xml  \n",
            "  inflating: darkflow-master/train/Annotations/formal12.xml  \n",
            "  inflating: darkflow-master/train/Annotations/formal13.xml  \n",
            "  inflating: darkflow-master/train/Annotations/formal14.xml  \n",
            "  inflating: darkflow-master/train/Annotations/formal15.xml  \n",
            "  inflating: darkflow-master/train/Annotations/formal16.xml  \n",
            "  inflating: darkflow-master/train/Annotations/formal17.xml  \n",
            "  inflating: darkflow-master/train/Annotations/formal18.xml  \n",
            "  inflating: darkflow-master/train/Annotations/formal19.xml  \n",
            "  inflating: darkflow-master/train/Annotations/formal20.xml  \n",
            "  inflating: darkflow-master/train/Annotations/formal21.xml  \n",
            "  inflating: darkflow-master/train/Annotations/formal22.xml  \n",
            "  inflating: darkflow-master/train/Annotations/formal23.xml  \n",
            "  inflating: darkflow-master/train/Annotations/formal24.xml  \n",
            "  inflating: darkflow-master/train/Annotations/formal25.xml  \n",
            "  inflating: darkflow-master/train/Annotations/formal26.xml  \n",
            "  inflating: darkflow-master/train/Annotations/formal27.xml  \n",
            "  inflating: darkflow-master/train/Annotations/formal28.xml  \n",
            "  inflating: darkflow-master/train/Annotations/formal29.xml  \n",
            "  inflating: darkflow-master/train/Annotations/formal30.xml  \n",
            "  inflating: darkflow-master/train/Annotations/formal31.xml  \n",
            "  inflating: darkflow-master/train/Annotations/formal32.xml  \n",
            "  inflating: darkflow-master/train/Annotations/formal33.xml  \n",
            "  inflating: darkflow-master/train/Annotations/formal34.xml  \n",
            "  inflating: darkflow-master/train/Annotations/formal35.xml  \n",
            "  inflating: darkflow-master/train/Annotations/formal36.xml  \n",
            "  inflating: darkflow-master/train/Annotations/formal37.xml  \n",
            "  inflating: darkflow-master/train/Annotations/formal38.xml  \n",
            "  inflating: darkflow-master/train/Annotations/formal39.xml  \n",
            "  inflating: darkflow-master/train/Annotations/formal40.xml  \n",
            "  inflating: darkflow-master/train/Annotations/formal41.xml  \n",
            "  inflating: darkflow-master/train/Annotations/formal42.xml  \n",
            "  inflating: darkflow-master/train/Annotations/formal43.xml  \n",
            "  inflating: darkflow-master/train/Annotations/formal44.xml  \n",
            "  inflating: darkflow-master/train/Annotations/formal45.xml  \n",
            "  inflating: darkflow-master/train/Annotations/formal46.xml  \n",
            "  inflating: darkflow-master/train/Annotations/formal47.xml  \n",
            "  inflating: darkflow-master/train/Annotations/formal48.xml  \n",
            "  inflating: darkflow-master/train/Annotations/formal49.xml  \n",
            "  inflating: darkflow-master/train/Annotations/formal50.xml  \n",
            "  inflating: darkflow-master/train/Annotations/formal51.xml  \n",
            "  inflating: darkflow-master/train/Annotations/formal52.xml  \n",
            "  inflating: darkflow-master/train/Annotations/formal53.xml  \n",
            "  inflating: darkflow-master/train/Annotations/formal54.xml  \n",
            "  inflating: darkflow-master/train/Annotations/formal55.xml  \n",
            "  inflating: darkflow-master/train/Annotations/formal56.xml  \n",
            "  inflating: darkflow-master/train/Annotations/formal57.xml  \n",
            "  inflating: darkflow-master/train/Annotations/formal58.xml  \n",
            "  inflating: darkflow-master/train/Annotations/formal59.xml  \n",
            "  inflating: darkflow-master/train/Annotations/formal60.xml  \n",
            "  inflating: darkflow-master/train/Annotations/formal61.xml  \n",
            "  inflating: darkflow-master/train/Annotations/formal62.xml  \n",
            "  inflating: darkflow-master/train/Annotations/formal63.xml  \n",
            "  inflating: darkflow-master/train/Annotations/formal64.xml  \n",
            "  inflating: darkflow-master/train/Annotations/formal65.xml  \n",
            "  inflating: darkflow-master/train/Annotations/formal66.xml  \n",
            "  inflating: darkflow-master/train/Annotations/formal67.xml  \n",
            "  inflating: darkflow-master/train/Annotations/formal68.xml  \n",
            "  inflating: darkflow-master/train/Annotations/formal69.xml  \n",
            "  inflating: darkflow-master/train/Annotations/formal70.xml  \n",
            "  inflating: darkflow-master/train/Annotations/formal71.xml  \n",
            "  inflating: darkflow-master/train/Annotations/formal72.xml  \n",
            "  inflating: darkflow-master/train/Annotations/formal73.xml  \n",
            "  inflating: darkflow-master/train/Annotations/formal74.xml  \n",
            "  inflating: darkflow-master/train/Annotations/formal75.xml  \n",
            "  inflating: darkflow-master/train/Annotations/formal76.xml  \n",
            "  inflating: darkflow-master/train/Annotations/formal77.xml  \n",
            "  inflating: darkflow-master/train/Annotations/formal78.xml  \n",
            "  inflating: darkflow-master/train/Annotations/formal79.xml  \n",
            "  inflating: darkflow-master/train/Annotations/formal80.xml  \n",
            "  inflating: darkflow-master/train/Annotations/formal81.xml  \n",
            "  inflating: darkflow-master/train/Annotations/formal82.xml  \n",
            "  inflating: darkflow-master/train/Annotations/formal83.xml  \n",
            "  inflating: darkflow-master/train/Annotations/formal84.xml  \n",
            "  inflating: darkflow-master/train/Annotations/formal85.xml  \n",
            "  inflating: darkflow-master/train/Annotations/formal86.xml  \n",
            "  inflating: darkflow-master/train/Annotations/formal87.xml  \n",
            "  inflating: darkflow-master/train/Annotations/formal88.xml  \n",
            "  inflating: darkflow-master/train/Annotations/formal89.xml  \n",
            "  inflating: darkflow-master/train/Annotations/formal90.xml  \n",
            "  inflating: darkflow-master/train/Annotations/formal91.xml  \n",
            "  inflating: darkflow-master/train/Annotations/formal92.xml  \n",
            "  inflating: darkflow-master/train/Annotations/formal93.xml  \n",
            "  inflating: darkflow-master/train/Annotations/formal94.xml  \n",
            "  inflating: darkflow-master/train/Annotations/formal95.xml  \n",
            "  inflating: darkflow-master/train/Annotations/formal96.xml  \n",
            "  inflating: darkflow-master/train/Annotations/formal97.xml  \n",
            "  inflating: darkflow-master/train/Annotations/formal98.xml  \n",
            "  inflating: darkflow-master/train/Annotations/formal99.xml  \n",
            "  inflating: darkflow-master/train/Annotations/formal100.xml  \n",
            "  inflating: darkflow-master/train/Annotations/sneakers1.xml  \n",
            "  inflating: darkflow-master/train/Annotations/sneakers2.xml  \n",
            "  inflating: darkflow-master/train/Annotations/sneakers3.xml  \n",
            "  inflating: darkflow-master/train/Annotations/sneakers4.xml  \n",
            "  inflating: darkflow-master/train/Annotations/sneakers5.xml  \n",
            "  inflating: darkflow-master/train/Annotations/sneakers6.xml  \n",
            "  inflating: darkflow-master/train/Annotations/sneakers7.xml  \n",
            "  inflating: darkflow-master/train/Annotations/sneakers8.xml  \n",
            "  inflating: darkflow-master/train/Annotations/sneakers9.xml  \n",
            "  inflating: darkflow-master/train/Annotations/sneakers10.xml  \n",
            "  inflating: darkflow-master/train/Annotations/sneakers11.xml  \n",
            "  inflating: darkflow-master/train/Annotations/sneakers12.xml  \n",
            "  inflating: darkflow-master/train/Annotations/sneakers13.xml  \n",
            "  inflating: darkflow-master/train/Annotations/sneakers14.xml  \n",
            "  inflating: darkflow-master/train/Annotations/sneakers15.xml  \n",
            "  inflating: darkflow-master/train/Annotations/sneakers16.xml  \n",
            "  inflating: darkflow-master/train/Annotations/sneakers17.xml  \n",
            "  inflating: darkflow-master/train/Annotations/sneakers18.xml  \n",
            "  inflating: darkflow-master/train/Annotations/sneakers19.xml  \n",
            "  inflating: darkflow-master/train/Annotations/sneakers20.xml  \n",
            "  inflating: darkflow-master/train/Annotations/sneakers21.xml  \n",
            "  inflating: darkflow-master/train/Annotations/sneakers22.xml  \n",
            "  inflating: darkflow-master/train/Annotations/sneakers23.xml  \n",
            "  inflating: darkflow-master/train/Annotations/sneakers24.xml  \n",
            "  inflating: darkflow-master/train/Annotations/sneakers25.xml  \n",
            "  inflating: darkflow-master/train/Annotations/sneakers26.xml  \n",
            "  inflating: darkflow-master/train/Annotations/sneakers27.xml  \n",
            "  inflating: darkflow-master/train/Annotations/sneakers28.xml  \n",
            "  inflating: darkflow-master/train/Annotations/sneakers29.xml  \n",
            "  inflating: darkflow-master/train/Annotations/sneakers30.xml  \n",
            "  inflating: darkflow-master/train/Annotations/sneakers31.xml  \n",
            "  inflating: darkflow-master/train/Annotations/sneakers32.xml  \n",
            "  inflating: darkflow-master/train/Annotations/sneakers33.xml  \n",
            "  inflating: darkflow-master/train/Annotations/sneakers34.xml  \n",
            "  inflating: darkflow-master/train/Annotations/sneakers35.xml  \n",
            "  inflating: darkflow-master/train/Annotations/sneakers36.xml  \n",
            "  inflating: darkflow-master/train/Annotations/sneakers37.xml  \n",
            "  inflating: darkflow-master/train/Annotations/sneakers38.xml  \n",
            "  inflating: darkflow-master/train/Annotations/sneakers39.xml  \n",
            "  inflating: darkflow-master/train/Annotations/sneakers40.xml  \n",
            "  inflating: darkflow-master/train/Annotations/sneakers41.xml  \n",
            "  inflating: darkflow-master/train/Annotations/sneakers42.xml  \n",
            "  inflating: darkflow-master/train/Annotations/sneakers43.xml  \n",
            "  inflating: darkflow-master/train/Annotations/sneakers44.xml  \n",
            "  inflating: darkflow-master/train/Annotations/sneakers45.xml  \n",
            "  inflating: darkflow-master/train/Annotations/sneakers46.xml  \n",
            "  inflating: darkflow-master/train/Annotations/sneakers47.xml  \n",
            "  inflating: darkflow-master/train/Annotations/sneakers48.xml  \n",
            "  inflating: darkflow-master/train/Annotations/sneakers49.xml  \n",
            "  inflating: darkflow-master/train/Annotations/sneakers50.xml  \n",
            "  inflating: darkflow-master/train/Annotations/sneakers51.xml  \n",
            "  inflating: darkflow-master/train/Annotations/sneakers52.xml  \n",
            "  inflating: darkflow-master/train/Annotations/sneakers53.xml  \n",
            "  inflating: darkflow-master/train/Annotations/sneakers54.xml  \n",
            "  inflating: darkflow-master/train/Annotations/sneakers55.xml  \n",
            "  inflating: darkflow-master/train/Annotations/sneakers56.xml  \n",
            "  inflating: darkflow-master/train/Annotations/sneakers57.xml  \n",
            "  inflating: darkflow-master/train/Annotations/sneakers58.xml  \n",
            "  inflating: darkflow-master/train/Annotations/sneakers59.xml  \n",
            "  inflating: darkflow-master/train/Annotations/sneakers60.xml  \n",
            "  inflating: darkflow-master/train/Annotations/sneakers61.xml  \n",
            "  inflating: darkflow-master/train/Annotations/sneakers62.xml  \n",
            "  inflating: darkflow-master/train/Annotations/sneakers63.xml  \n",
            "  inflating: darkflow-master/train/Annotations/sneakers64.xml  \n",
            "  inflating: darkflow-master/train/Annotations/sneakers65.xml  \n",
            "  inflating: darkflow-master/train/Annotations/sneakers66.xml  \n",
            "  inflating: darkflow-master/train/Annotations/sneakers67.xml  \n",
            "  inflating: darkflow-master/train/Annotations/sneakers68.xml  \n",
            "  inflating: darkflow-master/train/Annotations/sneakers69.xml  \n",
            "  inflating: darkflow-master/train/Annotations/sneakers70.xml  \n",
            "  inflating: darkflow-master/train/Annotations/sneakers71.xml  \n",
            "  inflating: darkflow-master/train/Annotations/sneakers72.xml  \n",
            "  inflating: darkflow-master/train/Annotations/sneakers73.xml  \n",
            "  inflating: darkflow-master/train/Annotations/sneakers74.xml  \n",
            "  inflating: darkflow-master/train/Annotations/sneakers75.xml  \n",
            "  inflating: darkflow-master/train/Annotations/sneakers76.xml  \n",
            "  inflating: darkflow-master/train/Annotations/sneakers77.xml  \n",
            "  inflating: darkflow-master/train/Annotations/sneakers78.xml  \n",
            "  inflating: darkflow-master/train/Annotations/sneakers79.xml  \n",
            "  inflating: darkflow-master/train/Annotations/sneakers80.xml  \n",
            "  inflating: darkflow-master/train/Annotations/sneakers81.xml  \n",
            "  inflating: darkflow-master/train/Annotations/sneakers82.xml  \n",
            "  inflating: darkflow-master/train/Annotations/sneakers83.xml  \n",
            "  inflating: darkflow-master/train/Annotations/sneakers84.xml  \n",
            "  inflating: darkflow-master/train/Annotations/sneakers85.xml  \n",
            "  inflating: darkflow-master/train/Annotations/sneakers86.xml  \n",
            "  inflating: darkflow-master/train/Annotations/sneakers87.xml  \n",
            "  inflating: darkflow-master/train/Annotations/sneakers88.xml  \n",
            "  inflating: darkflow-master/train/Annotations/sneakers89.xml  \n",
            "  inflating: darkflow-master/train/Annotations/sneakers90.xml  \n",
            "  inflating: darkflow-master/train/Annotations/sneakers91.xml  \n",
            "  inflating: darkflow-master/train/Annotations/sneakers92.xml  \n",
            "  inflating: darkflow-master/train/Annotations/sneakers93.xml  \n",
            "  inflating: darkflow-master/train/Annotations/sneakers94.xml  \n",
            "  inflating: darkflow-master/train/Annotations/sneakers95.xml  \n",
            "  inflating: darkflow-master/train/Annotations/sneakers96.xml  \n",
            "  inflating: darkflow-master/train/Annotations/sneakers97.xml  \n",
            "  inflating: darkflow-master/train/Annotations/sneakers98.xml  \n",
            "  inflating: darkflow-master/train/Annotations/sneakers99.xml  \n",
            "  inflating: darkflow-master/train/Annotations/sneakers100.xml  \n",
            "  inflating: darkflow-master/train/Annotations/sneakers101.xml  \n",
            "  inflating: darkflow-master/train/Annotations/sneakers102.xml  \n",
            "  inflating: darkflow-master/train/Annotations/sneakers103.xml  \n",
            "  inflating: darkflow-master/train/Annotations/sneakers104.xml  \n",
            "  inflating: darkflow-master/train/Annotations/sneakers105.xml  \n",
            "  inflating: darkflow-master/train/Annotations/sneakers106.xml  \n",
            "  inflating: darkflow-master/train/Annotations/sneakers107.xml  \n",
            "  inflating: darkflow-master/train/Annotations/sneakers108.xml  \n",
            "  inflating: darkflow-master/train/Annotations/sneakers109.xml  \n",
            "  inflating: darkflow-master/train/Annotations/sneakers110.xml  \n",
            "  inflating: darkflow-master/train/Annotations/sneakers111.xml  \n",
            "  inflating: darkflow-master/train/Annotations/sneakers112.xml  \n",
            "  inflating: darkflow-master/train/Annotations/sneakers113.xml  \n",
            "  inflating: darkflow-master/train/Annotations/sneakers114.xml  \n",
            "  inflating: darkflow-master/train/Annotations/sneakers115.xml  \n",
            "  inflating: darkflow-master/train/Annotations/sneakers116.xml  \n",
            "  inflating: darkflow-master/train/Annotations/sneakers117.xml  \n",
            "  inflating: darkflow-master/train/Annotations/sneakers118.xml  \n",
            "  inflating: darkflow-master/train/Annotations/sneakers119.xml  \n",
            "  inflating: darkflow-master/train/Annotations/sneakers120.xml  \n",
            "  inflating: darkflow-master/train/Annotations/sneakers121.xml  \n",
            "  inflating: darkflow-master/train/Annotations/sneakers122.xml  \n",
            "  inflating: darkflow-master/train/Annotations/sneakers123.xml  \n",
            "  inflating: darkflow-master/train/Annotations/sneakers124.xml  \n",
            "  inflating: darkflow-master/train/Annotations/sneakers125.xml  \n",
            "  inflating: darkflow-master/train/Annotations/sneakers126.xml  \n",
            "  inflating: darkflow-master/train/Annotations/sneakers127.xml  \n",
            "  inflating: darkflow-master/train/Annotations/sneakers128.xml  \n",
            "  inflating: darkflow-master/train/Annotations/sneakers129.xml  \n",
            "  inflating: darkflow-master/train/Annotations/sneakers130.xml  \n",
            "  inflating: darkflow-master/train/Annotations/sneakers131.xml  \n",
            "  inflating: darkflow-master/train/Annotations/sneakers132.xml  \n",
            "  inflating: darkflow-master/train/Annotations/sneakers133.xml  \n",
            "  inflating: darkflow-master/train/Annotations/sneakers134.xml  \n",
            "  inflating: darkflow-master/train/Annotations/sneakers135.xml  \n",
            "  inflating: darkflow-master/train/Annotations/sneakers136.xml  \n",
            "  inflating: darkflow-master/train/Annotations/sneakers137.xml  \n",
            "  inflating: darkflow-master/train/Annotations/sneakers138.xml  \n",
            "  inflating: darkflow-master/train/Annotations/sneakers139.xml  \n",
            "  inflating: darkflow-master/train/Annotations/sneakers140.xml  \n",
            "  inflating: darkflow-master/train/Annotations/sneakers141.xml  \n",
            "  inflating: darkflow-master/train/Annotations/sneakers142.xml  \n",
            "  inflating: darkflow-master/train/Annotations/sneakers143.xml  \n",
            "  inflating: darkflow-master/train/Annotations/sneakers144.xml  \n",
            "  inflating: darkflow-master/train/Annotations/sneakers145.xml  \n",
            "  inflating: darkflow-master/train/Annotations/sneakers146.xml  \n",
            "  inflating: darkflow-master/train/Annotations/sneakers147.xml  \n",
            "  inflating: darkflow-master/train/Annotations/sneakers148.xml  \n",
            "  inflating: darkflow-master/train/Annotations/sneakers149.xml  \n",
            "  inflating: darkflow-master/train/Annotations/sneakers150.xml  \n",
            "  inflating: darkflow-master/train/Annotations/sneakers151.xml  \n",
            "  inflating: darkflow-master/train/Annotations/sneakers152.xml  \n",
            "  inflating: darkflow-master/train/Annotations/sneakers153.xml  \n",
            "  inflating: darkflow-master/train/Annotations/sneakers154.xml  \n",
            "  inflating: darkflow-master/train/Annotations/sneakers155.xml  \n",
            "  inflating: darkflow-master/train/Annotations/sneakers156.xml  \n",
            "  inflating: darkflow-master/train/Annotations/sneakers157.xml  \n",
            "  inflating: darkflow-master/train/Annotations/sneakers158.xml  \n",
            "  inflating: darkflow-master/train/Annotations/sneakers159.xml  \n",
            "  inflating: darkflow-master/train/Annotations/sneakers160.xml  \n",
            "  inflating: darkflow-master/train/Annotations/sneakers161.xml  \n",
            "  inflating: darkflow-master/train/Annotations/sneakers162.xml  \n",
            "  inflating: darkflow-master/train/Annotations/sneakers163.xml  \n",
            "  inflating: darkflow-master/train/Annotations/sneakers164.xml  \n",
            "  inflating: darkflow-master/train/Annotations/sneakers165.xml  \n",
            "  inflating: darkflow-master/train/Annotations/sneakers166.xml  \n",
            "  inflating: darkflow-master/train/Annotations/sneakers167.xml  \n",
            "  inflating: darkflow-master/train/Annotations/sneakers168.xml  \n",
            "  inflating: darkflow-master/train/Annotations/sneakers169.xml  \n",
            "  inflating: darkflow-master/train/Annotations/sneakers170.xml  \n",
            "  inflating: darkflow-master/train/Annotations/sneakers171.xml  \n",
            "  inflating: darkflow-master/train/Annotations/sneakers172.xml  \n",
            "  inflating: darkflow-master/train/Annotations/sneakers173.xml  \n",
            "  inflating: darkflow-master/train/Annotations/sneakers174.xml  \n",
            "  inflating: darkflow-master/train/Annotations/sneakers175.xml  \n",
            "  inflating: darkflow-master/train/Annotations/sneakers176.xml  \n",
            "  inflating: darkflow-master/train/Annotations/sneakers177.xml  \n",
            "  inflating: darkflow-master/train/Annotations/sneakers178.xml  \n",
            "  inflating: darkflow-master/train/Annotations/sneakers179.xml  \n",
            "  inflating: darkflow-master/train/Annotations/sneakers180.xml  \n",
            "  inflating: darkflow-master/train/Annotations/sneakers181.xml  \n",
            "  inflating: darkflow-master/train/Annotations/sneakers182.xml  \n",
            "  inflating: darkflow-master/train/Annotations/sneakers183.xml  \n",
            "  inflating: darkflow-master/train/Annotations/sneakers184.xml  \n",
            "  inflating: darkflow-master/train/Annotations/sneakers185.xml  \n",
            "  inflating: darkflow-master/train/Annotations/sneakers186.xml  \n",
            "  inflating: darkflow-master/train/Annotations/sneakers187.xml  \n",
            "  inflating: darkflow-master/train/Annotations/sneakers188.xml  \n",
            "  inflating: darkflow-master/train/Annotations/sneakers189.xml  \n",
            "  inflating: darkflow-master/train/Annotations/sneakers190.xml  \n",
            "  inflating: darkflow-master/train/Annotations/sneakers191.xml  \n",
            "  inflating: darkflow-master/train/Annotations/sneakers192.xml  \n",
            "  inflating: darkflow-master/train/Annotations/sneakers193.xml  \n",
            "  inflating: darkflow-master/train/Annotations/sneakers194.xml  \n",
            "  inflating: darkflow-master/train/Annotations/sneakers195.xml  \n",
            "  inflating: darkflow-master/train/Annotations/sneakers196.xml  \n",
            "  inflating: darkflow-master/train/Annotations/sneakers197.xml  \n",
            "  inflating: darkflow-master/train/Annotations/sneakers198.xml  \n",
            "  inflating: darkflow-master/train/Annotations/sneakers199.xml  \n",
            "  inflating: darkflow-master/train/Annotations/sneakers200.xml  \n",
            "  inflating: darkflow-master/LICENSE  \n",
            "  inflating: darkflow-master/README.md  \n",
            "   creating: darkflow-master/cfg/\n",
            "  inflating: darkflow-master/cfg/coco.names  \n",
            "  inflating: darkflow-master/cfg/extraction.cfg  \n",
            "  inflating: darkflow-master/cfg/extraction.conv.cfg  \n",
            "  inflating: darkflow-master/cfg/tiny-yolo-4c.cfg  \n",
            "  inflating: darkflow-master/cfg/tiny-yolo-voc.cfg  \n",
            "  inflating: darkflow-master/cfg/tiny-yolo.cfg  \n",
            "   creating: darkflow-master/cfg/v1.1/\n",
            "  inflating: darkflow-master/cfg/v1.1/person-bottle.cfg  \n",
            "  inflating: darkflow-master/cfg/v1.1/tiny-coco.cfg  \n",
            "  inflating: darkflow-master/cfg/v1.1/tiny-yolo-4c.cfg  \n",
            "  inflating: darkflow-master/cfg/v1.1/tiny-yolov1.cfg  \n",
            "  inflating: darkflow-master/cfg/v1.1/yolo-coco.cfg  \n",
            "  inflating: darkflow-master/cfg/v1.1/yolov1.cfg  \n",
            "   creating: darkflow-master/cfg/v1/\n",
            "  inflating: darkflow-master/cfg/v1/tiny-old.profile  \n",
            "  inflating: darkflow-master/cfg/v1/tiny.profile  \n",
            "  inflating: darkflow-master/cfg/v1/yolo-2c.cfg  \n",
            "  inflating: darkflow-master/cfg/v1/yolo-4c.cfg  \n",
            "  inflating: darkflow-master/cfg/v1/yolo-full.cfg  \n",
            "  inflating: darkflow-master/cfg/v1/yolo-small.cfg  \n",
            "  inflating: darkflow-master/cfg/v1/yolo-tiny-extract.cfg  \n",
            "  inflating: darkflow-master/cfg/v1/yolo-tiny-extract_.cfg  \n",
            "  inflating: darkflow-master/cfg/v1/yolo-tiny.cfg  \n",
            "  inflating: darkflow-master/cfg/v1/yolo-tiny4c.cfg  \n",
            "  inflating: darkflow-master/cfg/yolo-voc.cfg  \n",
            "  inflating: darkflow-master/cfg/yolo.cfg  \n",
            "  inflating: darkflow-master/cfg/tiny-yolo-voc1.cfg  \n",
            "  inflating: darkflow-master/cfg/tiny-yolo1.cfg  \n",
            "  inflating: darkflow-master/cfg/yolo-voc1.cfg  \n",
            "   creating: darkflow-master/darkflow/\n",
            "  inflating: darkflow-master/darkflow/__init__.py  \n",
            "  inflating: darkflow-master/darkflow/cli.py  \n",
            "   creating: darkflow-master/darkflow/cython_utils/\n",
            "  inflating: darkflow-master/darkflow/cython_utils/__init__.py  \n",
            "  inflating: darkflow-master/darkflow/cython_utils/cy_yolo2_findboxes.pyx  \n",
            "  inflating: darkflow-master/darkflow/cython_utils/cy_yolo_findboxes.pyx  \n",
            "  inflating: darkflow-master/darkflow/cython_utils/nms.pxd  \n",
            "  inflating: darkflow-master/darkflow/cython_utils/nms.pyx  \n",
            "  inflating: darkflow-master/darkflow/cython_utils/cy_yolo2_findboxes.c  \n",
            "  inflating: darkflow-master/darkflow/cython_utils/cy_yolo_findboxes.c  \n",
            "  inflating: darkflow-master/darkflow/cython_utils/nms.c  \n",
            "  inflating: darkflow-master/darkflow/cython_utils/nms.cpython-36m-x86_64-linux-gnu.so  \n",
            "  inflating: darkflow-master/darkflow/cython_utils/cy_yolo2_findboxes.cpython-36m-x86_64-linux-gnu.so  \n",
            "  inflating: darkflow-master/darkflow/cython_utils/cy_yolo_findboxes.cpython-36m-x86_64-linux-gnu.so  \n",
            "   creating: darkflow-master/darkflow/cython_utils/__pycache__/\n",
            "  inflating: darkflow-master/darkflow/cython_utils/__pycache__/__init__.cpython-36.pyc  \n",
            "   creating: darkflow-master/darkflow/dark/\n",
            "  inflating: darkflow-master/darkflow/dark/__init__.py  \n",
            "  inflating: darkflow-master/darkflow/dark/connected.py  \n",
            "  inflating: darkflow-master/darkflow/dark/convolution.py  \n",
            "  inflating: darkflow-master/darkflow/dark/darknet.py  \n",
            "  inflating: darkflow-master/darkflow/dark/darkop.py  \n",
            "  inflating: darkflow-master/darkflow/dark/layer.py  \n",
            "   creating: darkflow-master/darkflow/dark/__pycache__/\n",
            "  inflating: darkflow-master/darkflow/dark/__pycache__/__init__.cpython-36.pyc  \n",
            "  inflating: darkflow-master/darkflow/dark/__pycache__/darknet.cpython-36.pyc  \n",
            "  inflating: darkflow-master/darkflow/dark/__pycache__/darkop.cpython-36.pyc  \n",
            "  inflating: darkflow-master/darkflow/dark/__pycache__/layer.cpython-36.pyc  \n",
            "  inflating: darkflow-master/darkflow/dark/__pycache__/convolution.cpython-36.pyc  \n",
            "  inflating: darkflow-master/darkflow/dark/__pycache__/connected.cpython-36.pyc  \n",
            "  inflating: darkflow-master/darkflow/defaults.py  \n",
            "   creating: darkflow-master/darkflow/net/\n",
            "  inflating: darkflow-master/darkflow/net/__init__.py  \n",
            "  inflating: darkflow-master/darkflow/net/build.py  \n",
            "  inflating: darkflow-master/darkflow/net/flow.py  \n",
            "  inflating: darkflow-master/darkflow/net/framework.py  \n",
            "  inflating: darkflow-master/darkflow/net/help.py  \n",
            "   creating: darkflow-master/darkflow/net/mnist/\n",
            "  inflating: darkflow-master/darkflow/net/mnist/run.py  \n",
            "   creating: darkflow-master/darkflow/net/ops/\n",
            "  inflating: darkflow-master/darkflow/net/ops/__init__.py  \n",
            "  inflating: darkflow-master/darkflow/net/ops/baseop.py  \n",
            "  inflating: darkflow-master/darkflow/net/ops/convolution.py  \n",
            "  inflating: darkflow-master/darkflow/net/ops/simple.py  \n",
            "   creating: darkflow-master/darkflow/net/ops/__pycache__/\n",
            "  inflating: darkflow-master/darkflow/net/ops/__pycache__/__init__.cpython-36.pyc  \n",
            "  inflating: darkflow-master/darkflow/net/ops/__pycache__/simple.cpython-36.pyc  \n",
            "  inflating: darkflow-master/darkflow/net/ops/__pycache__/baseop.cpython-36.pyc  \n",
            "  inflating: darkflow-master/darkflow/net/ops/__pycache__/convolution.cpython-36.pyc  \n",
            "   creating: darkflow-master/darkflow/net/vanilla/\n",
            "  inflating: darkflow-master/darkflow/net/vanilla/__init__.py  \n",
            "  inflating: darkflow-master/darkflow/net/vanilla/train.py  \n",
            "   creating: darkflow-master/darkflow/net/vanilla/__pycache__/\n",
            "  inflating: darkflow-master/darkflow/net/vanilla/__pycache__/__init__.cpython-36.pyc  \n",
            "  inflating: darkflow-master/darkflow/net/vanilla/__pycache__/train.cpython-36.pyc  \n",
            "   creating: darkflow-master/darkflow/net/yolo/\n",
            "  inflating: darkflow-master/darkflow/net/yolo/__init__.py  \n",
            "  inflating: darkflow-master/darkflow/net/yolo/misc.py  \n",
            "  inflating: darkflow-master/darkflow/net/yolo/predict.py  \n",
            "  inflating: darkflow-master/darkflow/net/yolo/train.py  \n",
            "  inflating: darkflow-master/darkflow/net/yolo/data.py  \n",
            "   creating: darkflow-master/darkflow/net/yolo/__pycache__/\n",
            "  inflating: darkflow-master/darkflow/net/yolo/__pycache__/__init__.cpython-36.pyc  \n",
            "  inflating: darkflow-master/darkflow/net/yolo/__pycache__/train.cpython-36.pyc  \n",
            "  inflating: darkflow-master/darkflow/net/yolo/__pycache__/misc.cpython-36.pyc  \n",
            "  inflating: darkflow-master/darkflow/net/yolo/__pycache__/predict.cpython-36.pyc  \n",
            "  inflating: darkflow-master/darkflow/net/yolo/__pycache__/data.cpython-36.pyc  \n",
            "   creating: darkflow-master/darkflow/net/yolov2/\n",
            "  inflating: darkflow-master/darkflow/net/yolov2/__init__.py  \n",
            "  inflating: darkflow-master/darkflow/net/yolov2/predict.py  \n",
            "  inflating: darkflow-master/darkflow/net/yolov2/train.py  \n",
            "  inflating: darkflow-master/darkflow/net/yolov2/data.py  \n",
            "   creating: darkflow-master/darkflow/net/yolov2/__pycache__/\n",
            "  inflating: darkflow-master/darkflow/net/yolov2/__pycache__/__init__.cpython-36.pyc  \n",
            "  inflating: darkflow-master/darkflow/net/yolov2/__pycache__/train.cpython-36.pyc  \n",
            "  inflating: darkflow-master/darkflow/net/yolov2/__pycache__/predict.cpython-36.pyc  \n",
            "  inflating: darkflow-master/darkflow/net/yolov2/__pycache__/data.cpython-36.pyc  \n",
            "   creating: darkflow-master/darkflow/net/__pycache__/\n",
            "  inflating: darkflow-master/darkflow/net/__pycache__/__init__.cpython-36.pyc  \n",
            "  inflating: darkflow-master/darkflow/net/__pycache__/build.cpython-36.pyc  \n",
            "  inflating: darkflow-master/darkflow/net/__pycache__/help.cpython-36.pyc  \n",
            "  inflating: darkflow-master/darkflow/net/__pycache__/flow.cpython-36.pyc  \n",
            "  inflating: darkflow-master/darkflow/net/__pycache__/framework.cpython-36.pyc  \n",
            "   creating: darkflow-master/darkflow/utils/\n",
            "  inflating: darkflow-master/darkflow/utils/__init__.py  \n",
            "  inflating: darkflow-master/darkflow/utils/box.py  \n",
            "  inflating: darkflow-master/darkflow/utils/im_transform.py  \n",
            "  inflating: darkflow-master/darkflow/utils/loader.py  \n",
            "  inflating: darkflow-master/darkflow/utils/process.py  \n",
            "  inflating: darkflow-master/darkflow/utils/pascal_voc_clean_xml.py  \n",
            "   creating: darkflow-master/darkflow/utils/__pycache__/\n",
            "  inflating: darkflow-master/darkflow/utils/__pycache__/__init__.cpython-36.pyc  \n",
            "  inflating: darkflow-master/darkflow/utils/__pycache__/loader.cpython-36.pyc  \n",
            "  inflating: darkflow-master/darkflow/utils/__pycache__/im_transform.cpython-36.pyc  \n",
            "  inflating: darkflow-master/darkflow/utils/__pycache__/box.cpython-36.pyc  \n",
            "  inflating: darkflow-master/darkflow/utils/__pycache__/process.cpython-36.pyc  \n",
            "  inflating: darkflow-master/darkflow/utils/__pycache__/pascal_voc_clean_xml.cpython-36.pyc  \n",
            "  inflating: darkflow-master/darkflow/version.py  \n",
            "   creating: darkflow-master/darkflow/__pycache__/\n",
            "  inflating: darkflow-master/darkflow/__pycache__/version.cpython-36.pyc  \n",
            "  inflating: darkflow-master/darkflow/__pycache__/__init__.cpython-36.pyc  \n",
            "  inflating: darkflow-master/darkflow/__pycache__/cli.cpython-36.pyc  \n",
            "  inflating: darkflow-master/darkflow/__pycache__/defaults.cpython-36.pyc  \n",
            "  inflating: darkflow-master/labels.txt  \n",
            "  inflating: darkflow-master/demo.gif  \n",
            "  inflating: darkflow-master/flow    \n",
            "  inflating: darkflow-master/preview.png  \n",
            "   creating: darkflow-master/sample_img/\n",
            "   creating: darkflow-master/sample_img/out/\n",
            "  inflating: darkflow-master/sample_img/sample_computer.jpg  \n",
            "  inflating: darkflow-master/sample_img/sample_dog.jpg  \n",
            "  inflating: darkflow-master/sample_img/sample_eagle.jpg  \n",
            "  inflating: darkflow-master/sample_img/sample_giraffe.jpg  \n",
            "  inflating: darkflow-master/sample_img/sample_horses.jpg  \n",
            "  inflating: darkflow-master/sample_img/sample_office.jpg  \n",
            "  inflating: darkflow-master/sample_img/sample_person.jpg  \n",
            "  inflating: darkflow-master/sample_img/sample_scream.jpg  \n",
            "  inflating: darkflow-master/setup.py  \n",
            "   creating: darkflow-master/test/\n",
            "  inflating: darkflow-master/test/requirements-testing.txt  \n",
            "  inflating: darkflow-master/test/test_darkflow.py  \n",
            "   creating: darkflow-master/test/training/\n",
            "   creating: darkflow-master/test/training/annotations/\n",
            "  inflating: darkflow-master/test/training/annotations/1.xml  \n",
            "  inflating: darkflow-master/test/training/annotations/2.xml  \n",
            "   creating: darkflow-master/test/training/images/\n",
            "  inflating: darkflow-master/test/training/images/1.jpg  \n",
            "  inflating: darkflow-master/test/training/images/2.jpg  \n",
            "   creating: darkflow-master/test/test_images/\n",
            "  inflating: darkflow-master/test/test_images/ankle_boot1.jpg  \n",
            "  inflating: darkflow-master/test/test_images/ankle_boot2.jpg  \n",
            "  inflating: darkflow-master/test/test_images/ankle_boot4.jpg  \n",
            "  inflating: darkflow-master/test/test_images/ankle_boot6.jpg  \n",
            "  inflating: darkflow-master/test/test_images/ankle_boot3.jpg  \n",
            "  inflating: darkflow-master/test/test_images/ankle_boot5.jpg  \n",
            "  inflating: darkflow-master/test/test_images/formal1.jpg  \n",
            "  inflating: darkflow-master/test/test_images/formal5.jpg  \n",
            "  inflating: darkflow-master/test/test_images/formal2.jpg  \n",
            "  inflating: darkflow-master/test/test_images/formal4.jpg  \n",
            "  inflating: darkflow-master/test/test_images/formal6.jpg  \n",
            "  inflating: darkflow-master/test/test_images/formal3.jpg  \n",
            "  inflating: darkflow-master/test/test_images/sneakers5.jpg  \n",
            "  inflating: darkflow-master/test/test_images/sneakers4.jpg  \n",
            "  inflating: darkflow-master/test/test_images/sneakers1.jpg  \n",
            "  inflating: darkflow-master/test/test_images/sneakers6.jpg  \n",
            "  inflating: darkflow-master/test/test_images/sneakers2.jpg  \n",
            "   creating: darkflow-master/.idea/\n",
            "  inflating: darkflow-master/.idea/workspace.xml  \n",
            "   creating: darkflow-master/build/\n",
            "   creating: darkflow-master/build/temp.linux-x86_64-3.6/\n",
            "   creating: darkflow-master/build/temp.linux-x86_64-3.6/darkflow/\n",
            "   creating: darkflow-master/build/temp.linux-x86_64-3.6/darkflow/cython_utils/\n",
            "  inflating: darkflow-master/build/temp.linux-x86_64-3.6/darkflow/cython_utils/nms.o  \n",
            "  inflating: darkflow-master/build/temp.linux-x86_64-3.6/darkflow/cython_utils/cy_yolo2_findboxes.o  \n",
            "  inflating: darkflow-master/build/temp.linux-x86_64-3.6/darkflow/cython_utils/cy_yolo_findboxes.o  \n",
            "   creating: darkflow-master/build/lib.linux-x86_64-3.6/\n",
            "   creating: darkflow-master/build/lib.linux-x86_64-3.6/darkflow/\n",
            "   creating: darkflow-master/build/lib.linux-x86_64-3.6/darkflow/cython_utils/\n",
            "  inflating: darkflow-master/build/lib.linux-x86_64-3.6/darkflow/cython_utils/nms.cpython-36m-x86_64-linux-gnu.so  \n",
            "  inflating: darkflow-master/build/lib.linux-x86_64-3.6/darkflow/cython_utils/cy_yolo2_findboxes.cpython-36m-x86_64-linux-gnu.so  \n",
            "  inflating: darkflow-master/build/lib.linux-x86_64-3.6/darkflow/cython_utils/cy_yolo_findboxes.cpython-36m-x86_64-linux-gnu.so  \n",
            "   creating: darkflow-master/darkflow.egg-info/\n",
            "  inflating: darkflow-master/darkflow.egg-info/PKG-INFO  \n",
            "  inflating: darkflow-master/darkflow.egg-info/dependency_links.txt  \n",
            "  inflating: darkflow-master/darkflow.egg-info/top_level.txt  \n",
            "  inflating: darkflow-master/darkflow.egg-info/SOURCES.txt  \n",
            "   creating: darkflow-master/bin/\n",
            "  inflating: darkflow-master/bin/tiny-yolo-voc.weights  \n",
            "   creating: darkflow-master/test_images/\n",
            "  inflating: darkflow-master/test_images/ankle_boot1.jpg  \n",
            "  inflating: darkflow-master/test_images/ankle_boot2.jpg  \n",
            "  inflating: darkflow-master/test_images/ankle_boot4.jpg  \n",
            "  inflating: darkflow-master/test_images/ankle_boot6.jpg  \n",
            "  inflating: darkflow-master/test_images/ankle_boot3.jpg  \n",
            "  inflating: darkflow-master/test_images/ankle_boot5.jpg  \n",
            "  inflating: darkflow-master/test_images/formal1.jpg  \n",
            "  inflating: darkflow-master/test_images/formal5.jpg  \n",
            "  inflating: darkflow-master/test_images/formal2.jpg  \n",
            "  inflating: darkflow-master/test_images/formal4.jpg  \n",
            "  inflating: darkflow-master/test_images/formal6.jpg  \n",
            "  inflating: darkflow-master/test_images/formal3.jpg  \n",
            "  inflating: darkflow-master/test_images/sneakers5.jpg  \n",
            "  inflating: darkflow-master/test_images/sneakers4.jpg  \n",
            "  inflating: darkflow-master/test_images/sneakers1.jpg  \n",
            "  inflating: darkflow-master/test_images/sneakers6.jpg  \n",
            "  inflating: darkflow-master/test_images/sneakers2.jpg  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W_0WfEmiGcnB",
        "colab_type": "code",
        "outputId": "13fabf08-b648-4f4f-9b72-345ab5f8dbb6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Move to the folder\n",
        "cd darkflow-master"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/darkflow-master\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ooNEJca3ybVY",
        "colab_type": "text"
      },
      "source": [
        "**Install darkflow with pip globally**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4DfdShGdGuwV",
        "colab_type": "code",
        "outputId": "da7e9e71-74f8-4b73-c4eb-a467fd357835",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        }
      },
      "source": [
        "!pip install ."
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Processing /content/darkflow-master\n",
            "Building wheels for collected packages: darkflow\n",
            "  Building wheel for darkflow (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/94/cd/45/fe842f09b1dc09b90a7b34f82e86d34db8978db075b9e3c173\n",
            "Successfully built darkflow\n",
            "Installing collected packages: darkflow\n",
            "Successfully installed darkflow-1.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IVklXf-syi5j",
        "colab_type": "text"
      },
      "source": [
        "**Train the yolo model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZX7j1TdWG7gH",
        "colab_type": "code",
        "outputId": "713f2905-d667-4e5a-9a32-4b851c61e9f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 64815
        }
      },
      "source": [
        "!flow --model cfg/tiny-yolo-voc1.cfg --load bin/tiny-yolo-voc.weights --train --annotation train/Annotations --dataset train/Images --epoch 150"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0613 17:35:58.914663 140264209213312 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/darkflow/net/build.py:15: The name tf.train.RMSPropOptimizer is deprecated. Please use tf.compat.v1.train.RMSPropOptimizer instead.\n",
            "\n",
            "W0613 17:35:58.914915 140264209213312 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/darkflow/net/build.py:16: The name tf.train.AdadeltaOptimizer is deprecated. Please use tf.compat.v1.train.AdadeltaOptimizer instead.\n",
            "\n",
            "W0613 17:35:58.915022 140264209213312 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/darkflow/net/build.py:17: The name tf.train.AdagradOptimizer is deprecated. Please use tf.compat.v1.train.AdagradOptimizer instead.\n",
            "\n",
            "W0613 17:35:58.915114 140264209213312 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/darkflow/net/build.py:18: The name tf.train.AdagradDAOptimizer is deprecated. Please use tf.compat.v1.train.AdagradDAOptimizer instead.\n",
            "\n",
            "W0613 17:35:58.915210 140264209213312 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/darkflow/net/build.py:19: The name tf.train.MomentumOptimizer is deprecated. Please use tf.compat.v1.train.MomentumOptimizer instead.\n",
            "\n",
            "\n",
            "Parsing ./cfg/tiny-yolo-voc.cfg\n",
            "Parsing cfg/tiny-yolo-voc1.cfg\n",
            "Loading bin/tiny-yolo-voc.weights ...\n",
            "Successfully identified 63471556 bytes\n",
            "Finished in 0.0034487247467041016s\n",
            "\n",
            "Building net ...\n",
            "W0613 17:35:58.925227 140264209213312 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/darkflow/net/build.py:105: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "Source | Train? | Layer description                | Output size\n",
            "-------+--------+----------------------------------+---------------\n",
            "W0613 17:35:58.929789 140264209213312 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/darkflow/net/ops/baseop.py:70: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "W0613 17:35:58.930092 140264209213312 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/darkflow/net/ops/baseop.py:71: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
            "\n",
            "W0613 17:35:58.938149 140264209213312 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/darkflow/net/ops/baseop.py:84: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "       |        | input                            | (?, 416, 416, 3)\n",
            " Load  |  Yep!  | conv 3x3p1_1  +bnorm  leaky      | (?, 416, 416, 16)\n",
            "W0613 17:35:58.993123 140264209213312 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/darkflow/net/ops/simple.py:106: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            " Load  |  Yep!  | maxp 2x2p0_2                     | (?, 208, 208, 16)\n",
            " Load  |  Yep!  | conv 3x3p1_1  +bnorm  leaky      | (?, 208, 208, 32)\n",
            " Load  |  Yep!  | maxp 2x2p0_2                     | (?, 104, 104, 32)\n",
            " Load  |  Yep!  | conv 3x3p1_1  +bnorm  leaky      | (?, 104, 104, 64)\n",
            " Load  |  Yep!  | maxp 2x2p0_2                     | (?, 52, 52, 64)\n",
            " Load  |  Yep!  | conv 3x3p1_1  +bnorm  leaky      | (?, 52, 52, 128)\n",
            " Load  |  Yep!  | maxp 2x2p0_2                     | (?, 26, 26, 128)\n",
            " Load  |  Yep!  | conv 3x3p1_1  +bnorm  leaky      | (?, 26, 26, 256)\n",
            " Load  |  Yep!  | maxp 2x2p0_2                     | (?, 13, 13, 256)\n",
            " Load  |  Yep!  | conv 3x3p1_1  +bnorm  leaky      | (?, 13, 13, 512)\n",
            " Load  |  Yep!  | maxp 2x2p0_1                     | (?, 13, 13, 512)\n",
            " Load  |  Yep!  | conv 3x3p1_1  +bnorm  leaky      | (?, 13, 13, 1024)\n",
            " Load  |  Yep!  | conv 3x3p1_1  +bnorm  leaky      | (?, 13, 13, 1024)\n",
            " Init  |  Yep!  | conv 1x1p0_1    linear           | (?, 13, 13, 40)\n",
            "-------+--------+----------------------------------+---------------\n",
            "Running entirely on CPU\n",
            "cfg/tiny-yolo-voc1.cfg loss hyper-parameters:\n",
            "\tH       = 13\n",
            "\tW       = 13\n",
            "\tbox     = 5\n",
            "\tclasses = 3\n",
            "\tscales  = [1.0, 5.0, 1.0, 1.0]\n",
            "W0613 17:36:00.052650 140264209213312 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/darkflow/net/yolov2/train.py:87: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "Building cfg/tiny-yolo-voc1.cfg loss\n",
            "W0613 17:36:00.074517 140264209213312 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/darkflow/net/yolov2/train.py:107: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
            "\n",
            "Building cfg/tiny-yolo-voc1.cfg train op\n",
            "W0613 17:36:00.144086 140264209213312 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1205: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "W0613 17:36:00.818711 140264209213312 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/rmsprop.py:119: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "W0613 17:36:01.081975 140264209213312 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/darkflow/net/build.py:145: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "2019-06-13 17:36:01.093681: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz\n",
            "2019-06-13 17:36:01.095503: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x32652c0 executing computations on platform Host. Devices:\n",
            "2019-06-13 17:36:01.095538: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\n",
            "2019-06-13 17:36:01.100869: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1\n",
            "2019-06-13 17:36:01.312719: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-06-13 17:36:01.313245: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x3264a00 executing computations on platform CUDA. Devices:\n",
            "2019-06-13 17:36:01.313273: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
            "2019-06-13 17:36:01.313440: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2019-06-13 17:36:01.313458: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      \n",
            "2019-06-13 17:36:01.919153: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\n",
            "Finished in 3.443902015686035s\n",
            "\n",
            "Enter training ...\n",
            "\n",
            "cfg/tiny-yolo-voc1.cfg parsing train/Annotations\n",
            "Parsing for ['Ankle boots', 'Formal', 'Sneakers'] \n",
            "[====================>]100%  formal17.xml\n",
            "Statistics:\n",
            "Formal: 131\n",
            "Ankle boots: 82\n",
            "Sneakers: 255\n",
            "Dataset size: 370\n",
            "Dataset of 370 instance(s)\n",
            "Training statistics: \n",
            "\tLearning rate : 1e-05\n",
            "\tBatch size    : 16\n",
            "\tEpoch number  : 150\n",
            "\tBackup every  : 2000\n",
            "step 1 - loss 101.34446716308594 - moving ave loss 101.34446716308594\n",
            "step 2 - loss 100.5382080078125 - moving ave loss 101.2638412475586\n",
            "step 3 - loss 99.65943908691406 - moving ave loss 101.10340103149414\n",
            "step 4 - loss 99.04957580566406 - moving ave loss 100.89801850891114\n",
            "step 5 - loss 98.28955078125 - moving ave loss 100.63717173614504\n",
            "step 6 - loss 97.86434936523438 - moving ave loss 100.35988949905396\n",
            "step 7 - loss 97.22270202636719 - moving ave loss 100.0461707517853\n",
            "step 8 - loss 96.99046325683594 - moving ave loss 99.74060000229036\n",
            "step 9 - loss 96.27372741699219 - moving ave loss 99.39391274376055\n",
            "step 10 - loss 95.91020202636719 - moving ave loss 99.04554167202122\n",
            "step 11 - loss 95.32563781738281 - moving ave loss 98.67355128655737\n",
            "step 12 - loss 94.73661041259766 - moving ave loss 98.2798571991614\n",
            "step 13 - loss 94.4484634399414 - moving ave loss 97.8967178232394\n",
            "step 14 - loss 93.9873046875 - moving ave loss 97.50577650966547\n",
            "step 15 - loss 93.883056640625 - moving ave loss 97.14350452276142\n",
            "step 16 - loss 93.38714599609375 - moving ave loss 96.76786867009466\n",
            "step 17 - loss 92.98385620117188 - moving ave loss 96.38946742320238\n",
            "step 18 - loss 92.79710388183594 - moving ave loss 96.03023106906574\n",
            "step 19 - loss 92.12623596191406 - moving ave loss 95.63983155835056\n",
            "step 20 - loss 91.6629638671875 - moving ave loss 95.24214478923426\n",
            "step 21 - loss 91.56269073486328 - moving ave loss 94.87419938379716\n",
            "step 22 - loss 91.32078552246094 - moving ave loss 94.51885799766355\n",
            "step 23 - loss 90.55785369873047 - moving ave loss 94.12275756777024\n",
            "Finish 1 epoch(es)\n",
            "step 24 - loss 90.473388671875 - moving ave loss 93.75782067818072\n",
            "step 25 - loss 89.9398193359375 - moving ave loss 93.3760205439564\n",
            "step 26 - loss 89.55233764648438 - moving ave loss 92.99365225420921\n",
            "step 27 - loss 89.48360443115234 - moving ave loss 92.64264747190353\n",
            "step 28 - loss 88.69523620605469 - moving ave loss 92.24790634531864\n",
            "step 29 - loss 88.47257995605469 - moving ave loss 91.87037370639226\n",
            "step 30 - loss 88.1658935546875 - moving ave loss 91.49992569122179\n",
            "step 31 - loss 87.62733459472656 - moving ave loss 91.11266658157227\n",
            "step 32 - loss 87.6705322265625 - moving ave loss 90.7684531460713\n",
            "step 33 - loss 87.26153564453125 - moving ave loss 90.4177613959173\n",
            "step 34 - loss 86.51582336425781 - moving ave loss 90.02756759275134\n",
            "step 35 - loss 86.41493225097656 - moving ave loss 89.66630405857387\n",
            "step 36 - loss 86.16051483154297 - moving ave loss 89.31572513587078\n",
            "step 37 - loss 85.39059448242188 - moving ave loss 88.92321207052589\n",
            "step 38 - loss 85.44625091552734 - moving ave loss 88.57551595502603\n",
            "step 39 - loss 84.7430191040039 - moving ave loss 88.19226626992382\n",
            "step 40 - loss 84.69398498535156 - moving ave loss 87.8424381414666\n",
            "step 41 - loss 84.00178527832031 - moving ave loss 87.45837285515196\n",
            "step 42 - loss 83.71863555908203 - moving ave loss 87.08439912554498\n",
            "step 43 - loss 83.90303039550781 - moving ave loss 86.76626225254127\n",
            "step 44 - loss 83.15569305419922 - moving ave loss 86.40520533270707\n",
            "step 45 - loss 82.76911926269531 - moving ave loss 86.0415967257059\n",
            "step 46 - loss 82.20639038085938 - moving ave loss 85.65807609122125\n",
            "Finish 2 epoch(es)\n",
            "step 47 - loss 82.14511108398438 - moving ave loss 85.30677959049757\n",
            "step 48 - loss 81.60025024414062 - moving ave loss 84.93612665586188\n",
            "step 49 - loss 81.52608489990234 - moving ave loss 84.59512248026593\n",
            "step 50 - loss 81.16429901123047 - moving ave loss 84.25204013336239\n",
            "step 51 - loss 80.54399108886719 - moving ave loss 83.88123522891286\n",
            "step 52 - loss 80.52980041503906 - moving ave loss 83.54609174752548\n",
            "step 53 - loss 79.8582534790039 - moving ave loss 83.17730792067333\n",
            "step 54 - loss 79.78865814208984 - moving ave loss 82.83844294281498\n",
            "step 55 - loss 79.37371826171875 - moving ave loss 82.49197047470535\n",
            "step 56 - loss 78.7824935913086 - moving ave loss 82.12102278636569\n",
            "step 57 - loss 78.69131469726562 - moving ave loss 81.77805197745569\n",
            "step 58 - loss 78.19071960449219 - moving ave loss 81.41931874015934\n",
            "step 59 - loss 77.78213500976562 - moving ave loss 81.05560036711996\n",
            "step 60 - loss 77.37602996826172 - moving ave loss 80.68764332723414\n",
            "step 61 - loss 77.15245056152344 - moving ave loss 80.33412405066306\n",
            "step 62 - loss 76.76171875 - moving ave loss 79.97688352059676\n",
            "step 63 - loss 76.50066375732422 - moving ave loss 79.62926154426951\n",
            "step 64 - loss 75.97312927246094 - moving ave loss 79.26364831708865\n",
            "step 65 - loss 75.444580078125 - moving ave loss 78.88174149319228\n",
            "step 66 - loss 75.49932861328125 - moving ave loss 78.54350020520118\n",
            "step 67 - loss 74.88164520263672 - moving ave loss 78.17731470494473\n",
            "step 68 - loss 74.38104248046875 - moving ave loss 77.79768748249714\n",
            "step 69 - loss 74.3297119140625 - moving ave loss 77.45088992565368\n",
            "Finish 3 epoch(es)\n",
            "step 70 - loss 74.13261413574219 - moving ave loss 77.11906234666253\n",
            "step 71 - loss 73.56219482421875 - moving ave loss 76.76337559441816\n",
            "step 72 - loss 73.39838409423828 - moving ave loss 76.42687644440018\n",
            "step 73 - loss 73.0968017578125 - moving ave loss 76.09386897574142\n",
            "step 74 - loss 72.217041015625 - moving ave loss 75.70618617972977\n",
            "step 75 - loss 72.00801086425781 - moving ave loss 75.33636864818259\n",
            "step 76 - loss 71.41184997558594 - moving ave loss 74.94391678092292\n",
            "step 77 - loss 71.35707092285156 - moving ave loss 74.5852321951158\n",
            "step 78 - loss 71.15533447265625 - moving ave loss 74.24224242286984\n",
            "step 79 - loss 70.28118896484375 - moving ave loss 73.84613707706723\n",
            "step 80 - loss 70.56078338623047 - moving ave loss 73.51760170798356\n",
            "step 81 - loss 69.34119415283203 - moving ave loss 73.09996095246841\n",
            "step 82 - loss 69.2259521484375 - moving ave loss 72.71256007206532\n",
            "step 83 - loss 69.01225280761719 - moving ave loss 72.3425293456205\n",
            "step 84 - loss 68.7478256225586 - moving ave loss 71.98305897331431\n",
            "step 85 - loss 68.48744201660156 - moving ave loss 71.63349727764304\n",
            "step 86 - loss 68.10057067871094 - moving ave loss 71.28020461774985\n",
            "step 87 - loss 67.84032440185547 - moving ave loss 70.9362165961604\n",
            "step 88 - loss 67.19288635253906 - moving ave loss 70.56188357179828\n",
            "step 89 - loss 66.8595962524414 - moving ave loss 70.1916548398626\n",
            "step 90 - loss 66.59571838378906 - moving ave loss 69.83206119425525\n",
            "step 91 - loss 66.93534088134766 - moving ave loss 69.54238916296448\n",
            "step 92 - loss 65.85525512695312 - moving ave loss 69.17367575936335\n",
            "Finish 4 epoch(es)\n",
            "step 93 - loss 66.23883056640625 - moving ave loss 68.88019124006765\n",
            "step 94 - loss 64.97539520263672 - moving ave loss 68.48971163632456\n",
            "step 95 - loss 64.62557220458984 - moving ave loss 68.10329769315109\n",
            "step 96 - loss 64.04472351074219 - moving ave loss 67.6974402749102\n",
            "step 97 - loss 64.15086364746094 - moving ave loss 67.34278261216528\n",
            "step 98 - loss 63.34632873535156 - moving ave loss 66.9431372244839\n",
            "step 99 - loss 62.864967346191406 - moving ave loss 66.53532023665466\n",
            "step 100 - loss 63.084625244140625 - moving ave loss 66.19025073740326\n",
            "step 101 - loss 62.254188537597656 - moving ave loss 65.7966445174227\n",
            "step 102 - loss 62.94640350341797 - moving ave loss 65.51162041602224\n",
            "step 103 - loss 61.97283172607422 - moving ave loss 65.15774154702744\n",
            "step 104 - loss 61.58039093017578 - moving ave loss 64.80000648534228\n",
            "step 105 - loss 60.541996002197266 - moving ave loss 64.37420543702778\n",
            "step 106 - loss 60.94831085205078 - moving ave loss 64.03161597853008\n",
            "step 107 - loss 59.945133209228516 - moving ave loss 63.62296770159993\n",
            "step 108 - loss 59.75621795654297 - moving ave loss 63.23629272709424\n",
            "step 109 - loss 58.814781188964844 - moving ave loss 62.794141573281294\n",
            "step 110 - loss 58.91512680053711 - moving ave loss 62.40624009600688\n",
            "step 111 - loss 59.11485290527344 - moving ave loss 62.07710137693354\n",
            "step 112 - loss 59.35531997680664 - moving ave loss 61.80492323692085\n",
            "step 113 - loss 58.1348991394043 - moving ave loss 61.437920827169194\n",
            "step 114 - loss 57.4002799987793 - moving ave loss 61.034156744330204\n",
            "step 115 - loss 57.762203216552734 - moving ave loss 60.70696139155246\n",
            "Finish 5 epoch(es)\n",
            "step 116 - loss 57.81344985961914 - moving ave loss 60.41761023835913\n",
            "step 117 - loss 56.3728141784668 - moving ave loss 60.0131306323699\n",
            "step 118 - loss 56.13981628417969 - moving ave loss 59.625799197550876\n",
            "step 119 - loss 55.750885009765625 - moving ave loss 59.23830777877235\n",
            "step 120 - loss 55.3818473815918 - moving ave loss 58.8526617390543\n",
            "step 121 - loss 54.739768981933594 - moving ave loss 58.44137246334223\n",
            "step 122 - loss 54.99312210083008 - moving ave loss 58.096547427091025\n",
            "step 123 - loss 54.06907653808594 - moving ave loss 57.69380033819052\n",
            "step 124 - loss 53.48311233520508 - moving ave loss 57.27273153789197\n",
            "step 125 - loss 54.12488555908203 - moving ave loss 56.95794694001098\n",
            "Checkpoint at step 125\n",
            "step 126 - loss 52.74711608886719 - moving ave loss 56.5368638548966\n",
            "step 127 - loss 53.162139892578125 - moving ave loss 56.199391458664756\n",
            "step 128 - loss 52.09318161010742 - moving ave loss 55.78877047380902\n",
            "step 129 - loss 53.97534942626953 - moving ave loss 55.60742836905508\n",
            "step 130 - loss 52.89284133911133 - moving ave loss 55.335969666060706\n",
            "step 131 - loss 52.36243438720703 - moving ave loss 55.03861613817534\n",
            "step 132 - loss 51.66173553466797 - moving ave loss 54.70092807782461\n",
            "step 133 - loss 50.876224517822266 - moving ave loss 54.31845772182437\n",
            "step 134 - loss 51.78166961669922 - moving ave loss 54.06477891131186\n",
            "step 135 - loss 50.65933609008789 - moving ave loss 53.72423462918947\n",
            "step 136 - loss 50.36305236816406 - moving ave loss 53.388116403086926\n",
            "step 137 - loss 49.91639709472656 - moving ave loss 53.04094447225089\n",
            "step 138 - loss 50.283843994140625 - moving ave loss 52.76523442443987\n",
            "Finish 6 epoch(es)\n",
            "step 139 - loss 49.923622131347656 - moving ave loss 52.48107319513065\n",
            "step 140 - loss 49.00566864013672 - moving ave loss 52.133532739631264\n",
            "step 141 - loss 48.643211364746094 - moving ave loss 51.78450060214275\n",
            "step 142 - loss 48.31068420410156 - moving ave loss 51.43711896233863\n",
            "step 143 - loss 47.9305419921875 - moving ave loss 51.08646126532352\n",
            "step 144 - loss 48.211856842041016 - moving ave loss 50.799000822995275\n",
            "step 145 - loss 46.808982849121094 - moving ave loss 50.39999902560786\n",
            "step 146 - loss 47.718772888183594 - moving ave loss 50.131876411865434\n",
            "step 147 - loss 46.1769905090332 - moving ave loss 49.73638782158221\n",
            "step 148 - loss 46.82929229736328 - moving ave loss 49.445678269160325\n",
            "step 149 - loss 46.06033706665039 - moving ave loss 49.107144148909335\n",
            "step 150 - loss 45.5452766418457 - moving ave loss 48.75095739820297\n",
            "step 151 - loss 45.899269104003906 - moving ave loss 48.46578856878307\n",
            "step 152 - loss 44.926795959472656 - moving ave loss 48.11188930785203\n",
            "step 153 - loss 45.97930908203125 - moving ave loss 47.89863128526996\n",
            "step 154 - loss 44.972923278808594 - moving ave loss 47.60606048462382\n",
            "step 155 - loss 45.080848693847656 - moving ave loss 47.35353930554621\n",
            "step 156 - loss 43.72056579589844 - moving ave loss 46.990241954581435\n",
            "step 157 - loss 44.274322509765625 - moving ave loss 46.71865001009986\n",
            "step 158 - loss 42.60966491699219 - moving ave loss 46.307751500789095\n",
            "step 159 - loss 43.331695556640625 - moving ave loss 46.01014590637425\n",
            "step 160 - loss 42.017295837402344 - moving ave loss 45.61086089947705\n",
            "step 161 - loss 42.46607971191406 - moving ave loss 45.296382780720755\n",
            "Finish 7 epoch(es)\n",
            "step 162 - loss 42.79016876220703 - moving ave loss 45.04576137886938\n",
            "step 163 - loss 41.815940856933594 - moving ave loss 44.722779326675806\n",
            "step 164 - loss 42.67091369628906 - moving ave loss 44.51759276363713\n",
            "step 165 - loss 42.000213623046875 - moving ave loss 44.26585484957811\n",
            "step 166 - loss 40.83049774169922 - moving ave loss 43.92231913879022\n",
            "step 167 - loss 40.503143310546875 - moving ave loss 43.580401555965885\n",
            "step 168 - loss 40.894325256347656 - moving ave loss 43.31179392600406\n",
            "step 169 - loss 41.093238830566406 - moving ave loss 43.08993841646029\n",
            "step 170 - loss 39.774810791015625 - moving ave loss 42.75842565391583\n",
            "step 171 - loss 39.353309631347656 - moving ave loss 42.41791405165901\n",
            "step 172 - loss 40.304847717285156 - moving ave loss 42.20660741822163\n",
            "step 173 - loss 38.328590393066406 - moving ave loss 41.818805715706105\n",
            "step 174 - loss 40.87458038330078 - moving ave loss 41.72438318246557\n",
            "step 175 - loss 40.00248336791992 - moving ave loss 41.55219320101101\n",
            "step 176 - loss 38.14173126220703 - moving ave loss 41.21114700713061\n",
            "step 177 - loss 39.227806091308594 - moving ave loss 41.012812915548416\n",
            "step 178 - loss 37.81105422973633 - moving ave loss 40.69263704696721\n",
            "step 179 - loss 37.269248962402344 - moving ave loss 40.350298238510725\n",
            "step 180 - loss 39.1014518737793 - moving ave loss 40.225413602037584\n",
            "step 181 - loss 38.535125732421875 - moving ave loss 40.05638481507601\n",
            "step 182 - loss 37.42431640625 - moving ave loss 39.793177974193405\n",
            "step 183 - loss 35.66973876953125 - moving ave loss 39.38083405372719\n",
            "step 184 - loss 35.91472625732422 - moving ave loss 39.0342232740869\n",
            "Finish 8 epoch(es)\n",
            "step 185 - loss 35.86227798461914 - moving ave loss 38.71702874514012\n",
            "step 186 - loss 36.627254486083984 - moving ave loss 38.508051319234504\n",
            "step 187 - loss 36.02234649658203 - moving ave loss 38.25948083696925\n",
            "step 188 - loss 35.26765441894531 - moving ave loss 37.960298195166864\n",
            "step 189 - loss 35.31005859375 - moving ave loss 37.69527423502518\n",
            "step 190 - loss 35.2985954284668 - moving ave loss 37.455606354369344\n",
            "step 191 - loss 34.65940856933594 - moving ave loss 37.175986575866006\n",
            "step 192 - loss 34.08047866821289 - moving ave loss 36.8664357851007\n",
            "step 193 - loss 37.27830505371094 - moving ave loss 36.90762271196172\n",
            "step 194 - loss 34.842872619628906 - moving ave loss 36.70114770272844\n",
            "step 195 - loss 35.20170593261719 - moving ave loss 36.55120352571731\n",
            "step 196 - loss 33.27427673339844 - moving ave loss 36.223510846485425\n",
            "step 197 - loss 32.76624298095703 - moving ave loss 35.87778405993259\n",
            "step 198 - loss 32.392173767089844 - moving ave loss 35.52922303064832\n",
            "step 199 - loss 32.704341888427734 - moving ave loss 35.246734916426256\n",
            "step 200 - loss 32.41785430908203 - moving ave loss 34.963846855691834\n",
            "step 201 - loss 33.6893196105957 - moving ave loss 34.836394131182224\n",
            "step 202 - loss 32.24700927734375 - moving ave loss 34.57745564579838\n",
            "step 203 - loss 32.41935348510742 - moving ave loss 34.361645429729286\n",
            "step 204 - loss 31.99396324157715 - moving ave loss 34.12487721091407\n",
            "step 205 - loss 32.01093292236328 - moving ave loss 33.91348278205899\n",
            "step 206 - loss 33.16413497924805 - moving ave loss 33.838548001777895\n",
            "step 207 - loss 32.38357162475586 - moving ave loss 33.69305036407569\n",
            "Finish 9 epoch(es)\n",
            "step 208 - loss 31.519519805908203 - moving ave loss 33.475697308258944\n",
            "step 209 - loss 32.72827911376953 - moving ave loss 33.400955488810006\n",
            "step 210 - loss 31.477529525756836 - moving ave loss 33.20861289250469\n",
            "step 211 - loss 29.792442321777344 - moving ave loss 32.86699583543195\n",
            "step 212 - loss 30.100191116333008 - moving ave loss 32.59031536352206\n",
            "step 213 - loss 30.979766845703125 - moving ave loss 32.42926051174017\n",
            "step 214 - loss 29.99608039855957 - moving ave loss 32.18594250042211\n",
            "step 215 - loss 30.298110961914062 - moving ave loss 31.997159346571305\n",
            "step 216 - loss 29.408920288085938 - moving ave loss 31.738335440722768\n",
            "step 217 - loss 28.91948890686035 - moving ave loss 31.456450787336525\n",
            "step 218 - loss 29.67582130432129 - moving ave loss 31.278387839035005\n",
            "step 219 - loss 29.7596435546875 - moving ave loss 31.126513410600257\n",
            "step 220 - loss 28.53630256652832 - moving ave loss 30.867492326193062\n",
            "step 221 - loss 28.22589683532715 - moving ave loss 30.603332777106473\n",
            "step 222 - loss 28.241195678710938 - moving ave loss 30.36711906726692\n",
            "step 223 - loss 28.70108413696289 - moving ave loss 30.20051557423652\n",
            "step 224 - loss 28.56165313720703 - moving ave loss 30.03662933053357\n",
            "step 225 - loss 27.77904510498047 - moving ave loss 29.810870907978263\n",
            "step 226 - loss 29.556983947753906 - moving ave loss 29.78548221195583\n",
            "step 227 - loss 27.193069458007812 - moving ave loss 29.52624093656103\n",
            "step 228 - loss 27.985353469848633 - moving ave loss 29.372152189889793\n",
            "step 229 - loss 27.373851776123047 - moving ave loss 29.172322148513118\n",
            "step 230 - loss 26.444194793701172 - moving ave loss 28.899509413031925\n",
            "Finish 10 epoch(es)\n",
            "step 231 - loss 27.54769515991211 - moving ave loss 28.764327987719945\n",
            "step 232 - loss 26.992883682250977 - moving ave loss 28.58718355717305\n",
            "step 233 - loss 26.760940551757812 - moving ave loss 28.404559256631526\n",
            "step 234 - loss 26.126731872558594 - moving ave loss 28.17677651822423\n",
            "step 235 - loss 25.568317413330078 - moving ave loss 27.915930607734815\n",
            "step 236 - loss 27.025005340576172 - moving ave loss 27.826838081018952\n",
            "step 237 - loss 26.161056518554688 - moving ave loss 27.66025992477253\n",
            "step 238 - loss 25.652074813842773 - moving ave loss 27.459441413679553\n",
            "step 239 - loss 26.23288917541504 - moving ave loss 27.336786189853104\n",
            "step 240 - loss 25.10154151916504 - moving ave loss 27.1132617227843\n",
            "step 241 - loss 25.168376922607422 - moving ave loss 26.918773242766616\n",
            "step 242 - loss 25.714433670043945 - moving ave loss 26.79833928549435\n",
            "step 243 - loss 24.110061645507812 - moving ave loss 26.529511521495696\n",
            "step 244 - loss 24.611684799194336 - moving ave loss 26.33772884926556\n",
            "step 245 - loss 24.02663803100586 - moving ave loss 26.106619767439593\n",
            "step 246 - loss 24.762310028076172 - moving ave loss 25.97218879350325\n",
            "step 247 - loss 24.868488311767578 - moving ave loss 25.861818745329682\n",
            "step 248 - loss 23.888168334960938 - moving ave loss 25.664453704292807\n",
            "step 249 - loss 24.136913299560547 - moving ave loss 25.51169966381958\n",
            "step 250 - loss 24.64890480041504 - moving ave loss 25.425420177479126\n",
            "Checkpoint at step 250\n",
            "step 251 - loss 23.65958595275879 - moving ave loss 25.24883675500709\n",
            "step 252 - loss 24.384174346923828 - moving ave loss 25.162370514198766\n",
            "step 253 - loss 23.01189613342285 - moving ave loss 24.947323076121176\n",
            "Finish 11 epoch(es)\n",
            "step 254 - loss 23.529699325561523 - moving ave loss 24.80556070106521\n",
            "step 255 - loss 23.68185043334961 - moving ave loss 24.693189674293652\n",
            "step 256 - loss 24.636333465576172 - moving ave loss 24.687504053421904\n",
            "step 257 - loss 22.544586181640625 - moving ave loss 24.473212266243777\n",
            "step 258 - loss 22.152376174926758 - moving ave loss 24.241128657112075\n",
            "step 259 - loss 21.666053771972656 - moving ave loss 23.98362116859813\n",
            "step 260 - loss 22.6981201171875 - moving ave loss 23.855071063457068\n",
            "step 261 - loss 23.429813385009766 - moving ave loss 23.81254529561234\n",
            "step 262 - loss 22.024192810058594 - moving ave loss 23.633710047056965\n",
            "step 263 - loss 22.708358764648438 - moving ave loss 23.541174918816115\n",
            "step 264 - loss 21.59579849243164 - moving ave loss 23.34663727617767\n",
            "step 265 - loss 21.332366943359375 - moving ave loss 23.145210242895843\n",
            "step 266 - loss 21.698810577392578 - moving ave loss 23.000570276345517\n",
            "step 267 - loss 20.76897430419922 - moving ave loss 22.777410679130888\n",
            "step 268 - loss 20.814741134643555 - moving ave loss 22.581143724682157\n",
            "step 269 - loss 21.91689109802246 - moving ave loss 22.51471846201619\n",
            "step 270 - loss 22.03864097595215 - moving ave loss 22.467110713409788\n",
            "step 271 - loss 21.03818130493164 - moving ave loss 22.324217772561973\n",
            "step 272 - loss 20.53127670288086 - moving ave loss 22.14492366559386\n",
            "step 273 - loss 20.027238845825195 - moving ave loss 21.933155183616993\n",
            "step 274 - loss 20.92965316772461 - moving ave loss 21.832804982027753\n",
            "step 275 - loss 20.968612670898438 - moving ave loss 21.746385750914822\n",
            "step 276 - loss 19.475706100463867 - moving ave loss 21.519317785869728\n",
            "Finish 12 epoch(es)\n",
            "step 277 - loss 21.018795013427734 - moving ave loss 21.469265508625526\n",
            "step 278 - loss 19.678325653076172 - moving ave loss 21.29017152307059\n",
            "step 279 - loss 19.848766326904297 - moving ave loss 21.146031003453963\n",
            "step 280 - loss 19.17599105834961 - moving ave loss 20.949027008943528\n",
            "step 281 - loss 19.113239288330078 - moving ave loss 20.765448236882182\n",
            "step 282 - loss 19.573925018310547 - moving ave loss 20.64629591502502\n",
            "step 283 - loss 19.700166702270508 - moving ave loss 20.55168299374957\n",
            "step 284 - loss 19.493328094482422 - moving ave loss 20.445847503822858\n",
            "step 285 - loss 18.717744827270508 - moving ave loss 20.273037236167625\n",
            "step 286 - loss 19.385265350341797 - moving ave loss 20.18426004758504\n",
            "step 287 - loss 18.75644302368164 - moving ave loss 20.041478345194705\n",
            "step 288 - loss 18.760086059570312 - moving ave loss 19.91333911663227\n",
            "step 289 - loss 19.62392234802246 - moving ave loss 19.884397439771288\n",
            "step 290 - loss 18.500951766967773 - moving ave loss 19.746052872490935\n",
            "step 291 - loss 18.26865005493164 - moving ave loss 19.598312590735006\n",
            "step 292 - loss 18.697528839111328 - moving ave loss 19.50823421557264\n",
            "step 293 - loss 18.895278930664062 - moving ave loss 19.446938687081783\n",
            "step 294 - loss 18.619810104370117 - moving ave loss 19.364225828810618\n",
            "step 295 - loss 17.35629653930664 - moving ave loss 19.163432899860222\n",
            "step 296 - loss 18.400999069213867 - moving ave loss 19.08718951679559\n",
            "step 297 - loss 17.665908813476562 - moving ave loss 18.94506144646369\n",
            "step 298 - loss 17.211307525634766 - moving ave loss 18.771686054380798\n",
            "step 299 - loss 17.713211059570312 - moving ave loss 18.66583855489975\n",
            "Finish 13 epoch(es)\n",
            "step 300 - loss 16.879764556884766 - moving ave loss 18.487231155098254\n",
            "step 301 - loss 17.120559692382812 - moving ave loss 18.350564008826712\n",
            "step 302 - loss 17.733856201171875 - moving ave loss 18.28889322806123\n",
            "step 303 - loss 18.237380981445312 - moving ave loss 18.283742003399638\n",
            "step 304 - loss 16.671871185302734 - moving ave loss 18.12255492158995\n",
            "step 305 - loss 16.17394256591797 - moving ave loss 17.927693686022753\n",
            "step 306 - loss 16.88311767578125 - moving ave loss 17.8232360849986\n",
            "step 307 - loss 16.117202758789062 - moving ave loss 17.65263275237765\n",
            "step 308 - loss 16.798282623291016 - moving ave loss 17.567197739468988\n",
            "step 309 - loss 16.46316909790039 - moving ave loss 17.456794875312127\n",
            "step 310 - loss 16.766359329223633 - moving ave loss 17.387751320703277\n",
            "step 311 - loss 17.631744384765625 - moving ave loss 17.41215062710951\n",
            "step 312 - loss 16.792325973510742 - moving ave loss 17.350168161749636\n",
            "step 313 - loss 16.375835418701172 - moving ave loss 17.252734887444788\n",
            "step 314 - loss 16.314655303955078 - moving ave loss 17.158926929095816\n",
            "step 315 - loss 15.818549156188965 - moving ave loss 17.024889151805134\n",
            "step 316 - loss 15.746893882751465 - moving ave loss 16.897089624899767\n",
            "step 317 - loss 15.654772758483887 - moving ave loss 16.772857938258177\n",
            "step 318 - loss 16.225921630859375 - moving ave loss 16.718164307518297\n",
            "step 319 - loss 15.30472183227539 - moving ave loss 16.576820059994006\n",
            "step 320 - loss 15.338638305664062 - moving ave loss 16.45300188456101\n",
            "step 321 - loss 14.992979049682617 - moving ave loss 16.306999601073173\n",
            "step 322 - loss 16.171070098876953 - moving ave loss 16.29340665085355\n",
            "Finish 14 epoch(es)\n",
            "step 323 - loss 15.114852905273438 - moving ave loss 16.17555127629554\n",
            "step 324 - loss 15.108688354492188 - moving ave loss 16.068864984115205\n",
            "step 325 - loss 15.1476411819458 - moving ave loss 15.976742603898266\n",
            "step 326 - loss 14.744773864746094 - moving ave loss 15.853545729983049\n",
            "step 327 - loss 14.922029495239258 - moving ave loss 15.76039410650867\n",
            "step 328 - loss 14.529121398925781 - moving ave loss 15.637266835750383\n",
            "step 329 - loss 15.367255210876465 - moving ave loss 15.610265673262992\n",
            "step 330 - loss 14.609702110290527 - moving ave loss 15.510209316965746\n",
            "step 331 - loss 14.062625885009766 - moving ave loss 15.365450973770148\n",
            "step 332 - loss 13.761197090148926 - moving ave loss 15.205025585408025\n",
            "step 333 - loss 14.503877639770508 - moving ave loss 15.134910790844275\n",
            "step 334 - loss 14.128569602966309 - moving ave loss 15.034276672056478\n",
            "step 335 - loss 14.608707427978516 - moving ave loss 14.991719747648682\n",
            "step 336 - loss 13.685667037963867 - moving ave loss 14.8611144766802\n",
            "step 337 - loss 13.991432189941406 - moving ave loss 14.774146248006321\n",
            "step 338 - loss 14.073089599609375 - moving ave loss 14.704040583166627\n",
            "step 339 - loss 13.478714942932129 - moving ave loss 14.581508019143179\n",
            "step 340 - loss 13.84184455871582 - moving ave loss 14.507541673100443\n",
            "step 341 - loss 13.763601303100586 - moving ave loss 14.433147636100458\n",
            "step 342 - loss 14.218181610107422 - moving ave loss 14.411651033501155\n",
            "step 343 - loss 13.720887184143066 - moving ave loss 14.342574648565346\n",
            "step 344 - loss 13.744159698486328 - moving ave loss 14.282733153557444\n",
            "step 345 - loss 13.110023498535156 - moving ave loss 14.165462188055216\n",
            "Finish 15 epoch(es)\n",
            "step 346 - loss 13.866758346557617 - moving ave loss 14.135591803905456\n",
            "step 347 - loss 13.045289993286133 - moving ave loss 14.026561622843525\n",
            "step 348 - loss 12.652482032775879 - moving ave loss 13.889153663836762\n",
            "step 349 - loss 13.860745429992676 - moving ave loss 13.886312840452353\n",
            "step 350 - loss 12.904767990112305 - moving ave loss 13.788158355418348\n",
            "step 351 - loss 13.397270202636719 - moving ave loss 13.749069540140185\n",
            "step 352 - loss 12.978753089904785 - moving ave loss 13.672037895116645\n",
            "step 353 - loss 12.835884094238281 - moving ave loss 13.588422515028808\n",
            "step 354 - loss 12.307782173156738 - moving ave loss 13.460358480841602\n",
            "step 355 - loss 12.24986457824707 - moving ave loss 13.33930909058215\n",
            "step 356 - loss 12.743001937866211 - moving ave loss 13.279678375310557\n",
            "step 357 - loss 13.236510276794434 - moving ave loss 13.275361565458944\n",
            "step 358 - loss 12.131597518920898 - moving ave loss 13.16098516080514\n",
            "step 359 - loss 12.465965270996094 - moving ave loss 13.091483171824237\n",
            "step 360 - loss 11.801139831542969 - moving ave loss 12.96244883779611\n",
            "step 361 - loss 12.15357780456543 - moving ave loss 12.881561734473042\n",
            "step 362 - loss 12.211055755615234 - moving ave loss 12.814511136587262\n",
            "step 363 - loss 12.49404239654541 - moving ave loss 12.782464262583078\n",
            "step 364 - loss 11.743953704833984 - moving ave loss 12.67861320680817\n",
            "step 365 - loss 11.578041076660156 - moving ave loss 12.568555993793368\n",
            "step 366 - loss 11.698980331420898 - moving ave loss 12.48159842755612\n",
            "step 367 - loss 11.230012893676758 - moving ave loss 12.356439874168183\n",
            "step 368 - loss 11.629753112792969 - moving ave loss 12.283771198030662\n",
            "Finish 16 epoch(es)\n",
            "step 369 - loss 12.00722885131836 - moving ave loss 12.256116963359432\n",
            "step 370 - loss 11.163331985473633 - moving ave loss 12.146838465570852\n",
            "step 371 - loss 11.096673965454102 - moving ave loss 12.041822015559177\n",
            "step 372 - loss 11.964479446411133 - moving ave loss 12.034087758644372\n",
            "step 373 - loss 11.292913436889648 - moving ave loss 11.9599703264689\n",
            "step 374 - loss 11.591049194335938 - moving ave loss 11.923078213255604\n",
            "step 375 - loss 11.09909439086914 - moving ave loss 11.840679831016958\n",
            "Checkpoint at step 375\n",
            "step 376 - loss 11.087858200073242 - moving ave loss 11.765397667922587\n",
            "step 377 - loss 11.408933639526367 - moving ave loss 11.729751265082966\n",
            "step 378 - loss 10.679031372070312 - moving ave loss 11.6246792757817\n",
            "step 379 - loss 11.400964736938477 - moving ave loss 11.602307821897378\n",
            "step 380 - loss 11.354708671569824 - moving ave loss 11.577547906864623\n",
            "step 381 - loss 10.737935066223145 - moving ave loss 11.493586622800475\n",
            "step 382 - loss 10.713752746582031 - moving ave loss 11.415603235178631\n",
            "step 383 - loss 11.146526336669922 - moving ave loss 11.38869554532776\n",
            "step 384 - loss 10.577079772949219 - moving ave loss 11.307533968089906\n",
            "step 385 - loss 10.461213111877441 - moving ave loss 11.22290188246866\n",
            "step 386 - loss 10.721906661987305 - moving ave loss 11.172802360420524\n",
            "step 387 - loss 10.198320388793945 - moving ave loss 11.075354163257867\n",
            "step 388 - loss 10.368059158325195 - moving ave loss 11.0046246627646\n",
            "step 389 - loss 10.773662567138672 - moving ave loss 10.981528453202008\n",
            "step 390 - loss 9.81789493560791 - moving ave loss 10.8651651014426\n",
            "step 391 - loss 9.782344818115234 - moving ave loss 10.756883073109863\n",
            "Finish 17 epoch(es)\n",
            "step 392 - loss 10.226194381713867 - moving ave loss 10.703814203970262\n",
            "step 393 - loss 9.787633895874023 - moving ave loss 10.612196173160639\n",
            "step 394 - loss 9.735249519348145 - moving ave loss 10.524501507779389\n",
            "step 395 - loss 10.11545467376709 - moving ave loss 10.48359682437816\n",
            "step 396 - loss 9.86598014831543 - moving ave loss 10.421835156771888\n",
            "step 397 - loss 9.4932861328125 - moving ave loss 10.32898025437595\n",
            "step 398 - loss 9.803019523620605 - moving ave loss 10.276384181300415\n",
            "step 399 - loss 10.19599723815918 - moving ave loss 10.268345486986293\n",
            "step 400 - loss 9.958576202392578 - moving ave loss 10.237368558526923\n",
            "step 401 - loss 9.231689453125 - moving ave loss 10.136800647986732\n",
            "step 402 - loss 9.445496559143066 - moving ave loss 10.067670239102366\n",
            "step 403 - loss 9.562053680419922 - moving ave loss 10.017108583234123\n",
            "step 404 - loss 10.08867073059082 - moving ave loss 10.024264797969794\n",
            "step 405 - loss 10.108312606811523 - moving ave loss 10.032669578853968\n",
            "step 406 - loss 9.50125503540039 - moving ave loss 9.979528124508612\n",
            "step 407 - loss 10.223892211914062 - moving ave loss 10.003964533249158\n",
            "step 408 - loss 9.16318130493164 - moving ave loss 9.919886210417406\n",
            "step 409 - loss 9.205077171325684 - moving ave loss 9.848405306508235\n",
            "step 410 - loss 9.051741600036621 - moving ave loss 9.768738935861075\n",
            "step 411 - loss 9.286725997924805 - moving ave loss 9.720537642067448\n",
            "step 412 - loss 9.347770690917969 - moving ave loss 9.683260946952501\n",
            "step 413 - loss 9.403759956359863 - moving ave loss 9.655310847893237\n",
            "step 414 - loss 8.560624122619629 - moving ave loss 9.545842175365877\n",
            "Finish 18 epoch(es)\n",
            "step 415 - loss 8.987923622131348 - moving ave loss 9.490050320042425\n",
            "step 416 - loss 8.446575164794922 - moving ave loss 9.385702804517674\n",
            "step 417 - loss 8.730710983276367 - moving ave loss 9.320203622393544\n",
            "step 418 - loss 10.317495346069336 - moving ave loss 9.419932794761124\n",
            "step 419 - loss 9.476231575012207 - moving ave loss 9.425562672786233\n",
            "step 420 - loss 8.604480743408203 - moving ave loss 9.343454479848429\n",
            "step 421 - loss 8.592751502990723 - moving ave loss 9.26838418216266\n",
            "step 422 - loss 8.469402313232422 - moving ave loss 9.188485995269636\n",
            "step 423 - loss 8.764474868774414 - moving ave loss 9.146084882620114\n",
            "step 424 - loss 8.52484130859375 - moving ave loss 9.083960525217478\n",
            "step 425 - loss 8.39858341217041 - moving ave loss 9.015422813912773\n",
            "step 426 - loss 8.260860443115234 - moving ave loss 8.939966576833019\n",
            "step 427 - loss 8.125395774841309 - moving ave loss 8.858509496633848\n",
            "step 428 - loss 8.328592300415039 - moving ave loss 8.805517777011968\n",
            "step 429 - loss 8.312722206115723 - moving ave loss 8.756238219922343\n",
            "step 430 - loss 8.402114868164062 - moving ave loss 8.720825884746516\n",
            "step 431 - loss 8.345006942749023 - moving ave loss 8.683243990546767\n",
            "step 432 - loss 8.176254272460938 - moving ave loss 8.632545018738185\n",
            "step 433 - loss 8.39744758605957 - moving ave loss 8.609035275470323\n",
            "step 434 - loss 8.203226089477539 - moving ave loss 8.568454356871044\n",
            "step 435 - loss 7.958911895751953 - moving ave loss 8.507500110759135\n",
            "step 436 - loss 7.997271537780762 - moving ave loss 8.456477253461298\n",
            "step 437 - loss 7.972614288330078 - moving ave loss 8.408090956948175\n",
            "Finish 19 epoch(es)\n",
            "step 438 - loss 7.657523155212402 - moving ave loss 8.333034176774598\n",
            "step 439 - loss 8.563474655151367 - moving ave loss 8.356078224612276\n",
            "step 440 - loss 7.500335693359375 - moving ave loss 8.270503971486985\n",
            "step 441 - loss 8.18177604675293 - moving ave loss 8.261631179013579\n",
            "step 442 - loss 7.839670658111572 - moving ave loss 8.21943512692338\n",
            "step 443 - loss 7.907895565032959 - moving ave loss 8.188281170734337\n",
            "step 444 - loss 7.417176246643066 - moving ave loss 8.11117067832521\n",
            "step 445 - loss 8.379254341125488 - moving ave loss 8.137979044605238\n",
            "step 446 - loss 7.814356803894043 - moving ave loss 8.105616820534118\n",
            "step 447 - loss 7.204776763916016 - moving ave loss 8.015532814872309\n",
            "step 448 - loss 7.5073137283325195 - moving ave loss 7.96471090621833\n",
            "step 449 - loss 7.219629764556885 - moving ave loss 7.890202792052185\n",
            "step 450 - loss 7.746308326721191 - moving ave loss 7.875813345519085\n",
            "step 451 - loss 7.674240589141846 - moving ave loss 7.855656069881362\n",
            "step 452 - loss 7.483173370361328 - moving ave loss 7.818407799929359\n",
            "step 453 - loss 6.91450309753418 - moving ave loss 7.728017329689841\n",
            "step 454 - loss 6.929748058319092 - moving ave loss 7.648190402552767\n",
            "step 455 - loss 7.328804969787598 - moving ave loss 7.61625185927625\n",
            "step 456 - loss 7.28145694732666 - moving ave loss 7.582772368081292\n",
            "step 457 - loss 7.210183143615723 - moving ave loss 7.545513445634735\n",
            "step 458 - loss 7.493585586547852 - moving ave loss 7.5403206597260475\n",
            "step 459 - loss 7.360874176025391 - moving ave loss 7.522376011355982\n",
            "step 460 - loss 7.94675350189209 - moving ave loss 7.564813760409593\n",
            "Finish 20 epoch(es)\n",
            "step 461 - loss 6.973202705383301 - moving ave loss 7.505652654906964\n",
            "step 462 - loss 6.89378547668457 - moving ave loss 7.444465937084725\n",
            "step 463 - loss 7.549012184143066 - moving ave loss 7.4549205617905585\n",
            "step 464 - loss 6.852494239807129 - moving ave loss 7.394677929592216\n",
            "step 465 - loss 6.878707408905029 - moving ave loss 7.343080877523498\n",
            "step 466 - loss 7.103885650634766 - moving ave loss 7.319161354834625\n",
            "step 467 - loss 6.916479587554932 - moving ave loss 7.278893178106656\n",
            "step 468 - loss 7.514609336853027 - moving ave loss 7.302464793981294\n",
            "step 469 - loss 7.121603012084961 - moving ave loss 7.284378615791661\n",
            "step 470 - loss 7.097781181335449 - moving ave loss 7.2657188723460395\n",
            "step 471 - loss 6.620747089385986 - moving ave loss 7.201221694050035\n",
            "step 472 - loss 7.107548236846924 - moving ave loss 7.1918543483297235\n",
            "step 473 - loss 6.791220664978027 - moving ave loss 7.151790979994555\n",
            "step 474 - loss 6.3265228271484375 - moving ave loss 7.069264164709943\n",
            "step 475 - loss 6.318176746368408 - moving ave loss 6.9941554228757905\n",
            "step 476 - loss 6.921755790710449 - moving ave loss 6.986915459659257\n",
            "step 477 - loss 6.817715167999268 - moving ave loss 6.969995430493258\n",
            "step 478 - loss 6.906134605407715 - moving ave loss 6.963609347984704\n",
            "step 479 - loss 5.960877418518066 - moving ave loss 6.8633361550380405\n",
            "step 480 - loss 6.1131486892700195 - moving ave loss 6.788317408461239\n",
            "step 481 - loss 5.972816467285156 - moving ave loss 6.706767314343631\n",
            "step 482 - loss 6.323689937591553 - moving ave loss 6.668459576668424\n",
            "step 483 - loss 6.352407455444336 - moving ave loss 6.636854364546015\n",
            "Finish 21 epoch(es)\n",
            "step 484 - loss 6.486711025238037 - moving ave loss 6.621840030615218\n",
            "step 485 - loss 6.339407444000244 - moving ave loss 6.59359677195372\n",
            "step 486 - loss 6.71766471862793 - moving ave loss 6.606003566621141\n",
            "step 487 - loss 6.091506004333496 - moving ave loss 6.5545538103923775\n",
            "step 488 - loss 6.536715984344482 - moving ave loss 6.552770027787588\n",
            "step 489 - loss 6.443380355834961 - moving ave loss 6.541831060592326\n",
            "step 490 - loss 6.315786361694336 - moving ave loss 6.5192265907025275\n",
            "step 491 - loss 6.5664472579956055 - moving ave loss 6.523948657431836\n",
            "step 492 - loss 6.4829607009887695 - moving ave loss 6.51984986178753\n",
            "step 493 - loss 5.901143550872803 - moving ave loss 6.457979230696058\n",
            "step 494 - loss 5.886857986450195 - moving ave loss 6.4008671062714715\n",
            "step 495 - loss 5.95436954498291 - moving ave loss 6.356217350142616\n",
            "step 496 - loss 6.869983196258545 - moving ave loss 6.4075939347542095\n",
            "step 497 - loss 5.887804985046387 - moving ave loss 6.355615039783427\n",
            "step 498 - loss 5.908207893371582 - moving ave loss 6.310874325142242\n",
            "step 499 - loss 6.161388397216797 - moving ave loss 6.295925732349698\n",
            "step 500 - loss 5.602646827697754 - moving ave loss 6.226597841884503\n",
            "Checkpoint at step 500\n",
            "step 501 - loss 5.8215789794921875 - moving ave loss 6.186095955645272\n",
            "step 502 - loss 5.7728071212768555 - moving ave loss 6.144767072208431\n",
            "step 503 - loss 5.834498405456543 - moving ave loss 6.113740205533243\n",
            "step 504 - loss 6.188786029815674 - moving ave loss 6.121244787961486\n",
            "step 505 - loss 5.682488918304443 - moving ave loss 6.077369200995783\n",
            "step 506 - loss 5.725855827331543 - moving ave loss 6.042217863629359\n",
            "Finish 22 epoch(es)\n",
            "step 507 - loss 5.717206001281738 - moving ave loss 6.009716677394597\n",
            "step 508 - loss 6.135340690612793 - moving ave loss 6.0222790787164175\n",
            "step 509 - loss 5.969620704650879 - moving ave loss 6.017013241309863\n",
            "step 510 - loss 5.518496513366699 - moving ave loss 5.967161568515547\n",
            "step 511 - loss 5.291210651397705 - moving ave loss 5.899566476803763\n",
            "step 512 - loss 5.8435821533203125 - moving ave loss 5.893968044455418\n",
            "step 513 - loss 5.733189582824707 - moving ave loss 5.877890198292347\n",
            "step 514 - loss 5.935084342956543 - moving ave loss 5.883609612758767\n",
            "step 515 - loss 5.735950469970703 - moving ave loss 5.868843698479961\n",
            "step 516 - loss 5.387988090515137 - moving ave loss 5.8207581376834785\n",
            "step 517 - loss 5.192476272583008 - moving ave loss 5.757929951173431\n",
            "step 518 - loss 5.694728374481201 - moving ave loss 5.751609793504208\n",
            "step 519 - loss 4.928256988525391 - moving ave loss 5.669274513006327\n",
            "step 520 - loss 4.905379295349121 - moving ave loss 5.592884991240607\n",
            "step 521 - loss 5.50757360458374 - moving ave loss 5.58435385257492\n",
            "step 522 - loss 5.25726842880249 - moving ave loss 5.551645310197677\n",
            "step 523 - loss 5.745423316955566 - moving ave loss 5.571023110873466\n",
            "step 524 - loss 5.2711181640625 - moving ave loss 5.5410326161923695\n",
            "step 525 - loss 6.160717964172363 - moving ave loss 5.603001150990369\n",
            "step 526 - loss 5.392611503601074 - moving ave loss 5.58196218625144\n",
            "step 527 - loss 5.164734363555908 - moving ave loss 5.540239403981888\n",
            "step 528 - loss 4.867190837860107 - moving ave loss 5.47293454736971\n",
            "step 529 - loss 5.375476837158203 - moving ave loss 5.463188776348559\n",
            "Finish 23 epoch(es)\n",
            "step 530 - loss 5.20828104019165 - moving ave loss 5.437698002732868\n",
            "step 531 - loss 5.492816925048828 - moving ave loss 5.443209894964464\n",
            "step 532 - loss 5.448412895202637 - moving ave loss 5.443730194988281\n",
            "step 533 - loss 5.119255542755127 - moving ave loss 5.411282729764966\n",
            "step 534 - loss 4.903374195098877 - moving ave loss 5.360491876298357\n",
            "step 535 - loss 5.521548271179199 - moving ave loss 5.376597515786441\n",
            "step 536 - loss 5.442917823791504 - moving ave loss 5.383229546586947\n",
            "step 537 - loss 4.942602157592773 - moving ave loss 5.339166807687531\n",
            "step 538 - loss 5.120151519775391 - moving ave loss 5.317265278896317\n",
            "step 539 - loss 4.608729362487793 - moving ave loss 5.246411687255464\n",
            "step 540 - loss 5.504899978637695 - moving ave loss 5.272260516393687\n",
            "step 541 - loss 4.471124649047852 - moving ave loss 5.192146929659104\n",
            "step 542 - loss 5.153850555419922 - moving ave loss 5.188317292235186\n",
            "step 543 - loss 5.511750221252441 - moving ave loss 5.220660585136912\n",
            "step 544 - loss 4.8194074630737305 - moving ave loss 5.180535272930594\n",
            "step 545 - loss 5.027589321136475 - moving ave loss 5.165240677751182\n",
            "step 546 - loss 4.875599384307861 - moving ave loss 5.13627654840685\n",
            "step 547 - loss 4.645439624786377 - moving ave loss 5.087192856044803\n",
            "step 548 - loss 4.966830730438232 - moving ave loss 5.075156643484147\n",
            "step 549 - loss 4.665351867675781 - moving ave loss 5.03417616590331\n",
            "step 550 - loss 4.791084289550781 - moving ave loss 5.009866978268057\n",
            "step 551 - loss 4.366898536682129 - moving ave loss 4.945570134109465\n",
            "step 552 - loss 5.283949851989746 - moving ave loss 4.979408105897493\n",
            "Finish 24 epoch(es)\n",
            "step 553 - loss 4.407894611358643 - moving ave loss 4.922256756443608\n",
            "step 554 - loss 4.684406280517578 - moving ave loss 4.898471708851005\n",
            "step 555 - loss 4.727010726928711 - moving ave loss 4.881325610658776\n",
            "step 556 - loss 4.6139445304870605 - moving ave loss 4.854587502641605\n",
            "step 557 - loss 4.8557047843933105 - moving ave loss 4.854699230816776\n",
            "step 558 - loss 4.516144752502441 - moving ave loss 4.820843782985342\n",
            "step 559 - loss 4.490819931030273 - moving ave loss 4.787841397789835\n",
            "step 560 - loss 4.980503082275391 - moving ave loss 4.807107566238391\n",
            "step 561 - loss 4.64322566986084 - moving ave loss 4.790719376600635\n",
            "step 562 - loss 4.85709285736084 - moving ave loss 4.797356724676655\n",
            "step 563 - loss 4.2739410400390625 - moving ave loss 4.745015156212896\n",
            "step 564 - loss 4.756435871124268 - moving ave loss 4.746157227704034\n",
            "step 565 - loss 4.949579238891602 - moving ave loss 4.76649942882279\n",
            "step 566 - loss 4.624819755554199 - moving ave loss 4.752331461495931\n",
            "step 567 - loss 5.243126392364502 - moving ave loss 4.8014109545827885\n",
            "step 568 - loss 5.0486602783203125 - moving ave loss 4.826135886956542\n",
            "step 569 - loss 4.36915397644043 - moving ave loss 4.780437695904931\n",
            "step 570 - loss 4.299032211303711 - moving ave loss 4.732297147444809\n",
            "step 571 - loss 4.832001686096191 - moving ave loss 4.742267601309948\n",
            "step 572 - loss 4.515861988067627 - moving ave loss 4.719627039985716\n",
            "step 573 - loss 3.997932195663452 - moving ave loss 4.64745755555349\n",
            "step 574 - loss 4.013650894165039 - moving ave loss 4.584076889414645\n",
            "step 575 - loss 4.497426986694336 - moving ave loss 4.5754118991426145\n",
            "Finish 25 epoch(es)\n",
            "step 576 - loss 4.542594909667969 - moving ave loss 4.57213020019515\n",
            "step 577 - loss 4.422394752502441 - moving ave loss 4.557156655425879\n",
            "step 578 - loss 4.995241641998291 - moving ave loss 4.60096515408312\n",
            "step 579 - loss 4.459525108337402 - moving ave loss 4.586821149508548\n",
            "step 580 - loss 4.27855110168457 - moving ave loss 4.55599414472615\n",
            "step 581 - loss 4.831614017486572 - moving ave loss 4.583556132002193\n",
            "step 582 - loss 4.545876502990723 - moving ave loss 4.579788169101046\n",
            "step 583 - loss 4.147027015686035 - moving ave loss 4.5365120537595445\n",
            "step 584 - loss 4.42012882232666 - moving ave loss 4.524873730616256\n",
            "step 585 - loss 4.302570343017578 - moving ave loss 4.502643391856388\n",
            "step 586 - loss 3.845686912536621 - moving ave loss 4.436947743924412\n",
            "step 587 - loss 4.269589424133301 - moving ave loss 4.420211911945301\n",
            "step 588 - loss 4.073179244995117 - moving ave loss 4.385508645250282\n",
            "step 589 - loss 4.366368293762207 - moving ave loss 4.383594610101475\n",
            "step 590 - loss 4.474539756774902 - moving ave loss 4.392689124768817\n",
            "step 591 - loss 4.134112358093262 - moving ave loss 4.366831448101261\n",
            "step 592 - loss 4.031926155090332 - moving ave loss 4.333340918800168\n",
            "step 593 - loss 4.2006611824035645 - moving ave loss 4.320072945160508\n",
            "step 594 - loss 4.45719051361084 - moving ave loss 4.333784702005541\n",
            "step 595 - loss 4.108147621154785 - moving ave loss 4.311220993920466\n",
            "step 596 - loss 3.8393871784210205 - moving ave loss 4.264037612370522\n",
            "step 597 - loss 3.6687252521514893 - moving ave loss 4.204506376348619\n",
            "step 598 - loss 3.884949207305908 - moving ave loss 4.172550659444348\n",
            "Finish 26 epoch(es)\n",
            "step 599 - loss 4.219377517700195 - moving ave loss 4.177233345269933\n",
            "step 600 - loss 3.952904462814331 - moving ave loss 4.154800457024373\n",
            "step 601 - loss 4.149022102355957 - moving ave loss 4.1542226215575315\n",
            "step 602 - loss 3.8089935779571533 - moving ave loss 4.119699717197494\n",
            "step 603 - loss 4.058323383331299 - moving ave loss 4.113562083810875\n",
            "step 604 - loss 4.102507591247559 - moving ave loss 4.112456634554543\n",
            "step 605 - loss 4.023221015930176 - moving ave loss 4.103533072692106\n",
            "step 606 - loss 4.096181869506836 - moving ave loss 4.102797952373579\n",
            "step 607 - loss 4.214990615844727 - moving ave loss 4.114017218720694\n",
            "step 608 - loss 4.049443244934082 - moving ave loss 4.107559821342033\n",
            "step 609 - loss 3.407160520553589 - moving ave loss 4.037519891263188\n",
            "step 610 - loss 3.6748123168945312 - moving ave loss 4.001249133826322\n",
            "step 611 - loss 3.9908063411712646 - moving ave loss 4.000204854560817\n",
            "step 612 - loss 3.963472843170166 - moving ave loss 3.996531653421752\n",
            "step 613 - loss 4.187047958374023 - moving ave loss 4.015583283916979\n",
            "step 614 - loss 3.4926815032958984 - moving ave loss 3.9632931058548713\n",
            "step 615 - loss 4.147238731384277 - moving ave loss 3.9816876684078117\n",
            "step 616 - loss 3.798346519470215 - moving ave loss 3.963353553514052\n",
            "step 617 - loss 3.662785291671753 - moving ave loss 3.933296727329822\n",
            "step 618 - loss 4.032548427581787 - moving ave loss 3.9432218973550186\n",
            "step 619 - loss 3.9275388717651367 - moving ave loss 3.9416535947960307\n",
            "step 620 - loss 4.140777587890625 - moving ave loss 3.96156599410549\n",
            "step 621 - loss 3.9930098056793213 - moving ave loss 3.9647103752628734\n",
            "Finish 27 epoch(es)\n",
            "step 622 - loss 3.2934670448303223 - moving ave loss 3.8975860422196185\n",
            "step 623 - loss 3.7131850719451904 - moving ave loss 3.8791459451921755\n",
            "step 624 - loss 4.050480842590332 - moving ave loss 3.8962794349319916\n",
            "step 625 - loss 3.904831886291504 - moving ave loss 3.8971346800679427\n",
            "Checkpoint at step 625\n",
            "step 626 - loss 4.157033920288086 - moving ave loss 3.9231246040899572\n",
            "step 627 - loss 3.9422481060028076 - moving ave loss 3.925036954281242\n",
            "step 628 - loss 4.339674949645996 - moving ave loss 3.9665007538177175\n",
            "step 629 - loss 3.8029589653015137 - moving ave loss 3.950146574966097\n",
            "step 630 - loss 3.5595462322235107 - moving ave loss 3.9110865406918385\n",
            "step 631 - loss 3.686340093612671 - moving ave loss 3.8886118959839218\n",
            "step 632 - loss 3.8696916103363037 - moving ave loss 3.8867198674191603\n",
            "step 633 - loss 4.601254940032959 - moving ave loss 3.9581733746805403\n",
            "step 634 - loss 3.6620984077453613 - moving ave loss 3.9285658779870225\n",
            "step 635 - loss 3.353759765625 - moving ave loss 3.8710852667508204\n",
            "step 636 - loss 3.520009994506836 - moving ave loss 3.835977739526422\n",
            "step 637 - loss 3.5325992107391357 - moving ave loss 3.8056398866476937\n",
            "step 638 - loss 3.1661853790283203 - moving ave loss 3.7416944358857567\n",
            "step 639 - loss 3.9474716186523438 - moving ave loss 3.7622721541624156\n",
            "step 640 - loss 3.545562505722046 - moving ave loss 3.7406011893183786\n",
            "step 641 - loss 4.336332321166992 - moving ave loss 3.80017430250324\n",
            "step 642 - loss 3.762146472930908 - moving ave loss 3.796371519546007\n",
            "step 643 - loss 3.45884370803833 - moving ave loss 3.762618738395239\n",
            "step 644 - loss 3.423797845840454 - moving ave loss 3.7287366491397607\n",
            "Finish 28 epoch(es)\n",
            "step 645 - loss 3.473801612854004 - moving ave loss 3.703243145511185\n",
            "step 646 - loss 3.3869924545288086 - moving ave loss 3.671618076412947\n",
            "step 647 - loss 3.5359861850738525 - moving ave loss 3.6580548872790377\n",
            "step 648 - loss 3.332001209259033 - moving ave loss 3.625449519477037\n",
            "step 649 - loss 3.6402335166931152 - moving ave loss 3.626927919198645\n",
            "step 650 - loss 3.8597302436828613 - moving ave loss 3.6502081516470666\n",
            "step 651 - loss 3.356111764907837 - moving ave loss 3.6207985129731437\n",
            "step 652 - loss 3.338087558746338 - moving ave loss 3.592527417550463\n",
            "step 653 - loss 3.854753255844116 - moving ave loss 3.618750001379828\n",
            "step 654 - loss 3.1788148880004883 - moving ave loss 3.574756490041894\n",
            "step 655 - loss 3.834806442260742 - moving ave loss 3.600761485263779\n",
            "step 656 - loss 4.079227447509766 - moving ave loss 3.6486080814883777\n",
            "step 657 - loss 3.6357808113098145 - moving ave loss 3.6473253544705213\n",
            "step 658 - loss 3.1661856174468994 - moving ave loss 3.599211380768159\n",
            "step 659 - loss 3.420816659927368 - moving ave loss 3.5813719086840803\n",
            "step 660 - loss 2.8826146125793457 - moving ave loss 3.511496179073607\n",
            "step 661 - loss 3.292205572128296 - moving ave loss 3.4895671183790755\n",
            "step 662 - loss 3.17862868309021 - moving ave loss 3.458473274850189\n",
            "step 663 - loss 3.7637715339660645 - moving ave loss 3.489003100761777\n",
            "step 664 - loss 3.70094633102417 - moving ave loss 3.5101974237880165\n",
            "step 665 - loss 4.193777084350586 - moving ave loss 3.5785553898442735\n",
            "step 666 - loss 3.215865135192871 - moving ave loss 3.5422863643791334\n",
            "step 667 - loss 3.2801308631896973 - moving ave loss 3.5160708142601895\n",
            "Finish 29 epoch(es)\n",
            "step 668 - loss 3.838949680328369 - moving ave loss 3.5483587008670074\n",
            "step 669 - loss 3.762453079223633 - moving ave loss 3.5697681387026705\n",
            "step 670 - loss 3.254319190979004 - moving ave loss 3.538223243930304\n",
            "step 671 - loss 3.1585307121276855 - moving ave loss 3.5002539907500423\n",
            "step 672 - loss 3.1538147926330566 - moving ave loss 3.465610070938344\n",
            "step 673 - loss 3.54998779296875 - moving ave loss 3.474047843141385\n",
            "step 674 - loss 3.2343485355377197 - moving ave loss 3.450077912381018\n",
            "step 675 - loss 3.04611873626709 - moving ave loss 3.4096819947696257\n",
            "step 676 - loss 3.0326504707336426 - moving ave loss 3.3719788423660275\n",
            "step 677 - loss 3.3578672409057617 - moving ave loss 3.370567682220001\n",
            "step 678 - loss 3.7466602325439453 - moving ave loss 3.408176937252396\n",
            "step 679 - loss 3.235666275024414 - moving ave loss 3.390925871029598\n",
            "step 680 - loss 3.728461742401123 - moving ave loss 3.424679458166751\n",
            "step 681 - loss 3.7731804847717285 - moving ave loss 3.459529560827249\n",
            "step 682 - loss 3.1530489921569824 - moving ave loss 3.428881503960222\n",
            "step 683 - loss 3.5356361865997314 - moving ave loss 3.4395569722241732\n",
            "step 684 - loss 2.972830295562744 - moving ave loss 3.3928843045580304\n",
            "step 685 - loss 3.355701208114624 - moving ave loss 3.38916599491369\n",
            "step 686 - loss 3.9108986854553223 - moving ave loss 3.4413392639678535\n",
            "step 687 - loss 2.619690179824829 - moving ave loss 3.359174355553551\n",
            "step 688 - loss 3.226979970932007 - moving ave loss 3.3459549170913965\n",
            "step 689 - loss 3.2646055221557617 - moving ave loss 3.3378199775978334\n",
            "step 690 - loss 3.0622470378875732 - moving ave loss 3.3102626836268074\n",
            "Finish 30 epoch(es)\n",
            "step 691 - loss 3.3428335189819336 - moving ave loss 3.31351976716232\n",
            "step 692 - loss 3.002938747406006 - moving ave loss 3.282461665186689\n",
            "step 693 - loss 2.799748420715332 - moving ave loss 3.2341903407395534\n",
            "step 694 - loss 3.0179014205932617 - moving ave loss 3.2125614487249243\n",
            "step 695 - loss 3.1570608615875244 - moving ave loss 3.2070113900111847\n",
            "step 696 - loss 3.2876408100128174 - moving ave loss 3.215074332011348\n",
            "step 697 - loss 3.1908679008483887 - moving ave loss 3.212653688895052\n",
            "step 698 - loss 3.800813913345337 - moving ave loss 3.271469711340081\n",
            "step 699 - loss 3.126655340194702 - moving ave loss 3.2569882742255434\n",
            "step 700 - loss 3.2237250804901123 - moving ave loss 3.2536619548520007\n",
            "step 701 - loss 3.2972958087921143 - moving ave loss 3.258025340246012\n",
            "step 702 - loss 2.871974229812622 - moving ave loss 3.2194202292026732\n",
            "step 703 - loss 3.21138858795166 - moving ave loss 3.218617065077572\n",
            "step 704 - loss 3.0453457832336426 - moving ave loss 3.201289936893179\n",
            "step 705 - loss 3.9626340866088867 - moving ave loss 3.2774243518647497\n",
            "step 706 - loss 2.8622260093688965 - moving ave loss 3.235904517615164\n",
            "step 707 - loss 3.310286521911621 - moving ave loss 3.24334271804481\n",
            "step 708 - loss 3.53326416015625 - moving ave loss 3.272334862255954\n",
            "step 709 - loss 2.8915514945983887 - moving ave loss 3.2342565254901974\n",
            "step 710 - loss 2.9899468421936035 - moving ave loss 3.209825557160538\n",
            "step 711 - loss 2.999948501586914 - moving ave loss 3.1888378516031755\n",
            "step 712 - loss 2.966395616531372 - moving ave loss 3.166593628095995\n",
            "step 713 - loss 3.222684144973755 - moving ave loss 3.172202679783771\n",
            "Finish 31 epoch(es)\n",
            "step 714 - loss 2.9004323482513428 - moving ave loss 3.145025646630528\n",
            "step 715 - loss 3.2913031578063965 - moving ave loss 3.159653397748115\n",
            "step 716 - loss 2.911198616027832 - moving ave loss 3.1348079195760867\n",
            "step 717 - loss 3.1533913612365723 - moving ave loss 3.1366662637421356\n",
            "step 718 - loss 3.0545854568481445 - moving ave loss 3.1284581830527367\n",
            "step 719 - loss 3.2872705459594727 - moving ave loss 3.14433941934341\n",
            "step 720 - loss 3.128939390182495 - moving ave loss 3.142799416427319\n",
            "step 721 - loss 2.7817625999450684 - moving ave loss 3.106695734779094\n",
            "step 722 - loss 2.683922290802002 - moving ave loss 3.064418390381385\n",
            "step 723 - loss 3.0254762172698975 - moving ave loss 3.0605241730702364\n",
            "step 724 - loss 2.7449867725372314 - moving ave loss 3.0289704330169362\n",
            "step 725 - loss 2.913543462753296 - moving ave loss 3.017427735990572\n",
            "step 726 - loss 3.778531551361084 - moving ave loss 3.0935381175276233\n",
            "step 727 - loss 2.795017719268799 - moving ave loss 3.063686077701741\n",
            "step 728 - loss 3.601633071899414 - moving ave loss 3.1174807771215085\n",
            "step 729 - loss 3.598731517791748 - moving ave loss 3.1656058511885328\n",
            "step 730 - loss 2.902576208114624 - moving ave loss 3.1393028868811417\n",
            "step 731 - loss 2.909086227416992 - moving ave loss 3.116281220934727\n",
            "step 732 - loss 2.589301824569702 - moving ave loss 3.0635832812982247\n",
            "step 733 - loss 3.083609104156494 - moving ave loss 3.065585863584052\n",
            "step 734 - loss 2.6962857246398926 - moving ave loss 3.0286558496896356\n",
            "step 735 - loss 2.8849034309387207 - moving ave loss 3.014280607814544\n",
            "step 736 - loss 2.7467613220214844 - moving ave loss 2.987528679235238\n",
            "Finish 32 epoch(es)\n",
            "step 737 - loss 2.8766674995422363 - moving ave loss 2.976442561265938\n",
            "step 738 - loss 2.716472625732422 - moving ave loss 2.9504455677125865\n",
            "step 739 - loss 3.33359956741333 - moving ave loss 2.9887609676826608\n",
            "step 740 - loss 3.2788758277893066 - moving ave loss 3.0177724536933255\n",
            "step 741 - loss 3.2654685974121094 - moving ave loss 3.042542068065204\n",
            "step 742 - loss 3.1903858184814453 - moving ave loss 3.0573264431068288\n",
            "step 743 - loss 3.2755844593048096 - moving ave loss 3.079152244726627\n",
            "step 744 - loss 2.9255404472351074 - moving ave loss 3.063791064977475\n",
            "step 745 - loss 3.434770345687866 - moving ave loss 3.1008889930485144\n",
            "step 746 - loss 2.756345272064209 - moving ave loss 3.066434620950084\n",
            "step 747 - loss 2.9440600872039795 - moving ave loss 3.0541971675754738\n",
            "step 748 - loss 2.7743983268737793 - moving ave loss 3.0262172835053045\n",
            "step 749 - loss 3.4459433555603027 - moving ave loss 3.0681898907108045\n",
            "step 750 - loss 2.816340446472168 - moving ave loss 3.043004946286941\n",
            "Checkpoint at step 750\n",
            "step 751 - loss 3.0281879901885986 - moving ave loss 3.041523250677107\n",
            "step 752 - loss 2.7132678031921387 - moving ave loss 3.00869770592861\n",
            "step 753 - loss 2.8844683170318604 - moving ave loss 2.9962747670389347\n",
            "step 754 - loss 3.109063148498535 - moving ave loss 3.007553605184895\n",
            "step 755 - loss 2.8329241275787354 - moving ave loss 2.990090657424279\n",
            "step 756 - loss 3.204310894012451 - moving ave loss 3.0115126810830963\n",
            "step 757 - loss 2.6805591583251953 - moving ave loss 2.978417328807306\n",
            "step 758 - loss 2.589930772781372 - moving ave loss 2.939568673204713\n",
            "step 759 - loss 2.619962215423584 - moving ave loss 2.9076080274266003\n",
            "Finish 33 epoch(es)\n",
            "step 760 - loss 3.149679183959961 - moving ave loss 2.9318151430799366\n",
            "step 761 - loss 2.9732704162597656 - moving ave loss 2.9359606703979195\n",
            "step 762 - loss 2.0874273777008057 - moving ave loss 2.851107341128208\n",
            "step 763 - loss 2.9638137817382812 - moving ave loss 2.8623779851892155\n",
            "step 764 - loss 2.653928279876709 - moving ave loss 2.8415330146579647\n",
            "step 765 - loss 3.032258987426758 - moving ave loss 2.860605611934844\n",
            "step 766 - loss 3.1947174072265625 - moving ave loss 2.894016791464016\n",
            "step 767 - loss 3.165499448776245 - moving ave loss 2.921165057195239\n",
            "step 768 - loss 2.5071284770965576 - moving ave loss 2.879761399185371\n",
            "step 769 - loss 2.6371288299560547 - moving ave loss 2.8554981422624395\n",
            "step 770 - loss 2.2802820205688477 - moving ave loss 2.7979765300930803\n",
            "step 771 - loss 3.5457334518432617 - moving ave loss 2.872752222268099\n",
            "step 772 - loss 2.7962169647216797 - moving ave loss 2.865098696513457\n",
            "step 773 - loss 3.2276577949523926 - moving ave loss 2.9013546063573505\n",
            "step 774 - loss 2.73877215385437 - moving ave loss 2.8850963611070526\n",
            "step 775 - loss 2.6487317085266113 - moving ave loss 2.8614598958490083\n",
            "step 776 - loss 2.818937301635742 - moving ave loss 2.857207636427682\n",
            "step 777 - loss 2.911101818084717 - moving ave loss 2.8625970545933854\n",
            "step 778 - loss 2.6459054946899414 - moving ave loss 2.840927898603041\n",
            "step 779 - loss 2.7113571166992188 - moving ave loss 2.827970820412659\n",
            "step 780 - loss 3.1268558502197266 - moving ave loss 2.857859323393366\n",
            "step 781 - loss 2.8056178092956543 - moving ave loss 2.8526351719835947\n",
            "step 782 - loss 2.770026683807373 - moving ave loss 2.844374323165973\n",
            "Finish 34 epoch(es)\n",
            "step 783 - loss 2.9378082752227783 - moving ave loss 2.8537177183716533\n",
            "step 784 - loss 2.5913383960723877 - moving ave loss 2.8274797861417267\n",
            "step 785 - loss 2.6299705505371094 - moving ave loss 2.807728862581265\n",
            "step 786 - loss 3.057795524597168 - moving ave loss 2.8327355287828557\n",
            "step 787 - loss 2.934593677520752 - moving ave loss 2.8429213436566454\n",
            "step 788 - loss 2.468080520629883 - moving ave loss 2.805437261353969\n",
            "step 789 - loss 2.5680572986602783 - moving ave loss 2.7816992650846\n",
            "step 790 - loss 2.494807243347168 - moving ave loss 2.753010062910857\n",
            "step 791 - loss 2.446168899536133 - moving ave loss 2.7223259465733847\n",
            "step 792 - loss 2.618283271789551 - moving ave loss 2.7119216790950014\n",
            "step 793 - loss 2.830526828765869 - moving ave loss 2.723782194062088\n",
            "step 794 - loss 3.305631160736084 - moving ave loss 2.781967090729488\n",
            "step 795 - loss 2.763852596282959 - moving ave loss 2.780155641284835\n",
            "step 796 - loss 2.938871383666992 - moving ave loss 2.796027215523051\n",
            "step 797 - loss 2.7846951484680176 - moving ave loss 2.794894008817548\n",
            "step 798 - loss 2.5619850158691406 - moving ave loss 2.771603109522707\n",
            "step 799 - loss 2.6715266704559326 - moving ave loss 2.76159546561603\n",
            "step 800 - loss 2.822824001312256 - moving ave loss 2.7677183191856525\n",
            "step 801 - loss 2.6951346397399902 - moving ave loss 2.7604599512410863\n",
            "step 802 - loss 2.7150399684906006 - moving ave loss 2.755917952966038\n",
            "step 803 - loss 2.669600486755371 - moving ave loss 2.7472862063449712\n",
            "step 804 - loss 3.083279848098755 - moving ave loss 2.7808855705203497\n",
            "step 805 - loss 2.7630696296691895 - moving ave loss 2.7791039764352337\n",
            "Finish 35 epoch(es)\n",
            "step 806 - loss 2.898639678955078 - moving ave loss 2.791057546687218\n",
            "step 807 - loss 2.7993879318237305 - moving ave loss 2.7918905852008695\n",
            "step 808 - loss 3.4644203186035156 - moving ave loss 2.8591435585411347\n",
            "step 809 - loss 2.5328145027160645 - moving ave loss 2.826510652958628\n",
            "step 810 - loss 2.8412222862243652 - moving ave loss 2.8279818162852015\n",
            "step 811 - loss 2.3778858184814453 - moving ave loss 2.782972216504826\n",
            "step 812 - loss 2.7840332984924316 - moving ave loss 2.7830783247035864\n",
            "step 813 - loss 2.5000853538513184 - moving ave loss 2.7547790276183597\n",
            "step 814 - loss 2.860877513885498 - moving ave loss 2.7653888762450736\n",
            "step 815 - loss 2.7791354656219482 - moving ave loss 2.7667635351827613\n",
            "step 816 - loss 2.5922956466674805 - moving ave loss 2.749316746331233\n",
            "step 817 - loss 2.593496799468994 - moving ave loss 2.7337347516450095\n",
            "step 818 - loss 3.262906074523926 - moving ave loss 2.786651883932901\n",
            "step 819 - loss 2.433675765991211 - moving ave loss 2.751354272138732\n",
            "step 820 - loss 2.2814178466796875 - moving ave loss 2.7043606295928275\n",
            "step 821 - loss 2.2373292446136475 - moving ave loss 2.6576574910949096\n",
            "step 822 - loss 2.115622043609619 - moving ave loss 2.6034539463463804\n",
            "step 823 - loss 3.0280661582946777 - moving ave loss 2.6459151675412103\n",
            "step 824 - loss 2.5259604454040527 - moving ave loss 2.6339196953274944\n",
            "step 825 - loss 2.936509609222412 - moving ave loss 2.664178686716986\n",
            "step 826 - loss 3.19924259185791 - moving ave loss 2.7176850772310783\n",
            "step 827 - loss 2.9008235931396484 - moving ave loss 2.7359989288219353\n",
            "step 828 - loss 2.5813584327697754 - moving ave loss 2.720534879216719\n",
            "Finish 36 epoch(es)\n",
            "step 829 - loss 2.3966457843780518 - moving ave loss 2.6881459697328522\n",
            "step 830 - loss 2.4753780364990234 - moving ave loss 2.6668691764094694\n",
            "step 831 - loss 2.893463134765625 - moving ave loss 2.6895285722450852\n",
            "step 832 - loss 2.865643262863159 - moving ave loss 2.707140041306893\n",
            "step 833 - loss 3.286360025405884 - moving ave loss 2.7650620397167924\n",
            "step 834 - loss 2.802265167236328 - moving ave loss 2.768782352468746\n",
            "step 835 - loss 2.6635549068450928 - moving ave loss 2.758259607906381\n",
            "step 836 - loss 2.2095303535461426 - moving ave loss 2.703386682470357\n",
            "step 837 - loss 2.7827372550964355 - moving ave loss 2.711321739732965\n",
            "step 838 - loss 2.8804731369018555 - moving ave loss 2.728236879449854\n",
            "step 839 - loss 2.5924437046051025 - moving ave loss 2.714657561965379\n",
            "step 840 - loss 2.434998035430908 - moving ave loss 2.6866916093119317\n",
            "step 841 - loss 2.0912106037139893 - moving ave loss 2.6271435087521375\n",
            "step 842 - loss 2.3332297801971436 - moving ave loss 2.5977521358966382\n",
            "step 843 - loss 2.33931040763855 - moving ave loss 2.5719079630708297\n",
            "step 844 - loss 2.6999599933624268 - moving ave loss 2.5847131660999896\n",
            "step 845 - loss 2.244565486907959 - moving ave loss 2.5506983981807867\n",
            "step 846 - loss 3.4886722564697266 - moving ave loss 2.644495784009681\n",
            "step 847 - loss 2.218003511428833 - moving ave loss 2.6018465567515965\n",
            "step 848 - loss 2.588503837585449 - moving ave loss 2.6005122848349815\n",
            "step 849 - loss 2.4853572845458984 - moving ave loss 2.5889967848060733\n",
            "step 850 - loss 2.598369598388672 - moving ave loss 2.589934066164333\n",
            "step 851 - loss 2.8067286014556885 - moving ave loss 2.611613519693469\n",
            "Finish 37 epoch(es)\n",
            "step 852 - loss 2.40315842628479 - moving ave loss 2.5907680103526007\n",
            "step 853 - loss 2.4775662422180176 - moving ave loss 2.579447833539142\n",
            "step 854 - loss 2.520690441131592 - moving ave loss 2.573572094298387\n",
            "step 855 - loss 2.494781494140625 - moving ave loss 2.5656930342826105\n",
            "step 856 - loss 2.7192554473876953 - moving ave loss 2.581049275593119\n",
            "step 857 - loss 2.8367583751678467 - moving ave loss 2.6066201855505917\n",
            "step 858 - loss 2.6676955223083496 - moving ave loss 2.6127277192263674\n",
            "step 859 - loss 2.6977195739746094 - moving ave loss 2.6212269047011914\n",
            "step 860 - loss 2.4897570610046387 - moving ave loss 2.6080799203315363\n",
            "step 861 - loss 2.6431825160980225 - moving ave loss 2.611590179908185\n",
            "step 862 - loss 2.639458179473877 - moving ave loss 2.6143769798647543\n",
            "step 863 - loss 2.4169623851776123 - moving ave loss 2.5946355203960403\n",
            "step 864 - loss 3.0697994232177734 - moving ave loss 2.642151910678214\n",
            "step 865 - loss 2.8362679481506348 - moving ave loss 2.661563514425456\n",
            "step 866 - loss 2.7823777198791504 - moving ave loss 2.673644934970826\n",
            "step 867 - loss 2.318087577819824 - moving ave loss 2.638089199255726\n",
            "step 868 - loss 2.5213427543640137 - moving ave loss 2.626414554766555\n",
            "step 869 - loss 2.8202433586120605 - moving ave loss 2.6457974351511058\n",
            "step 870 - loss 2.651709794998169 - moving ave loss 2.6463886711358118\n",
            "step 871 - loss 2.4857683181762695 - moving ave loss 2.630326635839858\n",
            "step 872 - loss 2.253817319869995 - moving ave loss 2.5926757042428714\n",
            "step 873 - loss 2.369089365005493 - moving ave loss 2.5703170703191334\n",
            "step 874 - loss 2.2690181732177734 - moving ave loss 2.540187180608997\n",
            "Finish 38 epoch(es)\n",
            "step 875 - loss 2.232496976852417 - moving ave loss 2.509418160233339\n",
            "Checkpoint at step 875\n",
            "step 876 - loss 2.884133815765381 - moving ave loss 2.5468897257865435\n",
            "step 877 - loss 2.5478315353393555 - moving ave loss 2.5469839067418247\n",
            "step 878 - loss 2.3618063926696777 - moving ave loss 2.5284661553346104\n",
            "step 879 - loss 2.177217960357666 - moving ave loss 2.493341335836916\n",
            "step 880 - loss 2.397247314453125 - moving ave loss 2.4837319336985373\n",
            "step 881 - loss 2.4293715953826904 - moving ave loss 2.4782958998669526\n",
            "step 882 - loss 2.341571807861328 - moving ave loss 2.46462349066639\n",
            "step 883 - loss 2.90910267829895 - moving ave loss 2.5090714094296462\n",
            "step 884 - loss 3.1424198150634766 - moving ave loss 2.5724062499930294\n",
            "step 885 - loss 2.290492057800293 - moving ave loss 2.5442148307737558\n",
            "step 886 - loss 1.8575949668884277 - moving ave loss 2.475552844385223\n",
            "step 887 - loss 2.3929409980773926 - moving ave loss 2.46729165975444\n",
            "step 888 - loss 2.982992172241211 - moving ave loss 2.518861711003117\n",
            "step 889 - loss 2.5790762901306152 - moving ave loss 2.524883168915867\n",
            "step 890 - loss 2.4092905521392822 - moving ave loss 2.5133239072382083\n",
            "step 891 - loss 2.527423620223999 - moving ave loss 2.5147338785367874\n",
            "step 892 - loss 2.544099807739258 - moving ave loss 2.5176704714570346\n",
            "step 893 - loss 2.867157459259033 - moving ave loss 2.5526191702372345\n",
            "step 894 - loss 2.321936845779419 - moving ave loss 2.529550937791453\n",
            "step 895 - loss 2.412219762802124 - moving ave loss 2.51781782029252\n",
            "step 896 - loss 2.333610773086548 - moving ave loss 2.4993971155719232\n",
            "step 897 - loss 2.9767303466796875 - moving ave loss 2.5471304386826996\n",
            "Finish 39 epoch(es)\n",
            "step 898 - loss 2.033808469772339 - moving ave loss 2.4957982417916633\n",
            "step 899 - loss 2.1318986415863037 - moving ave loss 2.459408281771127\n",
            "step 900 - loss 2.5160365104675293 - moving ave loss 2.465071104640767\n",
            "step 901 - loss 2.122887372970581 - moving ave loss 2.4308527314737485\n",
            "step 902 - loss 2.3994243144989014 - moving ave loss 2.427709889776264\n",
            "step 903 - loss 2.3480615615844727 - moving ave loss 2.419745056957085\n",
            "step 904 - loss 2.5738165378570557 - moving ave loss 2.4351522050470824\n",
            "step 905 - loss 3.0965919494628906 - moving ave loss 2.5012961794886635\n",
            "step 906 - loss 2.4697885513305664 - moving ave loss 2.498145416672854\n",
            "step 907 - loss 2.5593395233154297 - moving ave loss 2.5042648273371118\n",
            "step 908 - loss 2.795839309692383 - moving ave loss 2.5334222755726388\n",
            "step 909 - loss 2.5617470741271973 - moving ave loss 2.5362547554280948\n",
            "step 910 - loss 2.2920007705688477 - moving ave loss 2.51182935694217\n",
            "step 911 - loss 3.4183011054992676 - moving ave loss 2.60247653179788\n",
            "step 912 - loss 2.381324291229248 - moving ave loss 2.580361307741017\n",
            "step 913 - loss 2.2758843898773193 - moving ave loss 2.5499136159546474\n",
            "step 914 - loss 2.5729730129241943 - moving ave loss 2.552219555651602\n",
            "step 915 - loss 2.242234945297241 - moving ave loss 2.521221094616166\n",
            "step 916 - loss 2.286550283432007 - moving ave loss 2.49775401349775\n",
            "step 917 - loss 2.367490291595459 - moving ave loss 2.484727641307521\n",
            "step 918 - loss 2.058274030685425 - moving ave loss 2.4420822802453115\n",
            "step 919 - loss 2.647223711013794 - moving ave loss 2.4625964233221596\n",
            "step 920 - loss 2.661616325378418 - moving ave loss 2.4824984135277854\n",
            "Finish 40 epoch(es)\n",
            "step 921 - loss 2.276261329650879 - moving ave loss 2.4618747051400947\n",
            "step 922 - loss 2.3725693225860596 - moving ave loss 2.452944166884691\n",
            "step 923 - loss 2.442354202270508 - moving ave loss 2.4518851704232727\n",
            "step 924 - loss 2.559692144393921 - moving ave loss 2.4626658678203373\n",
            "step 925 - loss 2.290684938430786 - moving ave loss 2.4454677748813825\n",
            "step 926 - loss 2.761888027191162 - moving ave loss 2.477109800112361\n",
            "step 927 - loss 2.548550605773926 - moving ave loss 2.4842538806785175\n",
            "step 928 - loss 2.109754800796509 - moving ave loss 2.446803972690317\n",
            "step 929 - loss 2.256067991256714 - moving ave loss 2.4277303745469565\n",
            "step 930 - loss 2.4397459030151367 - moving ave loss 2.4289319273937746\n",
            "step 931 - loss 2.4550886154174805 - moving ave loss 2.4315475961961455\n",
            "step 932 - loss 2.2017507553100586 - moving ave loss 2.4085679121075367\n",
            "step 933 - loss 2.6563844680786133 - moving ave loss 2.4333495677046444\n",
            "step 934 - loss 2.278841972351074 - moving ave loss 2.417898808169287\n",
            "step 935 - loss 2.570301055908203 - moving ave loss 2.433139032943179\n",
            "step 936 - loss 2.1747264862060547 - moving ave loss 2.407297778269467\n",
            "step 937 - loss 2.707780361175537 - moving ave loss 2.437346036560074\n",
            "step 938 - loss 2.114680528640747 - moving ave loss 2.4050794857681415\n",
            "step 939 - loss 3.236175537109375 - moving ave loss 2.488189090902265\n",
            "step 940 - loss 2.335610866546631 - moving ave loss 2.4729312684667013\n",
            "step 941 - loss 2.8013858795166016 - moving ave loss 2.5057767295716915\n",
            "step 942 - loss 2.7584667205810547 - moving ave loss 2.5310457286726282\n",
            "step 943 - loss 2.314516067504883 - moving ave loss 2.5093927625558536\n",
            "Finish 41 epoch(es)\n",
            "step 944 - loss 2.4165701866149902 - moving ave loss 2.5001105049617673\n",
            "step 945 - loss 2.878349781036377 - moving ave loss 2.5379344325692283\n",
            "step 946 - loss 2.279308557510376 - moving ave loss 2.5120718450633435\n",
            "step 947 - loss 2.6377549171447754 - moving ave loss 2.5246401522714867\n",
            "step 948 - loss 2.4920449256896973 - moving ave loss 2.521380629613308\n",
            "step 949 - loss 2.2228002548217773 - moving ave loss 2.491522592134155\n",
            "step 950 - loss 2.4317312240600586 - moving ave loss 2.4855434553267455\n",
            "step 951 - loss 2.7213447093963623 - moving ave loss 2.509123580733707\n",
            "step 952 - loss 2.4831018447875977 - moving ave loss 2.5065214071390964\n",
            "step 953 - loss 2.5441927909851074 - moving ave loss 2.510288545523698\n",
            "step 954 - loss 2.3585867881774902 - moving ave loss 2.4951183697890773\n",
            "step 955 - loss 2.4559717178344727 - moving ave loss 2.491203704593617\n",
            "step 956 - loss 2.521840810775757 - moving ave loss 2.494267415211831\n",
            "step 957 - loss 2.874549150466919 - moving ave loss 2.53229558873734\n",
            "step 958 - loss 2.758679151535034 - moving ave loss 2.5549339450171096\n",
            "step 959 - loss 2.2001097202301025 - moving ave loss 2.519451522538409\n",
            "step 960 - loss 2.6484148502349854 - moving ave loss 2.5323478553080667\n",
            "step 961 - loss 2.746255874633789 - moving ave loss 2.553738657240639\n",
            "step 962 - loss 2.2207822799682617 - moving ave loss 2.5204430195134013\n",
            "step 963 - loss 2.510976552963257 - moving ave loss 2.519496372858387\n",
            "step 964 - loss 2.286371946334839 - moving ave loss 2.4961839302060325\n",
            "step 965 - loss 2.55960750579834 - moving ave loss 2.502526287765263\n",
            "step 966 - loss 2.0042836666107178 - moving ave loss 2.452702025649809\n",
            "Finish 42 epoch(es)\n",
            "step 967 - loss 2.48537540435791 - moving ave loss 2.455969363520619\n",
            "step 968 - loss 1.9829779863357544 - moving ave loss 2.4086702258021324\n",
            "step 969 - loss 2.4710350036621094 - moving ave loss 2.41490670358813\n",
            "step 970 - loss 2.640566825866699 - moving ave loss 2.437472715815987\n",
            "step 971 - loss 2.094681978225708 - moving ave loss 2.4031936420569595\n",
            "step 972 - loss 2.222238540649414 - moving ave loss 2.3850981319162052\n",
            "step 973 - loss 2.155651569366455 - moving ave loss 2.3621534756612306\n",
            "step 974 - loss 2.2521963119506836 - moving ave loss 2.351157759290176\n",
            "step 975 - loss 2.8857479095458984 - moving ave loss 2.4046167743157483\n",
            "step 976 - loss 1.8461849689483643 - moving ave loss 2.34877359377901\n",
            "step 977 - loss 2.1045024394989014 - moving ave loss 2.3243464783509995\n",
            "step 978 - loss 2.552694797515869 - moving ave loss 2.3471813102674868\n",
            "step 979 - loss 2.1140432357788086 - moving ave loss 2.323867502818619\n",
            "step 980 - loss 2.298548698425293 - moving ave loss 2.3213356223792863\n",
            "step 981 - loss 2.1495633125305176 - moving ave loss 2.3041583913944095\n",
            "step 982 - loss 2.631484031677246 - moving ave loss 2.3368909554226933\n",
            "step 983 - loss 2.5297093391418457 - moving ave loss 2.3561727937946086\n",
            "step 984 - loss 2.545306444168091 - moving ave loss 2.375086158831957\n",
            "step 985 - loss 3.0047192573547363 - moving ave loss 2.438049468684235\n",
            "step 986 - loss 2.3043880462646484 - moving ave loss 2.4246833264422762\n",
            "step 987 - loss 2.4660918712615967 - moving ave loss 2.4288241809242086\n",
            "step 988 - loss 2.4027366638183594 - moving ave loss 2.4262154292136238\n",
            "step 989 - loss 2.3595290184020996 - moving ave loss 2.4195467881324713\n",
            "Finish 43 epoch(es)\n",
            "step 990 - loss 2.5040628910064697 - moving ave loss 2.427998398419871\n",
            "step 991 - loss 2.119504928588867 - moving ave loss 2.397149051436771\n",
            "step 992 - loss 2.4660818576812744 - moving ave loss 2.404042332061221\n",
            "step 993 - loss 2.6950860023498535 - moving ave loss 2.4331466990900843\n",
            "step 994 - loss 1.692735195159912 - moving ave loss 2.359105548697067\n",
            "step 995 - loss 2.60825252532959 - moving ave loss 2.3840202463603193\n",
            "step 996 - loss 2.795363664627075 - moving ave loss 2.425154588186995\n",
            "step 997 - loss 2.266697406768799 - moving ave loss 2.4093088700451757\n",
            "step 998 - loss 2.316056489944458 - moving ave loss 2.399983632035104\n",
            "step 999 - loss 2.0331501960754395 - moving ave loss 2.3633002884391376\n",
            "step 1000 - loss 2.4489951133728027 - moving ave loss 2.3718697709325043\n",
            "Checkpoint at step 1000\n",
            "step 1001 - loss 2.9443225860595703 - moving ave loss 2.429115052445211\n",
            "step 1002 - loss 2.6136181354522705 - moving ave loss 2.447565360745917\n",
            "step 1003 - loss 2.4547970294952393 - moving ave loss 2.4482885276208495\n",
            "step 1004 - loss 2.1388368606567383 - moving ave loss 2.417343360924438\n",
            "step 1005 - loss 2.4311792850494385 - moving ave loss 2.418726953336938\n",
            "step 1006 - loss 2.2013001441955566 - moving ave loss 2.3969842724228\n",
            "step 1007 - loss 2.330699920654297 - moving ave loss 2.39035583724595\n",
            "step 1008 - loss 2.341728448867798 - moving ave loss 2.385493098408135\n",
            "step 1009 - loss 2.407827854156494 - moving ave loss 2.3877265739829707\n",
            "step 1010 - loss 2.494945526123047 - moving ave loss 2.3984484691969783\n",
            "step 1011 - loss 2.104896068572998 - moving ave loss 2.3690932291345805\n",
            "step 1012 - loss 2.2322943210601807 - moving ave loss 2.3554133383271405\n",
            "Finish 44 epoch(es)\n",
            "step 1013 - loss 2.3625006675720215 - moving ave loss 2.3561220712516286\n",
            "step 1014 - loss 2.6437323093414307 - moving ave loss 2.3848830950606086\n",
            "step 1015 - loss 2.0475292205810547 - moving ave loss 2.3511477076126535\n",
            "step 1016 - loss 2.7384629249572754 - moving ave loss 2.3898792293471156\n",
            "step 1017 - loss 2.4944472312927246 - moving ave loss 2.4003360295416765\n",
            "step 1018 - loss 2.4933900833129883 - moving ave loss 2.409641434918808\n",
            "step 1019 - loss 2.2670764923095703 - moving ave loss 2.3953849406578844\n",
            "step 1020 - loss 2.2454581260681152 - moving ave loss 2.3803922591989077\n",
            "step 1021 - loss 2.0201878547668457 - moving ave loss 2.344371818755701\n",
            "step 1022 - loss 2.1841373443603516 - moving ave loss 2.3283483713161663\n",
            "step 1023 - loss 2.1323797702789307 - moving ave loss 2.308751511212443\n",
            "step 1024 - loss 2.173640489578247 - moving ave loss 2.2952404090490237\n",
            "step 1025 - loss 2.608844757080078 - moving ave loss 2.3266008438521295\n",
            "step 1026 - loss 2.356513261795044 - moving ave loss 2.3295920856464214\n",
            "step 1027 - loss 2.0489888191223145 - moving ave loss 2.3015317589940105\n",
            "step 1028 - loss 2.7060251235961914 - moving ave loss 2.3419810954542286\n",
            "step 1029 - loss 2.443795680999756 - moving ave loss 2.3521625540087814\n",
            "step 1030 - loss 2.2574002742767334 - moving ave loss 2.342686326035577\n",
            "step 1031 - loss 2.025247812271118 - moving ave loss 2.310942474659131\n",
            "step 1032 - loss 2.2216970920562744 - moving ave loss 2.3020179363988453\n",
            "step 1033 - loss 2.2533693313598633 - moving ave loss 2.297153075894947\n",
            "step 1034 - loss 2.4049978256225586 - moving ave loss 2.3079375508677082\n",
            "step 1035 - loss 2.430305004119873 - moving ave loss 2.320174296192925\n",
            "Finish 45 epoch(es)\n",
            "step 1036 - loss 2.895510673522949 - moving ave loss 2.3777079339259273\n",
            "step 1037 - loss 2.559965133666992 - moving ave loss 2.3959336539000335\n",
            "step 1038 - loss 1.9767069816589355 - moving ave loss 2.354010986675924\n",
            "step 1039 - loss 2.213432788848877 - moving ave loss 2.339953166893219\n",
            "step 1040 - loss 1.951765775680542 - moving ave loss 2.3011344277719514\n",
            "step 1041 - loss 1.9499038457870483 - moving ave loss 2.266011369573461\n",
            "step 1042 - loss 2.1136293411254883 - moving ave loss 2.250773166728664\n",
            "step 1043 - loss 2.513017416000366 - moving ave loss 2.2769975916558343\n",
            "step 1044 - loss 2.138695240020752 - moving ave loss 2.2631673564923265\n",
            "step 1045 - loss 2.266446113586426 - moving ave loss 2.2634952322017368\n",
            "step 1046 - loss 2.252434015274048 - moving ave loss 2.262389110508968\n",
            "step 1047 - loss 2.498891830444336 - moving ave loss 2.2860393825025045\n",
            "step 1048 - loss 2.5437228679656982 - moving ave loss 2.3118077310488236\n",
            "step 1049 - loss 2.8363091945648193 - moving ave loss 2.3642578774004233\n",
            "step 1050 - loss 2.381795883178711 - moving ave loss 2.366011677978252\n",
            "step 1051 - loss 2.263338088989258 - moving ave loss 2.3557443190793528\n",
            "step 1052 - loss 1.9390288591384888 - moving ave loss 2.3140727730852664\n",
            "step 1053 - loss 2.2736239433288574 - moving ave loss 2.3100278901096254\n",
            "step 1054 - loss 2.2350590229034424 - moving ave loss 2.302531003389007\n",
            "step 1055 - loss 2.2827601432800293 - moving ave loss 2.3005539173781093\n",
            "step 1056 - loss 2.0016965866088867 - moving ave loss 2.2706681843011873\n",
            "step 1057 - loss 1.9603688716888428 - moving ave loss 2.2396382530399532\n",
            "step 1058 - loss 2.163360118865967 - moving ave loss 2.2320104396225546\n",
            "Finish 46 epoch(es)\n",
            "step 1059 - loss 2.7132840156555176 - moving ave loss 2.280137797225851\n",
            "step 1060 - loss 1.9820908308029175 - moving ave loss 2.250333100583558\n",
            "step 1061 - loss 2.6758687496185303 - moving ave loss 2.292886665487055\n",
            "step 1062 - loss 2.3703789710998535 - moving ave loss 2.300635896048335\n",
            "step 1063 - loss 2.3250327110290527 - moving ave loss 2.303075577546407\n",
            "step 1064 - loss 3.2374582290649414 - moving ave loss 2.3965138426982606\n",
            "step 1065 - loss 2.4036049842834473 - moving ave loss 2.3972229568567793\n",
            "step 1066 - loss 2.2156319618225098 - moving ave loss 2.3790638573533527\n",
            "step 1067 - loss 2.210930585861206 - moving ave loss 2.362250530204138\n",
            "step 1068 - loss 2.4461562633514404 - moving ave loss 2.3706411035188686\n",
            "step 1069 - loss 1.9853709936141968 - moving ave loss 2.332114092528401\n",
            "step 1070 - loss 1.8668954372406006 - moving ave loss 2.285592226999621\n",
            "step 1071 - loss 2.307030439376831 - moving ave loss 2.287736048237342\n",
            "step 1072 - loss 2.373321056365967 - moving ave loss 2.2962945490502045\n",
            "step 1073 - loss 2.278289318084717 - moving ave loss 2.294494025953656\n",
            "step 1074 - loss 2.488736629486084 - moving ave loss 2.3139182863068988\n",
            "step 1075 - loss 2.3012337684631348 - moving ave loss 2.3126498345225226\n",
            "step 1076 - loss 2.399679183959961 - moving ave loss 2.321352769466267\n",
            "step 1077 - loss 2.1079835891723633 - moving ave loss 2.3000158514368767\n",
            "step 1078 - loss 2.4825572967529297 - moving ave loss 2.3182699959684823\n",
            "step 1079 - loss 1.8778107166290283 - moving ave loss 2.274224068034537\n",
            "step 1080 - loss 2.06721830368042 - moving ave loss 2.253523491599125\n",
            "step 1081 - loss 1.8575503826141357 - moving ave loss 2.213926180700626\n",
            "Finish 47 epoch(es)\n",
            "step 1082 - loss 2.483771324157715 - moving ave loss 2.2409106950463347\n",
            "step 1083 - loss 2.818025588989258 - moving ave loss 2.298622184440627\n",
            "step 1084 - loss 2.049827814102173 - moving ave loss 2.2737427474067817\n",
            "step 1085 - loss 2.288062572479248 - moving ave loss 2.2751747299140286\n",
            "step 1086 - loss 2.8109164237976074 - moving ave loss 2.328748899302387\n",
            "step 1087 - loss 2.685772180557251 - moving ave loss 2.3644512274278733\n",
            "step 1088 - loss 2.0323076248168945 - moving ave loss 2.3312368671667754\n",
            "step 1089 - loss 2.074409008026123 - moving ave loss 2.30555408125271\n",
            "step 1090 - loss 2.0847864151000977 - moving ave loss 2.2834773146374494\n",
            "step 1091 - loss 2.1490554809570312 - moving ave loss 2.2700351312694074\n",
            "step 1092 - loss 2.1755080223083496 - moving ave loss 2.2605824203733014\n",
            "step 1093 - loss 2.2687292098999023 - moving ave loss 2.2613970993259613\n",
            "step 1094 - loss 2.1279890537261963 - moving ave loss 2.248056294765985\n",
            "step 1095 - loss 2.2309951782226562 - moving ave loss 2.246350183111652\n",
            "step 1096 - loss 1.883955717086792 - moving ave loss 2.2101107365091663\n",
            "step 1097 - loss 2.677100658416748 - moving ave loss 2.2568097286999245\n",
            "step 1098 - loss 1.8660593032836914 - moving ave loss 2.2177346861583014\n",
            "step 1099 - loss 2.6046652793884277 - moving ave loss 2.256427745481314\n",
            "step 1100 - loss 2.706606149673462 - moving ave loss 2.3014455859005287\n",
            "step 1101 - loss 2.350310802459717 - moving ave loss 2.3063321075564476\n",
            "step 1102 - loss 2.116830825805664 - moving ave loss 2.2873819793813692\n",
            "step 1103 - loss 2.5177299976348877 - moving ave loss 2.310416781206721\n",
            "step 1104 - loss 2.0166945457458496 - moving ave loss 2.281044557660634\n",
            "Finish 48 epoch(es)\n",
            "step 1105 - loss 1.9190441370010376 - moving ave loss 2.2448445155946746\n",
            "step 1106 - loss 2.2589879035949707 - moving ave loss 2.246258854394704\n",
            "step 1107 - loss 2.383995294570923 - moving ave loss 2.260032498412326\n",
            "step 1108 - loss 1.9026800394058228 - moving ave loss 2.2242972525116755\n",
            "step 1109 - loss 2.0560684204101562 - moving ave loss 2.2074743693015235\n",
            "step 1110 - loss 2.1250078678131104 - moving ave loss 2.1992277191526823\n",
            "step 1111 - loss 2.6406002044677734 - moving ave loss 2.2433649676841916\n",
            "step 1112 - loss 2.45233154296875 - moving ave loss 2.2642616252126477\n",
            "step 1113 - loss 2.3502206802368164 - moving ave loss 2.2728575307150645\n",
            "step 1114 - loss 2.2573413848876953 - moving ave loss 2.2713059161323277\n",
            "step 1115 - loss 1.983398199081421 - moving ave loss 2.242515144427237\n",
            "step 1116 - loss 2.0061464309692383 - moving ave loss 2.218878273081437\n",
            "step 1117 - loss 2.1282215118408203 - moving ave loss 2.2098125969573754\n",
            "step 1118 - loss 2.0495824813842773 - moving ave loss 2.1937895854000655\n",
            "step 1119 - loss 2.504436492919922 - moving ave loss 2.224854276152051\n",
            "step 1120 - loss 2.335415840148926 - moving ave loss 2.235910432551739\n",
            "step 1121 - loss 2.1330153942108154 - moving ave loss 2.2256209287176465\n",
            "step 1122 - loss 2.4036848545074463 - moving ave loss 2.2434273212966267\n",
            "step 1123 - loss 2.223769426345825 - moving ave loss 2.2414615318015465\n",
            "step 1124 - loss 2.440445899963379 - moving ave loss 2.2613599686177297\n",
            "step 1125 - loss 2.8403258323669434 - moving ave loss 2.319256554992651\n",
            "Checkpoint at step 1125\n",
            "step 1126 - loss 2.1136972904205322 - moving ave loss 2.2987006285354394\n",
            "step 1127 - loss 1.9827252626419067 - moving ave loss 2.2671030919460864\n",
            "Finish 49 epoch(es)\n",
            "step 1128 - loss 1.8817980289459229 - moving ave loss 2.22857258564607\n",
            "step 1129 - loss 1.8768537044525146 - moving ave loss 2.1934006975267146\n",
            "step 1130 - loss 1.956131100654602 - moving ave loss 2.1696737378395032\n",
            "step 1131 - loss 2.292300224304199 - moving ave loss 2.181936386485973\n",
            "step 1132 - loss 1.81645667552948 - moving ave loss 2.1453884153903235\n",
            "step 1133 - loss 2.419210433959961 - moving ave loss 2.1727706172472874\n",
            "step 1134 - loss 1.89586341381073 - moving ave loss 2.1450798969036318\n",
            "step 1135 - loss 2.379572868347168 - moving ave loss 2.168529194047985\n",
            "step 1136 - loss 2.255403757095337 - moving ave loss 2.17721665035272\n",
            "step 1137 - loss 3.0100150108337402 - moving ave loss 2.260496486400822\n",
            "step 1138 - loss 2.335270643234253 - moving ave loss 2.267973902084165\n",
            "step 1139 - loss 2.1944620609283447 - moving ave loss 2.260622717968583\n",
            "step 1140 - loss 1.9073524475097656 - moving ave loss 2.2252956909227013\n",
            "step 1141 - loss 2.161498546600342 - moving ave loss 2.2189159764904653\n",
            "step 1142 - loss 2.2827954292297363 - moving ave loss 2.2253039217643926\n",
            "step 1143 - loss 1.7960731983184814 - moving ave loss 2.1823808494198014\n",
            "step 1144 - loss 2.598184585571289 - moving ave loss 2.22396122303495\n",
            "step 1145 - loss 2.2121028900146484 - moving ave loss 2.2227753897329197\n",
            "step 1146 - loss 2.1040332317352295 - moving ave loss 2.2109011739331508\n",
            "step 1147 - loss 2.0957107543945312 - moving ave loss 2.199382131979289\n",
            "step 1148 - loss 2.9525036811828613 - moving ave loss 2.274694286899646\n",
            "step 1149 - loss 2.14729905128479 - moving ave loss 2.2619547633381605\n",
            "step 1150 - loss 2.1876816749572754 - moving ave loss 2.2545274545000717\n",
            "Finish 50 epoch(es)\n",
            "step 1151 - loss 2.314236640930176 - moving ave loss 2.260498373143082\n",
            "step 1152 - loss 2.359312057495117 - moving ave loss 2.270379741578285\n",
            "step 1153 - loss 2.3561925888061523 - moving ave loss 2.2789610263010722\n",
            "step 1154 - loss 1.8212721347808838 - moving ave loss 2.2331921371490537\n",
            "step 1155 - loss 1.8301283121109009 - moving ave loss 2.1928857546452383\n",
            "step 1156 - loss 2.2979533672332764 - moving ave loss 2.203392515904042\n",
            "step 1157 - loss 2.2392959594726562 - moving ave loss 2.206982860260904\n",
            "step 1158 - loss 2.5533676147460938 - moving ave loss 2.241621335709423\n",
            "step 1159 - loss 2.1036643981933594 - moving ave loss 2.227825641957817\n",
            "step 1160 - loss 2.251197576522827 - moving ave loss 2.2301628354143177\n",
            "step 1161 - loss 2.6731812953948975 - moving ave loss 2.274464681412376\n",
            "step 1162 - loss 2.4491069316864014 - moving ave loss 2.2919289064397788\n",
            "step 1163 - loss 2.1155006885528564 - moving ave loss 2.2742860846510866\n",
            "step 1164 - loss 2.125378131866455 - moving ave loss 2.2593952893726232\n",
            "step 1165 - loss 2.0519957542419434 - moving ave loss 2.2386553358595553\n",
            "step 1166 - loss 2.6716747283935547 - moving ave loss 2.2819572751129553\n",
            "step 1167 - loss 1.9744824171066284 - moving ave loss 2.2512097893123224\n",
            "step 1168 - loss 1.9996671676635742 - moving ave loss 2.2260555271474476\n",
            "step 1169 - loss 2.1062052249908447 - moving ave loss 2.2140704969317873\n",
            "step 1170 - loss 2.2501673698425293 - moving ave loss 2.2176801842228615\n",
            "step 1171 - loss 2.0102107524871826 - moving ave loss 2.196933241049294\n",
            "step 1172 - loss 2.193422317504883 - moving ave loss 2.196582148694853\n",
            "step 1173 - loss 2.3647866249084473 - moving ave loss 2.2134025963162127\n",
            "Finish 51 epoch(es)\n",
            "step 1174 - loss 2.1662685871124268 - moving ave loss 2.2086891953958343\n",
            "step 1175 - loss 2.3068461418151855 - moving ave loss 2.2185048900377695\n",
            "step 1176 - loss 2.161611557006836 - moving ave loss 2.212815556734676\n",
            "step 1177 - loss 2.963958740234375 - moving ave loss 2.287929875084646\n",
            "step 1178 - loss 2.080782890319824 - moving ave loss 2.267215176608164\n",
            "step 1179 - loss 2.071054697036743 - moving ave loss 2.247599128651022\n",
            "step 1180 - loss 2.646453857421875 - moving ave loss 2.287484601528107\n",
            "step 1181 - loss 2.2713446617126465 - moving ave loss 2.285870607546561\n",
            "step 1182 - loss 2.2627320289611816 - moving ave loss 2.283556749688023\n",
            "step 1183 - loss 2.2191689014434814 - moving ave loss 2.277117964863569\n",
            "step 1184 - loss 2.39843487739563 - moving ave loss 2.2892496561167754\n",
            "step 1185 - loss 1.9465336799621582 - moving ave loss 2.254978058501314\n",
            "step 1186 - loss 2.2242074012756348 - moving ave loss 2.251900992778746\n",
            "step 1187 - loss 1.848447561264038 - moving ave loss 2.2115556496272752\n",
            "step 1188 - loss 2.338404655456543 - moving ave loss 2.224240550210202\n",
            "step 1189 - loss 2.2364089488983154 - moving ave loss 2.2254573900790136\n",
            "step 1190 - loss 1.7705681324005127 - moving ave loss 2.1799684643111634\n",
            "step 1191 - loss 2.0988216400146484 - moving ave loss 2.171853781881512\n",
            "step 1192 - loss 1.9475398063659668 - moving ave loss 2.1494223843299576\n",
            "step 1193 - loss 2.045320510864258 - moving ave loss 2.1390121969833875\n",
            "step 1194 - loss 2.0683555603027344 - moving ave loss 2.1319465333153222\n",
            "step 1195 - loss 2.370577812194824 - moving ave loss 2.1558096612032727\n",
            "step 1196 - loss 2.5178136825561523 - moving ave loss 2.192010063338561\n",
            "Finish 52 epoch(es)\n",
            "step 1197 - loss 1.9941632747650146 - moving ave loss 2.172225384481206\n",
            "step 1198 - loss 2.2774999141693115 - moving ave loss 2.1827528374500167\n",
            "step 1199 - loss 1.9594224691390991 - moving ave loss 2.160419800618925\n",
            "step 1200 - loss 2.2668681144714355 - moving ave loss 2.171064632004176\n",
            "step 1201 - loss 2.3196706771850586 - moving ave loss 2.185925236522264\n",
            "step 1202 - loss 1.781327486038208 - moving ave loss 2.1454654614738584\n",
            "step 1203 - loss 2.463047981262207 - moving ave loss 2.177223713452693\n",
            "step 1204 - loss 1.7632592916488647 - moving ave loss 2.1358272712723103\n",
            "step 1205 - loss 1.9953174591064453 - moving ave loss 2.121776290055724\n",
            "step 1206 - loss 2.744950771331787 - moving ave loss 2.1840937381833303\n",
            "step 1207 - loss 1.8664183616638184 - moving ave loss 2.152326200531379\n",
            "step 1208 - loss 2.261605739593506 - moving ave loss 2.163254154437592\n",
            "step 1209 - loss 1.8502742052078247 - moving ave loss 2.1319561595146155\n",
            "step 1210 - loss 2.8614015579223633 - moving ave loss 2.2049006993553903\n",
            "step 1211 - loss 2.546816825866699 - moving ave loss 2.2390923120065214\n",
            "step 1212 - loss 2.3955750465393066 - moving ave loss 2.2547405854598\n",
            "step 1213 - loss 2.4522347450256348 - moving ave loss 2.274490001416384\n",
            "step 1214 - loss 2.575059175491333 - moving ave loss 2.304546918823879\n",
            "step 1215 - loss 2.3589861392974854 - moving ave loss 2.3099908408712397\n",
            "step 1216 - loss 2.0338473320007324 - moving ave loss 2.282376489984189\n",
            "step 1217 - loss 2.264961004257202 - moving ave loss 2.28063494141149\n",
            "step 1218 - loss 2.1154375076293945 - moving ave loss 2.2641151980332808\n",
            "step 1219 - loss 2.6285364627838135 - moving ave loss 2.300557324508334\n",
            "Finish 53 epoch(es)\n",
            "step 1220 - loss 1.7320036888122559 - moving ave loss 2.243701960938726\n",
            "step 1221 - loss 2.0952656269073486 - moving ave loss 2.228858327535588\n",
            "step 1222 - loss 2.1857786178588867 - moving ave loss 2.224550356567918\n",
            "step 1223 - loss 2.169788360595703 - moving ave loss 2.2190741569706964\n",
            "step 1224 - loss 1.9384344816207886 - moving ave loss 2.1910101894357057\n",
            "step 1225 - loss 2.261051654815674 - moving ave loss 2.1980143359737028\n",
            "step 1226 - loss 2.779329299926758 - moving ave loss 2.2561458323690085\n",
            "step 1227 - loss 2.030731201171875 - moving ave loss 2.2336043692492953\n",
            "step 1228 - loss 2.2815897464752197 - moving ave loss 2.2384029069718876\n",
            "step 1229 - loss 2.26069974899292 - moving ave loss 2.240632591173991\n",
            "step 1230 - loss 2.094355344772339 - moving ave loss 2.2260048665338257\n",
            "step 1231 - loss 2.463773250579834 - moving ave loss 2.2497817049384268\n",
            "step 1232 - loss 2.027010440826416 - moving ave loss 2.227504578527226\n",
            "step 1233 - loss 1.937788963317871 - moving ave loss 2.1985330170062904\n",
            "step 1234 - loss 1.9162380695343018 - moving ave loss 2.1703035222590916\n",
            "step 1235 - loss 2.076855421066284 - moving ave loss 2.160958712139811\n",
            "step 1236 - loss 1.9381229877471924 - moving ave loss 2.138675139700549\n",
            "step 1237 - loss 1.7945387363433838 - moving ave loss 2.1042614993648328\n",
            "step 1238 - loss 2.842531681060791 - moving ave loss 2.1780885175344284\n",
            "step 1239 - loss 2.1726088523864746 - moving ave loss 2.177540551019633\n",
            "step 1240 - loss 2.152987241744995 - moving ave loss 2.1750852200921695\n",
            "step 1241 - loss 2.2872791290283203 - moving ave loss 2.1863046109857844\n",
            "step 1242 - loss 2.2759995460510254 - moving ave loss 2.1952741044923085\n",
            "Finish 54 epoch(es)\n",
            "step 1243 - loss 2.3118162155151367 - moving ave loss 2.2069283155945913\n",
            "step 1244 - loss 2.192218542098999 - moving ave loss 2.205457338245032\n",
            "step 1245 - loss 2.1066813468933105 - moving ave loss 2.19557973910986\n",
            "step 1246 - loss 2.0541176795959473 - moving ave loss 2.1814335331584687\n",
            "step 1247 - loss 1.8949894905090332 - moving ave loss 2.152789128893525\n",
            "step 1248 - loss 2.3450779914855957 - moving ave loss 2.172018015152732\n",
            "step 1249 - loss 2.0461690425872803 - moving ave loss 2.159433117896187\n",
            "step 1250 - loss 2.026232957839966 - moving ave loss 2.146113101890565\n",
            "Checkpoint at step 1250\n",
            "step 1251 - loss 2.2010116577148438 - moving ave loss 2.151602957472993\n",
            "step 1252 - loss 1.8661015033721924 - moving ave loss 2.123052812062913\n",
            "step 1253 - loss 2.281693696975708 - moving ave loss 2.138916900554192\n",
            "step 1254 - loss 2.0776302814483643 - moving ave loss 2.1327882386436094\n",
            "step 1255 - loss 2.339232921600342 - moving ave loss 2.1534327069392827\n",
            "step 1256 - loss 2.2297723293304443 - moving ave loss 2.161066669178399\n",
            "step 1257 - loss 2.0225653648376465 - moving ave loss 2.1472165387443236\n",
            "step 1258 - loss 1.9655978679656982 - moving ave loss 2.129054671666461\n",
            "step 1259 - loss 1.9174580574035645 - moving ave loss 2.1078950102401715\n",
            "step 1260 - loss 2.1761059761047363 - moving ave loss 2.114716106826628\n",
            "step 1261 - loss 1.910684585571289 - moving ave loss 2.094312954701094\n",
            "step 1262 - loss 1.8302874565124512 - moving ave loss 2.06791040488223\n",
            "step 1263 - loss 2.2596077919006348 - moving ave loss 2.0870801435840702\n",
            "step 1264 - loss 2.4718711376190186 - moving ave loss 2.125559242987565\n",
            "step 1265 - loss 2.4262404441833496 - moving ave loss 2.1556273631071434\n",
            "Finish 55 epoch(es)\n",
            "step 1266 - loss 1.8976221084594727 - moving ave loss 2.1298268376423763\n",
            "step 1267 - loss 2.403567314147949 - moving ave loss 2.1572008852929336\n",
            "step 1268 - loss 2.1628899574279785 - moving ave loss 2.1577697925064383\n",
            "step 1269 - loss 1.6454975605010986 - moving ave loss 2.1065425693059043\n",
            "step 1270 - loss 2.3981592655181885 - moving ave loss 2.135704238927133\n",
            "step 1271 - loss 1.9547889232635498 - moving ave loss 2.1176127073607747\n",
            "step 1272 - loss 2.2030029296875 - moving ave loss 2.126151729593447\n",
            "step 1273 - loss 1.9320138692855835 - moving ave loss 2.1067379435626608\n",
            "step 1274 - loss 2.2437124252319336 - moving ave loss 2.120435391729588\n",
            "step 1275 - loss 1.8691946268081665 - moving ave loss 2.095311315237446\n",
            "step 1276 - loss 2.274550437927246 - moving ave loss 2.113235227506426\n",
            "step 1277 - loss 2.1520769596099854 - moving ave loss 2.117119400716782\n",
            "step 1278 - loss 2.178201198577881 - moving ave loss 2.123227580502892\n",
            "step 1279 - loss 2.037691593170166 - moving ave loss 2.1146739817696196\n",
            "step 1280 - loss 2.2044763565063477 - moving ave loss 2.1236542192432926\n",
            "step 1281 - loss 2.3898093700408936 - moving ave loss 2.150269734323053\n",
            "step 1282 - loss 2.32273530960083 - moving ave loss 2.1675162918508306\n",
            "step 1283 - loss 2.206350088119507 - moving ave loss 2.1713996714776984\n",
            "step 1284 - loss 2.1381607055664062 - moving ave loss 2.168075774886569\n",
            "step 1285 - loss 1.9057308435440063 - moving ave loss 2.141841281752313\n",
            "step 1286 - loss 1.832682490348816 - moving ave loss 2.1109254026119633\n",
            "step 1287 - loss 2.1343157291412354 - moving ave loss 2.1132644352648904\n",
            "step 1288 - loss 1.8019006252288818 - moving ave loss 2.0821280542612897\n",
            "Finish 56 epoch(es)\n",
            "step 1289 - loss 2.206697463989258 - moving ave loss 2.0945849952340865\n",
            "step 1290 - loss 2.592834234237671 - moving ave loss 2.144409919134445\n",
            "step 1291 - loss 2.2007617950439453 - moving ave loss 2.150045106725395\n",
            "step 1292 - loss 2.011108875274658 - moving ave loss 2.1361514835803215\n",
            "step 1293 - loss 2.359299659729004 - moving ave loss 2.1584663011951895\n",
            "step 1294 - loss 2.0392818450927734 - moving ave loss 2.1465478555849478\n",
            "step 1295 - loss 2.023956537246704 - moving ave loss 2.134288723751123\n",
            "step 1296 - loss 1.9522978067398071 - moving ave loss 2.1160896320499916\n",
            "step 1297 - loss 1.9806466102600098 - moving ave loss 2.1025453298709933\n",
            "step 1298 - loss 2.2024545669555664 - moving ave loss 2.1125362535794507\n",
            "step 1299 - loss 2.327169418334961 - moving ave loss 2.1339995700550016\n",
            "step 1300 - loss 2.6298887729644775 - moving ave loss 2.1835884903459495\n",
            "step 1301 - loss 2.342406749725342 - moving ave loss 2.1994703162838887\n",
            "step 1302 - loss 1.807389497756958 - moving ave loss 2.1602622344311957\n",
            "step 1303 - loss 2.3437228202819824 - moving ave loss 2.1786082930162745\n",
            "step 1304 - loss 2.1785693168640137 - moving ave loss 2.1786043954010483\n",
            "step 1305 - loss 2.0617990493774414 - moving ave loss 2.1669238607986876\n",
            "step 1306 - loss 2.224242687225342 - moving ave loss 2.172655743441353\n",
            "step 1307 - loss 2.231679916381836 - moving ave loss 2.1785581607354016\n",
            "step 1308 - loss 1.499099612236023 - moving ave loss 2.110612305885464\n",
            "step 1309 - loss 1.6209757328033447 - moving ave loss 2.061648648577252\n",
            "step 1310 - loss 2.597898006439209 - moving ave loss 2.1152735843634476\n",
            "step 1311 - loss 2.0956778526306152 - moving ave loss 2.1133140111901643\n",
            "Finish 57 epoch(es)\n",
            "step 1312 - loss 2.0545654296875 - moving ave loss 2.107439153039898\n",
            "step 1313 - loss 2.2198688983917236 - moving ave loss 2.1186821275750805\n",
            "step 1314 - loss 2.4037952423095703 - moving ave loss 2.1471934390485297\n",
            "step 1315 - loss 2.876673698425293 - moving ave loss 2.220141464986206\n",
            "step 1316 - loss 1.8661630153656006 - moving ave loss 2.1847436200241455\n",
            "step 1317 - loss 2.385021209716797 - moving ave loss 2.2047713789934105\n",
            "step 1318 - loss 2.312981367111206 - moving ave loss 2.21559237780519\n",
            "step 1319 - loss 1.7662385702133179 - moving ave loss 2.170656997046003\n",
            "step 1320 - loss 1.977458119392395 - moving ave loss 2.151337109280642\n",
            "step 1321 - loss 1.8363687992095947 - moving ave loss 2.1198402782735375\n",
            "step 1322 - loss 3.1353864669799805 - moving ave loss 2.2213948971441817\n",
            "step 1323 - loss 1.8257811069488525 - moving ave loss 2.1818335181246487\n",
            "step 1324 - loss 2.035374641418457 - moving ave loss 2.1671876304540296\n",
            "step 1325 - loss 1.9408921003341675 - moving ave loss 2.1445580774420434\n",
            "step 1326 - loss 1.8172581195831299 - moving ave loss 2.111828081656152\n",
            "step 1327 - loss 1.9990378618240356 - moving ave loss 2.1005490596729404\n",
            "step 1328 - loss 2.1545569896698 - moving ave loss 2.105949852672626\n",
            "step 1329 - loss 2.3439927101135254 - moving ave loss 2.129754138416716\n",
            "step 1330 - loss 1.880553960800171 - moving ave loss 2.1048341206550614\n",
            "step 1331 - loss 2.126638412475586 - moving ave loss 2.107014549837114\n",
            "step 1332 - loss 1.8207379579544067 - moving ave loss 2.078386890648843\n",
            "step 1333 - loss 1.9234180450439453 - moving ave loss 2.0628900060883533\n",
            "step 1334 - loss 1.677112340927124 - moving ave loss 2.02431223957223\n",
            "Finish 58 epoch(es)\n",
            "step 1335 - loss 2.2158608436584473 - moving ave loss 2.0434670999808517\n",
            "step 1336 - loss 1.439632534980774 - moving ave loss 1.983083643480844\n",
            "step 1337 - loss 2.3210034370422363 - moving ave loss 2.0168756228369835\n",
            "step 1338 - loss 2.05776309967041 - moving ave loss 2.020964370520326\n",
            "step 1339 - loss 2.212414264678955 - moving ave loss 2.040109359936189\n",
            "step 1340 - loss 2.3223299980163574 - moving ave loss 2.068331423744206\n",
            "step 1341 - loss 1.9880361557006836 - moving ave loss 2.060301896939854\n",
            "step 1342 - loss 2.3188881874084473 - moving ave loss 2.0861605259867133\n",
            "step 1343 - loss 2.2738523483276367 - moving ave loss 2.104929708220806\n",
            "step 1344 - loss 1.7534267902374268 - moving ave loss 2.0697794164224677\n",
            "step 1345 - loss 2.1662516593933105 - moving ave loss 2.079426640719552\n",
            "step 1346 - loss 2.148439645767212 - moving ave loss 2.086327941224318\n",
            "step 1347 - loss 2.087538480758667 - moving ave loss 2.086448995177753\n",
            "step 1348 - loss 2.2637481689453125 - moving ave loss 2.104178912554509\n",
            "step 1349 - loss 1.748736023902893 - moving ave loss 2.0686346236893476\n",
            "step 1350 - loss 2.268796443939209 - moving ave loss 2.088650805714334\n",
            "step 1351 - loss 2.470783233642578 - moving ave loss 2.126864048507158\n",
            "step 1352 - loss 2.067746162414551 - moving ave loss 2.1209522598978974\n",
            "step 1353 - loss 1.9646086692810059 - moving ave loss 2.105317900836208\n",
            "step 1354 - loss 2.011629581451416 - moving ave loss 2.095949068897729\n",
            "step 1355 - loss 2.0070056915283203 - moving ave loss 2.087054731160788\n",
            "step 1356 - loss 1.8463342189788818 - moving ave loss 2.0629826799425977\n",
            "step 1357 - loss 2.3105664253234863 - moving ave loss 2.0877410544806865\n",
            "Finish 59 epoch(es)\n",
            "step 1358 - loss 1.79502272605896 - moving ave loss 2.058469221638514\n",
            "step 1359 - loss 2.5158653259277344 - moving ave loss 2.104208832067436\n",
            "step 1360 - loss 2.3022103309631348 - moving ave loss 2.1240089819570063\n",
            "step 1361 - loss 1.9217902421951294 - moving ave loss 2.1037871079808186\n",
            "step 1362 - loss 2.099382162094116 - moving ave loss 2.1033466133921483\n",
            "step 1363 - loss 1.887144684791565 - moving ave loss 2.0817264205320902\n",
            "step 1364 - loss 1.8850529193878174 - moving ave loss 2.062059070417663\n",
            "step 1365 - loss 2.221737861633301 - moving ave loss 2.0780269495392267\n",
            "step 1366 - loss 2.068549394607544 - moving ave loss 2.0770791940460587\n",
            "step 1367 - loss 2.3348519802093506 - moving ave loss 2.102856472662388\n",
            "step 1368 - loss 2.0260143280029297 - moving ave loss 2.0951722581964423\n",
            "step 1369 - loss 2.0306410789489746 - moving ave loss 2.0887191402716954\n",
            "step 1370 - loss 1.6083121299743652 - moving ave loss 2.0406784392419626\n",
            "step 1371 - loss 2.117908477783203 - moving ave loss 2.0484014430960866\n",
            "step 1372 - loss 1.909353494644165 - moving ave loss 2.0344966482508946\n",
            "step 1373 - loss 2.3263278007507324 - moving ave loss 2.0636797635008786\n",
            "step 1374 - loss 2.099159002304077 - moving ave loss 2.0672276873811986\n",
            "step 1375 - loss 2.1960041522979736 - moving ave loss 2.080105333872876\n",
            "Checkpoint at step 1375\n",
            "step 1376 - loss 2.0156617164611816 - moving ave loss 2.073660972131707\n",
            "step 1377 - loss 1.792823076248169 - moving ave loss 2.0455771825433535\n",
            "step 1378 - loss 2.117121934890747 - moving ave loss 2.052731657778093\n",
            "step 1379 - loss 1.86182701587677 - moving ave loss 2.0336411935879606\n",
            "step 1380 - loss 1.7390094995498657 - moving ave loss 2.004178024184151\n",
            "Finish 60 epoch(es)\n",
            "step 1381 - loss 2.3823695182800293 - moving ave loss 2.041997173593739\n",
            "step 1382 - loss 1.7035959959030151 - moving ave loss 2.0081570558246664\n",
            "step 1383 - loss 2.268354892730713 - moving ave loss 2.034176839515271\n",
            "step 1384 - loss 2.1696345806121826 - moving ave loss 2.047722613624962\n",
            "step 1385 - loss 2.5070595741271973 - moving ave loss 2.0936563096751857\n",
            "step 1386 - loss 2.3086161613464355 - moving ave loss 2.1151522948423107\n",
            "step 1387 - loss 2.5495035648345947 - moving ave loss 2.1585874218415393\n",
            "step 1388 - loss 1.737949252128601 - moving ave loss 2.1165236048702454\n",
            "step 1389 - loss 1.9181616306304932 - moving ave loss 2.0966874074462702\n",
            "step 1390 - loss 1.781426191329956 - moving ave loss 2.065161285834639\n",
            "step 1391 - loss 2.1197590827941895 - moving ave loss 2.070621065530594\n",
            "step 1392 - loss 1.5570224523544312 - moving ave loss 2.019261204212978\n",
            "step 1393 - loss 1.897627353668213 - moving ave loss 2.0070978191585014\n",
            "step 1394 - loss 2.241715908050537 - moving ave loss 2.030559628047705\n",
            "step 1395 - loss 2.218397855758667 - moving ave loss 2.0493434508188013\n",
            "step 1396 - loss 1.962558388710022 - moving ave loss 2.0406649446079235\n",
            "step 1397 - loss 1.866424798965454 - moving ave loss 2.0232409300436767\n",
            "step 1398 - loss 1.568488359451294 - moving ave loss 1.9777656729844384\n",
            "step 1399 - loss 1.5643210411071777 - moving ave loss 1.9364212097967124\n",
            "step 1400 - loss 2.5945358276367188 - moving ave loss 2.002232671580713\n",
            "step 1401 - loss 1.7593611478805542 - moving ave loss 1.9779455192106972\n",
            "step 1402 - loss 1.440830945968628 - moving ave loss 1.9242340618864902\n",
            "step 1403 - loss 2.5229945182800293 - moving ave loss 1.984110107525844\n",
            "Finish 61 epoch(es)\n",
            "step 1404 - loss 2.161362648010254 - moving ave loss 2.001835361574285\n",
            "step 1405 - loss 1.8618894815444946 - moving ave loss 1.987840773571306\n",
            "step 1406 - loss 2.1232552528381348 - moving ave loss 2.001382221497989\n",
            "step 1407 - loss 1.6297128200531006 - moving ave loss 1.9642152813534999\n",
            "step 1408 - loss 2.3743739128112793 - moving ave loss 2.005231144499278\n",
            "step 1409 - loss 1.961908221244812 - moving ave loss 2.0008988521738313\n",
            "step 1410 - loss 1.9487419128417969 - moving ave loss 1.9956831582406278\n",
            "step 1411 - loss 1.8509844541549683 - moving ave loss 1.981213287832062\n",
            "step 1412 - loss 1.8403162956237793 - moving ave loss 1.967123588611234\n",
            "step 1413 - loss 2.109800338745117 - moving ave loss 1.9813912636246225\n",
            "step 1414 - loss 2.0567193031311035 - moving ave loss 1.9889240675752706\n",
            "step 1415 - loss 2.007631778717041 - moving ave loss 1.9907948386894476\n",
            "step 1416 - loss 1.9001638889312744 - moving ave loss 1.9817317437136304\n",
            "step 1417 - loss 2.3062777519226074 - moving ave loss 2.014186344534528\n",
            "step 1418 - loss 1.868289828300476 - moving ave loss 1.9995966929111229\n",
            "step 1419 - loss 2.295130491256714 - moving ave loss 2.029150072745682\n",
            "step 1420 - loss 2.1717288494110107 - moving ave loss 2.043407950412215\n",
            "step 1421 - loss 2.193220615386963 - moving ave loss 2.05838921690969\n",
            "step 1422 - loss 2.3525941371917725 - moving ave loss 2.087809708937898\n",
            "step 1423 - loss 2.013363838195801 - moving ave loss 2.0803651218636885\n",
            "step 1424 - loss 1.9732381105422974 - moving ave loss 2.0696524207315496\n",
            "step 1425 - loss 2.0882906913757324 - moving ave loss 2.071516247795968\n",
            "step 1426 - loss 1.8857663869857788 - moving ave loss 2.052941261714949\n",
            "Finish 62 epoch(es)\n",
            "step 1427 - loss 1.983616590499878 - moving ave loss 2.0460087945934418\n",
            "step 1428 - loss 2.0397934913635254 - moving ave loss 2.0453872642704503\n",
            "step 1429 - loss 2.0261785984039307 - moving ave loss 2.0434663976837983\n",
            "step 1430 - loss 1.8081724643707275 - moving ave loss 2.0199370043524913\n",
            "step 1431 - loss 1.827881097793579 - moving ave loss 2.0007314136966\n",
            "step 1432 - loss 2.0114834308624268 - moving ave loss 2.001806615413183\n",
            "step 1433 - loss 1.991021752357483 - moving ave loss 2.000728129107613\n",
            "step 1434 - loss 1.6241238117218018 - moving ave loss 1.9630676973690322\n",
            "step 1435 - loss 1.8232561349868774 - moving ave loss 1.9490865411308167\n",
            "step 1436 - loss 1.6985678672790527 - moving ave loss 1.9240346737456404\n",
            "step 1437 - loss 1.6959583759307861 - moving ave loss 1.901227043964155\n",
            "step 1438 - loss 2.4757676124572754 - moving ave loss 1.958681100813467\n",
            "step 1439 - loss 1.938833475112915 - moving ave loss 1.956696338243412\n",
            "step 1440 - loss 1.8095813989639282 - moving ave loss 1.9419848443154637\n",
            "step 1441 - loss 2.1135268211364746 - moving ave loss 1.959139041997565\n",
            "step 1442 - loss 1.954005241394043 - moving ave loss 1.9586256619372129\n",
            "step 1443 - loss 2.238194465637207 - moving ave loss 1.9865825423072123\n",
            "step 1444 - loss 2.1883716583251953 - moving ave loss 2.006761453909011\n",
            "step 1445 - loss 1.6702255010604858 - moving ave loss 1.9731078586241586\n",
            "step 1446 - loss 2.056804895401001 - moving ave loss 1.9814775623018428\n",
            "step 1447 - loss 2.106105327606201 - moving ave loss 1.9939403388322785\n",
            "step 1448 - loss 1.4624788761138916 - moving ave loss 1.9407941925604397\n",
            "step 1449 - loss 2.4128377437591553 - moving ave loss 1.9879985476803113\n",
            "Finish 63 epoch(es)\n",
            "step 1450 - loss 1.9690253734588623 - moving ave loss 1.9861012302581664\n",
            "step 1451 - loss 2.0748989582061768 - moving ave loss 1.9949810030529675\n",
            "step 1452 - loss 1.8919034004211426 - moving ave loss 1.984673242789785\n",
            "step 1453 - loss 1.67952299118042 - moving ave loss 1.9541582176288486\n",
            "step 1454 - loss 2.22501802444458 - moving ave loss 1.9812441983104219\n",
            "step 1455 - loss 1.9064555168151855 - moving ave loss 1.9737653301608984\n",
            "step 1456 - loss 2.284493923187256 - moving ave loss 2.004838189463534\n",
            "step 1457 - loss 1.7678608894348145 - moving ave loss 1.9811404594606623\n",
            "step 1458 - loss 1.4927690029144287 - moving ave loss 1.932303313806039\n",
            "step 1459 - loss 2.1454899311065674 - moving ave loss 1.953621975536092\n",
            "step 1460 - loss 2.2345144748687744 - moving ave loss 1.9817112254693603\n",
            "step 1461 - loss 2.650075912475586 - moving ave loss 2.048547694169983\n",
            "step 1462 - loss 1.987078309059143 - moving ave loss 2.0424007556588992\n",
            "step 1463 - loss 1.8992857933044434 - moving ave loss 2.0280892594234534\n",
            "step 1464 - loss 1.9791641235351562 - moving ave loss 2.023196745834624\n",
            "step 1465 - loss 2.146339178085327 - moving ave loss 2.035510989059694\n",
            "step 1466 - loss 2.1709556579589844 - moving ave loss 2.049055455949623\n",
            "step 1467 - loss 2.3535900115966797 - moving ave loss 2.0795089115143286\n",
            "step 1468 - loss 1.7705132961273193 - moving ave loss 2.048609349975628\n",
            "step 1469 - loss 1.6598284244537354 - moving ave loss 2.0097312574234385\n",
            "step 1470 - loss 1.8404884338378906 - moving ave loss 1.9928069750648838\n",
            "step 1471 - loss 1.718961238861084 - moving ave loss 1.965422401444504\n",
            "step 1472 - loss 2.4882872104644775 - moving ave loss 2.0177088823465015\n",
            "Finish 64 epoch(es)\n",
            "step 1473 - loss 2.1454412937164307 - moving ave loss 2.0304821234834947\n",
            "step 1474 - loss 2.538691282272339 - moving ave loss 2.0813030393623793\n",
            "step 1475 - loss 2.0980334281921387 - moving ave loss 2.0829760782453555\n",
            "step 1476 - loss 1.5735989809036255 - moving ave loss 2.0320383685111825\n",
            "step 1477 - loss 2.0517663955688477 - moving ave loss 2.034011171216949\n",
            "step 1478 - loss 2.2266275882720947 - moving ave loss 2.0532728129224638\n",
            "step 1479 - loss 2.106009006500244 - moving ave loss 2.0585464322802416\n",
            "step 1480 - loss 2.011984348297119 - moving ave loss 2.0538902238819294\n",
            "step 1481 - loss 2.1114704608917236 - moving ave loss 2.059648247582909\n",
            "step 1482 - loss 1.5610454082489014 - moving ave loss 2.0097879636495084\n",
            "step 1483 - loss 2.016077756881714 - moving ave loss 2.010416942972729\n",
            "step 1484 - loss 1.9962900876998901 - moving ave loss 2.0090042574454454\n",
            "step 1485 - loss 1.7758111953735352 - moving ave loss 1.9856849512382544\n",
            "step 1486 - loss 2.106602907180786 - moving ave loss 1.9977767468325076\n",
            "step 1487 - loss 2.097439765930176 - moving ave loss 2.0077430487422747\n",
            "step 1488 - loss 2.146106004714966 - moving ave loss 2.021579344339544\n",
            "step 1489 - loss 1.9497789144515991 - moving ave loss 2.0143993013507497\n",
            "step 1490 - loss 2.1209990978240967 - moving ave loss 2.0250592809980845\n",
            "step 1491 - loss 1.8120840787887573 - moving ave loss 2.003761760777152\n",
            "step 1492 - loss 2.0068466663360596 - moving ave loss 2.004070251333043\n",
            "step 1493 - loss 1.6150351762771606 - moving ave loss 1.9651667438274547\n",
            "step 1494 - loss 2.1023571491241455 - moving ave loss 1.978885784357124\n",
            "step 1495 - loss 1.9238359928131104 - moving ave loss 1.9733808052027226\n",
            "Finish 65 epoch(es)\n",
            "step 1496 - loss 1.9079978466033936 - moving ave loss 1.9668425093427897\n",
            "step 1497 - loss 2.3655881881713867 - moving ave loss 2.0067170772256495\n",
            "step 1498 - loss 1.9715793132781982 - moving ave loss 2.0032033008309047\n",
            "step 1499 - loss 2.0676660537719727 - moving ave loss 2.0096495761250113\n",
            "step 1500 - loss 1.9841665029525757 - moving ave loss 2.0071012688077676\n",
            "Checkpoint at step 1500\n",
            "step 1501 - loss 2.1857683658599854 - moving ave loss 2.0249679785129895\n",
            "step 1502 - loss 2.0284061431884766 - moving ave loss 2.025311794980538\n",
            "step 1503 - loss 1.843237280845642 - moving ave loss 2.0071043435670486\n",
            "step 1504 - loss 1.6534216403961182 - moving ave loss 1.9717360732499556\n",
            "step 1505 - loss 2.2122771739959717 - moving ave loss 1.9957901833245573\n",
            "step 1506 - loss 1.9759020805358887 - moving ave loss 1.9938013730456905\n",
            "step 1507 - loss 2.026128053665161 - moving ave loss 1.9970340411076377\n",
            "step 1508 - loss 1.8664692640304565 - moving ave loss 1.9839775633999197\n",
            "step 1509 - loss 2.1000237464904785 - moving ave loss 1.9955821817089756\n",
            "step 1510 - loss 2.3507437705993652 - moving ave loss 2.0310983405980148\n",
            "step 1511 - loss 2.0942344665527344 - moving ave loss 2.037411953193487\n",
            "step 1512 - loss 1.8210666179656982 - moving ave loss 2.015777419670708\n",
            "step 1513 - loss 1.782573938369751 - moving ave loss 1.9924570715406122\n",
            "step 1514 - loss 1.9952569007873535 - moving ave loss 1.9927370544652865\n",
            "step 1515 - loss 1.4375216960906982 - moving ave loss 1.9372155186278277\n",
            "step 1516 - loss 1.8107292652130127 - moving ave loss 1.9245668932863462\n",
            "step 1517 - loss 2.1987080574035645 - moving ave loss 1.951981009698068\n",
            "step 1518 - loss 1.8302528858184814 - moving ave loss 1.9398081973101096\n",
            "Finish 66 epoch(es)\n",
            "step 1519 - loss 2.483177661895752 - moving ave loss 1.994145143768674\n",
            "step 1520 - loss 2.025012969970703 - moving ave loss 1.997231926388877\n",
            "step 1521 - loss 2.1076459884643555 - moving ave loss 2.008273332596425\n",
            "step 1522 - loss 1.8891295194625854 - moving ave loss 1.996358951283041\n",
            "step 1523 - loss 1.8540624380111694 - moving ave loss 1.982129299955854\n",
            "step 1524 - loss 2.2693405151367188 - moving ave loss 2.0108504214739407\n",
            "step 1525 - loss 2.031702756881714 - moving ave loss 2.012935655014718\n",
            "step 1526 - loss 1.82429838180542 - moving ave loss 1.9940719276937884\n",
            "step 1527 - loss 2.3039376735687256 - moving ave loss 2.025058502281282\n",
            "step 1528 - loss 2.119403839111328 - moving ave loss 2.0344930359642865\n",
            "step 1529 - loss 2.270723342895508 - moving ave loss 2.0581160666574085\n",
            "step 1530 - loss 2.249741554260254 - moving ave loss 2.0772786154176934\n",
            "step 1531 - loss 1.769718050956726 - moving ave loss 2.046522558971597\n",
            "step 1532 - loss 2.1187353134155273 - moving ave loss 2.05374383441599\n",
            "step 1533 - loss 1.686744213104248 - moving ave loss 2.017043872284816\n",
            "step 1534 - loss 1.414121389389038 - moving ave loss 1.9567516239952383\n",
            "step 1535 - loss 2.023439884185791 - moving ave loss 1.9634204500142936\n",
            "step 1536 - loss 2.254652976989746 - moving ave loss 1.992543702711839\n",
            "step 1537 - loss 1.7216283082962036 - moving ave loss 1.9654521632702755\n",
            "step 1538 - loss 2.071737289428711 - moving ave loss 1.9760806758861191\n",
            "step 1539 - loss 1.8170835971832275 - moving ave loss 1.96018096801583\n",
            "step 1540 - loss 1.656618595123291 - moving ave loss 1.9298247307265761\n",
            "step 1541 - loss 2.0334701538085938 - moving ave loss 1.940189273034778\n",
            "Finish 67 epoch(es)\n",
            "step 1542 - loss 1.9952449798583984 - moving ave loss 1.9456948437171402\n",
            "step 1543 - loss 1.4443707466125488 - moving ave loss 1.895562434006681\n",
            "step 1544 - loss 1.8630013465881348 - moving ave loss 1.8923063252648264\n",
            "step 1545 - loss 1.7599024772644043 - moving ave loss 1.8790659404647845\n",
            "step 1546 - loss 2.49855899810791 - moving ave loss 1.9410152462290973\n",
            "step 1547 - loss 2.2794759273529053 - moving ave loss 1.9748613143414782\n",
            "step 1548 - loss 1.8067691326141357 - moving ave loss 1.958052096168744\n",
            "step 1549 - loss 1.7502660751342773 - moving ave loss 1.9372734940652974\n",
            "step 1550 - loss 2.020740270614624 - moving ave loss 1.9456201717202302\n",
            "step 1551 - loss 2.144191265106201 - moving ave loss 1.9654772810588272\n",
            "step 1552 - loss 2.1264395713806152 - moving ave loss 1.9815735100910061\n",
            "step 1553 - loss 2.139507293701172 - moving ave loss 1.9973668884520228\n",
            "step 1554 - loss 2.266538143157959 - moving ave loss 2.0242840139226166\n",
            "step 1555 - loss 1.9660799503326416 - moving ave loss 2.018463607563619\n",
            "step 1556 - loss 1.889359474182129 - moving ave loss 2.0055531942254703\n",
            "step 1557 - loss 2.324911117553711 - moving ave loss 2.0374889865582944\n",
            "step 1558 - loss 1.9749314785003662 - moving ave loss 2.031233235752502\n",
            "step 1559 - loss 1.349395990371704 - moving ave loss 1.9630495112144222\n",
            "step 1560 - loss 1.8756685256958008 - moving ave loss 1.9543114126625603\n",
            "step 1561 - loss 2.193878173828125 - moving ave loss 1.9782680887791169\n",
            "step 1562 - loss 1.877626895904541 - moving ave loss 1.9682039694916593\n",
            "step 1563 - loss 1.7401992082595825 - moving ave loss 1.9454034933684516\n",
            "step 1564 - loss 2.6053574085235596 - moving ave loss 2.0113988848839623\n",
            "Finish 68 epoch(es)\n",
            "step 1565 - loss 2.4911041259765625 - moving ave loss 2.0593694089932226\n",
            "step 1566 - loss 2.2046518325805664 - moving ave loss 2.073897651351957\n",
            "step 1567 - loss 1.8502774238586426 - moving ave loss 2.0515356286026254\n",
            "step 1568 - loss 1.7634081840515137 - moving ave loss 2.022722884147514\n",
            "step 1569 - loss 1.84414803981781 - moving ave loss 2.004865399714544\n",
            "step 1570 - loss 2.5074195861816406 - moving ave loss 2.0551208183612535\n",
            "step 1571 - loss 2.0379371643066406 - moving ave loss 2.053402452955792\n",
            "step 1572 - loss 1.74729585647583 - moving ave loss 2.022791793307796\n",
            "step 1573 - loss 1.843277931213379 - moving ave loss 2.0048404070983543\n",
            "step 1574 - loss 1.6280698776245117 - moving ave loss 1.96716335415097\n",
            "step 1575 - loss 1.896754264831543 - moving ave loss 1.9601224452190276\n",
            "step 1576 - loss 2.1266543865203857 - moving ave loss 1.9767756393491633\n",
            "step 1577 - loss 1.5647271871566772 - moving ave loss 1.935570794129915\n",
            "step 1578 - loss 1.959485411643982 - moving ave loss 1.9379622558813216\n",
            "step 1579 - loss 2.161497116088867 - moving ave loss 1.9603157419020762\n",
            "step 1580 - loss 1.96492338180542 - moving ave loss 1.9607765058924107\n",
            "step 1581 - loss 1.7428884506225586 - moving ave loss 1.9389877003654257\n",
            "step 1582 - loss 1.8354856967926025 - moving ave loss 1.9286375000081435\n",
            "step 1583 - loss 1.905583381652832 - moving ave loss 1.9263320881726123\n",
            "step 1584 - loss 2.4173452854156494 - moving ave loss 1.975433407896916\n",
            "step 1585 - loss 1.9971072673797607 - moving ave loss 1.9776007938452005\n",
            "step 1586 - loss 1.8256202936172485 - moving ave loss 1.9624027438224054\n",
            "step 1587 - loss 1.6447999477386475 - moving ave loss 1.9306424642140296\n",
            "Finish 69 epoch(es)\n",
            "step 1588 - loss 2.01358699798584 - moving ave loss 1.9389369175912106\n",
            "step 1589 - loss 1.8223695755004883 - moving ave loss 1.9272801833821385\n",
            "step 1590 - loss 1.9738566875457764 - moving ave loss 1.9319378337985023\n",
            "step 1591 - loss 1.791835069656372 - moving ave loss 1.917927557384289\n",
            "step 1592 - loss 1.8184099197387695 - moving ave loss 1.9079757936197372\n",
            "step 1593 - loss 1.8091936111450195 - moving ave loss 1.8980975753722655\n",
            "step 1594 - loss 1.8564784526824951 - moving ave loss 1.8939356631032884\n",
            "step 1595 - loss 1.878237247467041 - moving ave loss 1.8923658215396637\n",
            "step 1596 - loss 2.111077308654785 - moving ave loss 1.914236970251176\n",
            "step 1597 - loss 2.121837615966797 - moving ave loss 1.934997034822738\n",
            "step 1598 - loss 2.2082481384277344 - moving ave loss 1.9623221451832378\n",
            "step 1599 - loss 2.1950106620788574 - moving ave loss 1.9855909968727998\n",
            "step 1600 - loss 1.9916596412658691 - moving ave loss 1.9861978613121067\n",
            "step 1601 - loss 1.8106931447982788 - moving ave loss 1.968647389660724\n",
            "step 1602 - loss 2.3129031658172607 - moving ave loss 2.0030729672763776\n",
            "step 1603 - loss 1.8448913097381592 - moving ave loss 1.9872548015225557\n",
            "step 1604 - loss 2.044353485107422 - moving ave loss 1.9929646698810424\n",
            "step 1605 - loss 2.0115718841552734 - moving ave loss 1.9948253913084657\n",
            "step 1606 - loss 1.7217624187469482 - moving ave loss 1.967519094052314\n",
            "step 1607 - loss 1.81913161277771 - moving ave loss 1.9526803459248536\n",
            "step 1608 - loss 1.9268900156021118 - moving ave loss 1.9501013128925795\n",
            "step 1609 - loss 2.0728883743286133 - moving ave loss 1.962380019036183\n",
            "step 1610 - loss 1.533186912536621 - moving ave loss 1.919460708386227\n",
            "Finish 70 epoch(es)\n",
            "step 1611 - loss 2.1079440116882324 - moving ave loss 1.9383090387164275\n",
            "step 1612 - loss 1.8598754405975342 - moving ave loss 1.930465678904538\n",
            "step 1613 - loss 1.8171201944351196 - moving ave loss 1.9191311304575962\n",
            "step 1614 - loss 1.9259580373764038 - moving ave loss 1.9198138211494769\n",
            "step 1615 - loss 2.0163302421569824 - moving ave loss 1.9294654632502275\n",
            "step 1616 - loss 1.5632227659225464 - moving ave loss 1.8928411935174596\n",
            "step 1617 - loss 1.7822701930999756 - moving ave loss 1.8817840934757113\n",
            "step 1618 - loss 1.632960557937622 - moving ave loss 1.8569017399219023\n",
            "step 1619 - loss 1.8743535280227661 - moving ave loss 1.858646918731989\n",
            "step 1620 - loss 2.033977508544922 - moving ave loss 1.8761799777132824\n",
            "step 1621 - loss 2.079801082611084 - moving ave loss 1.8965420882030626\n",
            "step 1622 - loss 1.6708405017852783 - moving ave loss 1.8739719295612842\n",
            "step 1623 - loss 1.7166072130203247 - moving ave loss 1.8582354579071882\n",
            "step 1624 - loss 2.0976386070251465 - moving ave loss 1.8821757728189843\n",
            "step 1625 - loss 2.3113903999328613 - moving ave loss 1.9250972355303722\n",
            "Checkpoint at step 1625\n",
            "step 1626 - loss 1.7768512964248657 - moving ave loss 1.9102726416198217\n",
            "step 1627 - loss 2.0052783489227295 - moving ave loss 1.9197732123501126\n",
            "step 1628 - loss 2.152984619140625 - moving ave loss 1.9430943530291638\n",
            "step 1629 - loss 1.9223363399505615 - moving ave loss 1.9410185517213034\n",
            "step 1630 - loss 1.8954722881317139 - moving ave loss 1.9364639253623446\n",
            "step 1631 - loss 1.5113348960876465 - moving ave loss 1.8939510224348748\n",
            "step 1632 - loss 1.9636032581329346 - moving ave loss 1.900916246004681\n",
            "step 1633 - loss 2.3092501163482666 - moving ave loss 1.9417496330390396\n",
            "Finish 71 epoch(es)\n",
            "step 1634 - loss 1.6086223125457764 - moving ave loss 1.9084369009897135\n",
            "step 1635 - loss 1.8208506107330322 - moving ave loss 1.8996782719640453\n",
            "step 1636 - loss 1.572493314743042 - moving ave loss 1.866959776241945\n",
            "step 1637 - loss 1.721951961517334 - moving ave loss 1.8524589947694838\n",
            "step 1638 - loss 2.000871181488037 - moving ave loss 1.8673002134413392\n",
            "step 1639 - loss 1.6630011796951294 - moving ave loss 1.8468703100667183\n",
            "step 1640 - loss 1.684631586074829 - moving ave loss 1.8306464376675295\n",
            "step 1641 - loss 1.9687538146972656 - moving ave loss 1.8444571753705032\n",
            "step 1642 - loss 2.110523223876953 - moving ave loss 1.8710637802211485\n",
            "step 1643 - loss 1.4995081424713135 - moving ave loss 1.833908216446165\n",
            "step 1644 - loss 1.8334901332855225 - moving ave loss 1.8338664081301008\n",
            "step 1645 - loss 1.9708468914031982 - moving ave loss 1.8475644564574107\n",
            "step 1646 - loss 1.7397915124893188 - moving ave loss 1.8367871620606016\n",
            "step 1647 - loss 1.96699857711792 - moving ave loss 1.8498083035663335\n",
            "step 1648 - loss 1.8113956451416016 - moving ave loss 1.8459670377238604\n",
            "step 1649 - loss 1.7497981786727905 - moving ave loss 1.8363501518187535\n",
            "step 1650 - loss 2.0805623531341553 - moving ave loss 1.8607713719502939\n",
            "step 1651 - loss 1.744268536567688 - moving ave loss 1.8491210884120333\n",
            "step 1652 - loss 2.2393851280212402 - moving ave loss 1.888147492372954\n",
            "step 1653 - loss 1.8442977666854858 - moving ave loss 1.8837625198042072\n",
            "step 1654 - loss 1.5562547445297241 - moving ave loss 1.851011742276759\n",
            "step 1655 - loss 2.06207013130188 - moving ave loss 1.8721175811792712\n",
            "step 1656 - loss 2.0034022331237793 - moving ave loss 1.885246046373722\n",
            "Finish 72 epoch(es)\n",
            "step 1657 - loss 1.8773767948150635 - moving ave loss 1.8844591212178563\n",
            "step 1658 - loss 1.79999577999115 - moving ave loss 1.8760127870951857\n",
            "step 1659 - loss 1.4713020324707031 - moving ave loss 1.8355417116327375\n",
            "step 1660 - loss 2.60241961479187 - moving ave loss 1.9122295019486508\n",
            "step 1661 - loss 1.9515659809112549 - moving ave loss 1.9161631498449112\n",
            "step 1662 - loss 1.5342711210250854 - moving ave loss 1.8779739469629286\n",
            "step 1663 - loss 1.966058373451233 - moving ave loss 1.8867823896117593\n",
            "step 1664 - loss 2.0455923080444336 - moving ave loss 1.9026633814550267\n",
            "step 1665 - loss 1.9147133827209473 - moving ave loss 1.9038683815816189\n",
            "step 1666 - loss 1.6728174686431885 - moving ave loss 1.8807632902877758\n",
            "step 1667 - loss 1.7425155639648438 - moving ave loss 1.8669385176554827\n",
            "step 1668 - loss 1.6807491779327393 - moving ave loss 1.8483195836832083\n",
            "step 1669 - loss 1.916844367980957 - moving ave loss 1.855172062112983\n",
            "step 1670 - loss 2.0296006202697754 - moving ave loss 1.8726149179286624\n",
            "step 1671 - loss 1.6218409538269043 - moving ave loss 1.8475375215184868\n",
            "step 1672 - loss 2.0885512828826904 - moving ave loss 1.8716388976549072\n",
            "step 1673 - loss 1.7602840662002563 - moving ave loss 1.8605034145094423\n",
            "step 1674 - loss 2.0563650131225586 - moving ave loss 1.880089574370754\n",
            "step 1675 - loss 1.9983640909194946 - moving ave loss 1.8919170260256282\n",
            "step 1676 - loss 2.425737142562866 - moving ave loss 1.9452990376793522\n",
            "step 1677 - loss 1.7714643478393555 - moving ave loss 1.9279155686953526\n",
            "step 1678 - loss 2.0323901176452637 - moving ave loss 1.9383630235903437\n",
            "step 1679 - loss 1.9089865684509277 - moving ave loss 1.9354253780764021\n",
            "Finish 73 epoch(es)\n",
            "step 1680 - loss 1.7916538715362549 - moving ave loss 1.9210482274223875\n",
            "step 1681 - loss 1.6546106338500977 - moving ave loss 1.8944044680651586\n",
            "step 1682 - loss 1.8644890785217285 - moving ave loss 1.8914129291108157\n",
            "step 1683 - loss 1.759934425354004 - moving ave loss 1.8782650787351345\n",
            "step 1684 - loss 2.1914775371551514 - moving ave loss 1.909586324577136\n",
            "step 1685 - loss 2.291719675064087 - moving ave loss 1.9477996596258313\n",
            "step 1686 - loss 1.9801111221313477 - moving ave loss 1.951030805876383\n",
            "step 1687 - loss 1.860837459564209 - moving ave loss 1.9420114712451655\n",
            "step 1688 - loss 1.9364720582962036 - moving ave loss 1.9414575299502694\n",
            "step 1689 - loss 1.9406229257583618 - moving ave loss 1.9413740695310786\n",
            "step 1690 - loss 1.9386882781982422 - moving ave loss 1.941105490397795\n",
            "step 1691 - loss 1.9847192764282227 - moving ave loss 1.9454668690008377\n",
            "step 1692 - loss 1.7888319492340088 - moving ave loss 1.9298033770241547\n",
            "step 1693 - loss 1.31369149684906 - moving ave loss 1.8681921890066453\n",
            "step 1694 - loss 2.031806230545044 - moving ave loss 1.884553593160485\n",
            "step 1695 - loss 1.860009789466858 - moving ave loss 1.8820992127911225\n",
            "step 1696 - loss 1.5474131107330322 - moving ave loss 1.8486306025853136\n",
            "step 1697 - loss 2.04512357711792 - moving ave loss 1.8682799000385744\n",
            "step 1698 - loss 1.9213752746582031 - moving ave loss 1.8735894375005373\n",
            "step 1699 - loss 2.0064890384674072 - moving ave loss 1.8868793975972242\n",
            "step 1700 - loss 1.6658766269683838 - moving ave loss 1.8647791205343403\n",
            "step 1701 - loss 1.5789430141448975 - moving ave loss 1.8361955098953961\n",
            "step 1702 - loss 2.175504207611084 - moving ave loss 1.870126379666965\n",
            "Finish 74 epoch(es)\n",
            "step 1703 - loss 2.016295909881592 - moving ave loss 1.8847433326884275\n",
            "step 1704 - loss 1.5213935375213623 - moving ave loss 1.848408353171721\n",
            "step 1705 - loss 1.4835236072540283 - moving ave loss 1.811919878579952\n",
            "step 1706 - loss 1.9341763257980347 - moving ave loss 1.8241455233017603\n",
            "step 1707 - loss 1.6838277578353882 - moving ave loss 1.8101137467551232\n",
            "step 1708 - loss 1.999855637550354 - moving ave loss 1.8290879358346464\n",
            "step 1709 - loss 1.7302253246307373 - moving ave loss 1.8192016747142556\n",
            "step 1710 - loss 1.7461702823638916 - moving ave loss 1.8118985354792194\n",
            "step 1711 - loss 2.7962353229522705 - moving ave loss 1.9103322142265244\n",
            "step 1712 - loss 1.801476001739502 - moving ave loss 1.8994465929778221\n",
            "step 1713 - loss 1.9600396156311035 - moving ave loss 1.9055058952431503\n",
            "step 1714 - loss 1.939511775970459 - moving ave loss 1.9089064833158813\n",
            "step 1715 - loss 2.0497641563415527 - moving ave loss 1.9229922506184485\n",
            "step 1716 - loss 1.702521800994873 - moving ave loss 1.9009452056560912\n",
            "step 1717 - loss 1.5408906936645508 - moving ave loss 1.8649397544569373\n",
            "step 1718 - loss 1.878425121307373 - moving ave loss 1.8662882911419807\n",
            "step 1719 - loss 2.4194822311401367 - moving ave loss 1.9216076851417965\n",
            "step 1720 - loss 1.8472801446914673 - moving ave loss 1.9141749310967637\n",
            "step 1721 - loss 1.5781030654907227 - moving ave loss 1.8805677445361597\n",
            "step 1722 - loss 1.5379279851913452 - moving ave loss 1.8463037686016783\n",
            "step 1723 - loss 1.71307373046875 - moving ave loss 1.8329807647883856\n",
            "step 1724 - loss 1.7928829193115234 - moving ave loss 1.8289709802406993\n",
            "step 1725 - loss 1.5548436641693115 - moving ave loss 1.8015582486335606\n",
            "Finish 75 epoch(es)\n",
            "step 1726 - loss 1.8257508277893066 - moving ave loss 1.8039775065491352\n",
            "step 1727 - loss 1.9616565704345703 - moving ave loss 1.8197454129376789\n",
            "step 1728 - loss 1.610375165939331 - moving ave loss 1.798808388237844\n",
            "step 1729 - loss 2.194877862930298 - moving ave loss 1.8384153357070894\n",
            "step 1730 - loss 2.3414688110351562 - moving ave loss 1.8887206832398962\n",
            "step 1731 - loss 1.6042094230651855 - moving ave loss 1.8602695572224253\n",
            "step 1732 - loss 1.6466717720031738 - moving ave loss 1.8389097787005002\n",
            "step 1733 - loss 2.1188278198242188 - moving ave loss 1.866901582812872\n",
            "step 1734 - loss 1.5945020914077759 - moving ave loss 1.8396616336723626\n",
            "step 1735 - loss 1.3867011070251465 - moving ave loss 1.7943655810076409\n",
            "step 1736 - loss 1.4907312393188477 - moving ave loss 1.7640021468387617\n",
            "step 1737 - loss 1.567210078239441 - moving ave loss 1.7443229399788296\n",
            "step 1738 - loss 1.7276203632354736 - moving ave loss 1.742652682304494\n",
            "step 1739 - loss 2.1814064979553223 - moving ave loss 1.786528063869577\n",
            "step 1740 - loss 1.3409653902053833 - moving ave loss 1.7419717965031578\n",
            "step 1741 - loss 1.7906510829925537 - moving ave loss 1.7468397251520975\n",
            "step 1742 - loss 1.9783375263214111 - moving ave loss 1.769989505269029\n",
            "step 1743 - loss 1.848905324935913 - moving ave loss 1.7778810872357176\n",
            "step 1744 - loss 1.5981554985046387 - moving ave loss 1.7599085283626097\n",
            "step 1745 - loss 2.131892681121826 - moving ave loss 1.7971069436385312\n",
            "step 1746 - loss 2.0323405265808105 - moving ave loss 1.8206303019327592\n",
            "step 1747 - loss 1.631727695465088 - moving ave loss 1.801740041285992\n",
            "step 1748 - loss 1.826653003692627 - moving ave loss 1.8042313375266557\n",
            "Finish 76 epoch(es)\n",
            "step 1749 - loss 2.2953455448150635 - moving ave loss 1.8533427582554964\n",
            "step 1750 - loss 1.5929542779922485 - moving ave loss 1.8273039102291717\n",
            "Checkpoint at step 1750\n",
            "step 1751 - loss 1.8689064979553223 - moving ave loss 1.8314641690017868\n",
            "step 1752 - loss 1.7067798376083374 - moving ave loss 1.8189957358624418\n",
            "step 1753 - loss 1.9256253242492676 - moving ave loss 1.8296586947011244\n",
            "step 1754 - loss 1.6542577743530273 - moving ave loss 1.8121186026663147\n",
            "step 1755 - loss 1.7511295080184937 - moving ave loss 1.8060196932015327\n",
            "step 1756 - loss 1.6451902389526367 - moving ave loss 1.789936747776643\n",
            "step 1757 - loss 2.1309454441070557 - moving ave loss 1.8240376174096842\n",
            "step 1758 - loss 1.7851166725158691 - moving ave loss 1.8201455229203027\n",
            "step 1759 - loss 1.7113819122314453 - moving ave loss 1.8092691618514172\n",
            "step 1760 - loss 1.9848551750183105 - moving ave loss 1.8268277631681067\n",
            "step 1761 - loss 1.6909704208374023 - moving ave loss 1.8132420289350364\n",
            "step 1762 - loss 1.4622668027877808 - moving ave loss 1.778144506320311\n",
            "step 1763 - loss 1.7319135665893555 - moving ave loss 1.7735214123472154\n",
            "step 1764 - loss 1.8914055824279785 - moving ave loss 1.7853098293552918\n",
            "step 1765 - loss 1.5958218574523926 - moving ave loss 1.766361032165002\n",
            "step 1766 - loss 2.2346179485321045 - moving ave loss 1.8131867238017123\n",
            "step 1767 - loss 1.5473499298095703 - moving ave loss 1.786603044402498\n",
            "step 1768 - loss 1.7322065830230713 - moving ave loss 1.7811633982645554\n",
            "step 1769 - loss 2.0169854164123535 - moving ave loss 1.8047456000793352\n",
            "step 1770 - loss 1.570691704750061 - moving ave loss 1.781340210546408\n",
            "step 1771 - loss 2.188922882080078 - moving ave loss 1.822098477699775\n",
            "Finish 77 epoch(es)\n",
            "step 1772 - loss 1.849980115890503 - moving ave loss 1.824886641518848\n",
            "step 1773 - loss 1.8182921409606934 - moving ave loss 1.8242271914630326\n",
            "step 1774 - loss 2.0696463584899902 - moving ave loss 1.8487691081657283\n",
            "step 1775 - loss 1.7777563333511353 - moving ave loss 1.8416678306842689\n",
            "step 1776 - loss 1.9073041677474976 - moving ave loss 1.848231464390592\n",
            "step 1777 - loss 2.060870409011841 - moving ave loss 1.869495358852717\n",
            "step 1778 - loss 1.632557988166809 - moving ave loss 1.8458016217841262\n",
            "step 1779 - loss 2.0905938148498535 - moving ave loss 1.870280841090699\n",
            "step 1780 - loss 1.861570954322815 - moving ave loss 1.8694098524139107\n",
            "step 1781 - loss 2.1982510089874268 - moving ave loss 1.9022939680712623\n",
            "step 1782 - loss 1.7818050384521484 - moving ave loss 1.890245075109351\n",
            "step 1783 - loss 1.5074433088302612 - moving ave loss 1.8519648984814419\n",
            "step 1784 - loss 1.8021645545959473 - moving ave loss 1.8469848640928923\n",
            "step 1785 - loss 2.049272298812866 - moving ave loss 1.86721360756489\n",
            "step 1786 - loss 1.6581480503082275 - moving ave loss 1.8463070518392237\n",
            "step 1787 - loss 1.5208196640014648 - moving ave loss 1.8137583130554478\n",
            "step 1788 - loss 1.7499239444732666 - moving ave loss 1.8073748761972297\n",
            "step 1789 - loss 2.07019305229187 - moving ave loss 1.8336566938066938\n",
            "step 1790 - loss 1.9385199546813965 - moving ave loss 1.8441430198941642\n",
            "step 1791 - loss 1.837367057800293 - moving ave loss 1.8434654236847772\n",
            "step 1792 - loss 2.005791187286377 - moving ave loss 1.8596980000449372\n",
            "step 1793 - loss 1.8197935819625854 - moving ave loss 1.8557075582367022\n",
            "step 1794 - loss 1.6332371234893799 - moving ave loss 1.83346051476197\n",
            "Finish 78 epoch(es)\n",
            "step 1795 - loss 1.8992843627929688 - moving ave loss 1.84004289956507\n",
            "step 1796 - loss 1.8165792226791382 - moving ave loss 1.8376965318764769\n",
            "step 1797 - loss 1.7170096635818481 - moving ave loss 1.825627845047014\n",
            "step 1798 - loss 1.9202911853790283 - moving ave loss 1.8350941790802155\n",
            "step 1799 - loss 1.675811767578125 - moving ave loss 1.8191659379300067\n",
            "step 1800 - loss 2.099921941757202 - moving ave loss 1.8472415383127263\n",
            "step 1801 - loss 1.6097347736358643 - moving ave loss 1.8234908618450403\n",
            "step 1802 - loss 2.4576597213745117 - moving ave loss 1.8869077477979874\n",
            "step 1803 - loss 1.7953355312347412 - moving ave loss 1.8777505261416627\n",
            "step 1804 - loss 2.023775339126587 - moving ave loss 1.8923530074401553\n",
            "step 1805 - loss 2.2937021255493164 - moving ave loss 1.9324879192510716\n",
            "step 1806 - loss 1.7499881982803345 - moving ave loss 1.9142379471539979\n",
            "step 1807 - loss 1.5389368534088135 - moving ave loss 1.8767078377794795\n",
            "step 1808 - loss 1.8399221897125244 - moving ave loss 1.873029272972784\n",
            "step 1809 - loss 1.7658134698867798 - moving ave loss 1.8623076926641837\n",
            "step 1810 - loss 1.4468003511428833 - moving ave loss 1.8207569585120538\n",
            "step 1811 - loss 1.6907780170440674 - moving ave loss 1.8077590643652552\n",
            "step 1812 - loss 1.859477162361145 - moving ave loss 1.8129308741648442\n",
            "step 1813 - loss 1.8288874626159668 - moving ave loss 1.8145265330099565\n",
            "step 1814 - loss 1.9765052795410156 - moving ave loss 1.8307244076630624\n",
            "step 1815 - loss 1.6052442789077759 - moving ave loss 1.8081763947875338\n",
            "step 1816 - loss 1.5480031967163086 - moving ave loss 1.7821590749804113\n",
            "step 1817 - loss 1.7052208185195923 - moving ave loss 1.7744652493343294\n",
            "Finish 79 epoch(es)\n",
            "step 1818 - loss 2.041351795196533 - moving ave loss 1.8011539039205497\n",
            "step 1819 - loss 1.5762276649475098 - moving ave loss 1.7786612800232457\n",
            "step 1820 - loss 1.5672556161880493 - moving ave loss 1.7575207136397262\n",
            "step 1821 - loss 1.5543934106826782 - moving ave loss 1.7372079833440215\n",
            "step 1822 - loss 2.427323818206787 - moving ave loss 1.8062195668302983\n",
            "step 1823 - loss 2.0495524406433105 - moving ave loss 1.8305528542115996\n",
            "step 1824 - loss 1.5344741344451904 - moving ave loss 1.8009449822349586\n",
            "step 1825 - loss 1.910457968711853 - moving ave loss 1.8118962808826482\n",
            "step 1826 - loss 2.0555214881896973 - moving ave loss 1.8362588016133532\n",
            "step 1827 - loss 1.8317104578018188 - moving ave loss 1.8358039672321997\n",
            "step 1828 - loss 1.634192943572998 - moving ave loss 1.8156428648662797\n",
            "step 1829 - loss 1.9317976236343384 - moving ave loss 1.8272583407430856\n",
            "step 1830 - loss 2.396792411804199 - moving ave loss 1.8842117478491969\n",
            "step 1831 - loss 1.5378212928771973 - moving ave loss 1.849572702351997\n",
            "step 1832 - loss 2.0171217918395996 - moving ave loss 1.8663276113007574\n",
            "step 1833 - loss 1.6521154642105103 - moving ave loss 1.8449063965917327\n",
            "step 1834 - loss 1.8211562633514404 - moving ave loss 1.8425313832677035\n",
            "step 1835 - loss 1.7564115524291992 - moving ave loss 1.8339194001838532\n",
            "step 1836 - loss 1.9096733331680298 - moving ave loss 1.841494793482271\n",
            "step 1837 - loss 1.785767674446106 - moving ave loss 1.8359220815786546\n",
            "step 1838 - loss 2.0128531455993652 - moving ave loss 1.8536151879807257\n",
            "step 1839 - loss 1.4894258975982666 - moving ave loss 1.8171962589424797\n",
            "step 1840 - loss 1.4093544483184814 - moving ave loss 1.7764120778800798\n",
            "Finish 80 epoch(es)\n",
            "step 1841 - loss 1.7958970069885254 - moving ave loss 1.7783605707909245\n",
            "step 1842 - loss 2.1510300636291504 - moving ave loss 1.8156275200747471\n",
            "step 1843 - loss 1.8659744262695312 - moving ave loss 1.8206622106942256\n",
            "step 1844 - loss 1.8604016304016113 - moving ave loss 1.8246361526649642\n",
            "step 1845 - loss 2.1332669258117676 - moving ave loss 1.8554992299796444\n",
            "step 1846 - loss 1.7089157104492188 - moving ave loss 1.840840878026602\n",
            "step 1847 - loss 1.9225153923034668 - moving ave loss 1.8490083294542885\n",
            "step 1848 - loss 1.6313073635101318 - moving ave loss 1.8272382328598729\n",
            "step 1849 - loss 1.5880333185195923 - moving ave loss 1.8033177414258448\n",
            "step 1850 - loss 1.5971988439559937 - moving ave loss 1.7827058516788596\n",
            "step 1851 - loss 2.0231709480285645 - moving ave loss 1.8067523613138303\n",
            "step 1852 - loss 1.5878231525421143 - moving ave loss 1.7848594404366587\n",
            "step 1853 - loss 1.9107853174209595 - moving ave loss 1.7974520281350888\n",
            "step 1854 - loss 1.642573595046997 - moving ave loss 1.7819641848262795\n",
            "step 1855 - loss 1.9950987100601196 - moving ave loss 1.8032776373496635\n",
            "step 1856 - loss 1.977986454963684 - moving ave loss 1.8207485191110657\n",
            "step 1857 - loss 1.9686297178268433 - moving ave loss 1.8355366389826435\n",
            "step 1858 - loss 1.7159258127212524 - moving ave loss 1.8235755563565046\n",
            "step 1859 - loss 2.1831798553466797 - moving ave loss 1.859535986255522\n",
            "step 1860 - loss 1.5571656227111816 - moving ave loss 1.829298949901088\n",
            "step 1861 - loss 1.8246089220046997 - moving ave loss 1.8288299471114493\n",
            "step 1862 - loss 1.668654203414917 - moving ave loss 1.812812372741796\n",
            "step 1863 - loss 1.7201131582260132 - moving ave loss 1.8035424512902176\n",
            "Finish 81 epoch(es)\n",
            "step 1864 - loss 1.9856374263763428 - moving ave loss 1.8217519487988303\n",
            "step 1865 - loss 1.8574997186660767 - moving ave loss 1.8253267257855548\n",
            "step 1866 - loss 2.0832557678222656 - moving ave loss 1.8511196299892259\n",
            "step 1867 - loss 1.4869219064712524 - moving ave loss 1.8146998576374285\n",
            "step 1868 - loss 1.5188589096069336 - moving ave loss 1.785115762834379\n",
            "step 1869 - loss 1.7010080814361572 - moving ave loss 1.7767049946945568\n",
            "step 1870 - loss 2.1006956100463867 - moving ave loss 1.80910405622974\n",
            "step 1871 - loss 1.828731894493103 - moving ave loss 1.8110668400560763\n",
            "step 1872 - loss 1.65110182762146 - moving ave loss 1.7950703388126146\n",
            "step 1873 - loss 2.0579662322998047 - moving ave loss 1.8213599281613337\n",
            "step 1874 - loss 1.5812079906463623 - moving ave loss 1.7973447344098368\n",
            "step 1875 - loss 1.7560007572174072 - moving ave loss 1.7932103366905938\n",
            "Checkpoint at step 1875\n",
            "step 1876 - loss 1.8765519857406616 - moving ave loss 1.8015445015956006\n",
            "step 1877 - loss 1.662429928779602 - moving ave loss 1.787633044314001\n",
            "step 1878 - loss 1.8560608625411987 - moving ave loss 1.7944758261367206\n",
            "step 1879 - loss 2.1313629150390625 - moving ave loss 1.8281645350269549\n",
            "step 1880 - loss 1.937101125717163 - moving ave loss 1.8390581940959758\n",
            "step 1881 - loss 1.6053102016448975 - moving ave loss 1.8156833948508682\n",
            "step 1882 - loss 1.81867253780365 - moving ave loss 1.8159823091461462\n",
            "step 1883 - loss 1.6447432041168213 - moving ave loss 1.7988583986432136\n",
            "step 1884 - loss 1.668616771697998 - moving ave loss 1.785834235948692\n",
            "step 1885 - loss 2.12115216255188 - moving ave loss 1.819366028609011\n",
            "step 1886 - loss 1.9436464309692383 - moving ave loss 1.8317940688450338\n",
            "Finish 82 epoch(es)\n",
            "step 1887 - loss 1.9471608400344849 - moving ave loss 1.843330745963979\n",
            "step 1888 - loss 1.635146975517273 - moving ave loss 1.8225123689193086\n",
            "step 1889 - loss 1.6383037567138672 - moving ave loss 1.8040915076987645\n",
            "step 1890 - loss 1.741387128829956 - moving ave loss 1.7978210698118837\n",
            "step 1891 - loss 2.1325325965881348 - moving ave loss 1.831292222489509\n",
            "step 1892 - loss 1.4860637187957764 - moving ave loss 1.7967693721201357\n",
            "step 1893 - loss 1.802620768547058 - moving ave loss 1.797354511762828\n",
            "step 1894 - loss 1.6585773229599 - moving ave loss 1.7834767928825352\n",
            "step 1895 - loss 1.8040494918823242 - moving ave loss 1.7855340627825143\n",
            "step 1896 - loss 2.1087141036987305 - moving ave loss 1.8178520668741358\n",
            "step 1897 - loss 1.8408833742141724 - moving ave loss 1.8201551976081396\n",
            "step 1898 - loss 1.8851284980773926 - moving ave loss 1.826652527655065\n",
            "step 1899 - loss 1.9328621625900269 - moving ave loss 1.8372734911485613\n",
            "step 1900 - loss 1.7455309629440308 - moving ave loss 1.8280992383281083\n",
            "step 1901 - loss 1.532711148262024 - moving ave loss 1.7985604293215\n",
            "step 1902 - loss 2.0002856254577637 - moving ave loss 1.8187329489351263\n",
            "step 1903 - loss 1.5272469520568848 - moving ave loss 1.789584349247302\n",
            "step 1904 - loss 1.8427441120147705 - moving ave loss 1.794900325524049\n",
            "step 1905 - loss 1.5758395195007324 - moving ave loss 1.7729942449217173\n",
            "step 1906 - loss 1.407850742340088 - moving ave loss 1.7364798946635545\n",
            "step 1907 - loss 1.7063192129135132 - moving ave loss 1.7334638264885505\n",
            "step 1908 - loss 2.018246650695801 - moving ave loss 1.7619421089092755\n",
            "step 1909 - loss 1.906546950340271 - moving ave loss 1.776402593052375\n",
            "Finish 83 epoch(es)\n",
            "step 1910 - loss 2.101351022720337 - moving ave loss 1.8088974360191712\n",
            "step 1911 - loss 1.9817001819610596 - moving ave loss 1.82617771061336\n",
            "step 1912 - loss 1.6653742790222168 - moving ave loss 1.810097367454246\n",
            "step 1913 - loss 1.6488173007965088 - moving ave loss 1.7939693607884721\n",
            "step 1914 - loss 1.631181240081787 - moving ave loss 1.7776905487178036\n",
            "step 1915 - loss 1.8158345222473145 - moving ave loss 1.7815049460707548\n",
            "step 1916 - loss 2.086617946624756 - moving ave loss 1.8120162461261549\n",
            "step 1917 - loss 1.3459970951080322 - moving ave loss 1.7654143310243426\n",
            "step 1918 - loss 2.1061763763427734 - moving ave loss 1.7994905355561859\n",
            "step 1919 - loss 1.6285525560379028 - moving ave loss 1.7823967376043577\n",
            "step 1920 - loss 1.7215378284454346 - moving ave loss 1.7763108466884654\n",
            "step 1921 - loss 1.8539824485778809 - moving ave loss 1.7840780068774071\n",
            "step 1922 - loss 1.9780921936035156 - moving ave loss 1.803479425550018\n",
            "step 1923 - loss 2.3104336261749268 - moving ave loss 1.854174845612509\n",
            "step 1924 - loss 2.0813539028167725 - moving ave loss 1.8768927513329352\n",
            "step 1925 - loss 1.4715096950531006 - moving ave loss 1.8363544457049519\n",
            "step 1926 - loss 2.1639840602874756 - moving ave loss 1.8691174071632044\n",
            "step 1927 - loss 1.8284404277801514 - moving ave loss 1.8650497092248992\n",
            "step 1928 - loss 2.0223238468170166 - moving ave loss 1.880777122984111\n",
            "step 1929 - loss 1.7330583333969116 - moving ave loss 1.8660052440253914\n",
            "step 1930 - loss 1.368791103363037 - moving ave loss 1.816283829959156\n",
            "step 1931 - loss 1.6100151538848877 - moving ave loss 1.7956569623517291\n",
            "step 1932 - loss 1.6601868867874146 - moving ave loss 1.7821099547952977\n",
            "Finish 84 epoch(es)\n",
            "step 1933 - loss 2.1724886894226074 - moving ave loss 1.8211478282580287\n",
            "step 1934 - loss 2.0288665294647217 - moving ave loss 1.841919698378698\n",
            "step 1935 - loss 1.5491788387298584 - moving ave loss 1.8126456124138142\n",
            "step 1936 - loss 2.108400821685791 - moving ave loss 1.842221133341012\n",
            "step 1937 - loss 1.8327276706695557 - moving ave loss 1.8412717870738664\n",
            "step 1938 - loss 1.948028564453125 - moving ave loss 1.8519474648117924\n",
            "step 1939 - loss 1.6215941905975342 - moving ave loss 1.8289121373903667\n",
            "step 1940 - loss 1.9333345890045166 - moving ave loss 1.8393543825517817\n",
            "step 1941 - loss 2.3128952980041504 - moving ave loss 1.8867084740970186\n",
            "step 1942 - loss 1.7597713470458984 - moving ave loss 1.8740147613919067\n",
            "step 1943 - loss 1.3063149452209473 - moving ave loss 1.8172447797748108\n",
            "step 1944 - loss 1.9124916791915894 - moving ave loss 1.8267694697164887\n",
            "step 1945 - loss 1.7263303995132446 - moving ave loss 1.8167255626961643\n",
            "step 1946 - loss 1.3814034461975098 - moving ave loss 1.7731933510462987\n",
            "step 1947 - loss 1.8238253593444824 - moving ave loss 1.778256551876117\n",
            "step 1948 - loss 1.7391856908798218 - moving ave loss 1.7743494657764876\n",
            "step 1949 - loss 2.0040810108184814 - moving ave loss 1.797322620280687\n",
            "step 1950 - loss 1.5154011249542236 - moving ave loss 1.7691304707480406\n",
            "step 1951 - loss 2.003535509109497 - moving ave loss 1.7925709745841865\n",
            "step 1952 - loss 1.5219080448150635 - moving ave loss 1.7655046816072741\n",
            "step 1953 - loss 1.8682705163955688 - moving ave loss 1.7757812650861038\n",
            "step 1954 - loss 1.3966225385665894 - moving ave loss 1.7378653924341525\n",
            "step 1955 - loss 1.7424317598342896 - moving ave loss 1.7383220291741663\n",
            "Finish 85 epoch(es)\n",
            "step 1956 - loss 2.199808120727539 - moving ave loss 1.7844706383295037\n",
            "step 1957 - loss 1.4189225435256958 - moving ave loss 1.747915828849123\n",
            "step 1958 - loss 2.4786086082458496 - moving ave loss 1.8209851067887957\n",
            "step 1959 - loss 1.7468159198760986 - moving ave loss 1.813568188097526\n",
            "step 1960 - loss 1.6532971858978271 - moving ave loss 1.7975410878775562\n",
            "step 1961 - loss 1.8217947483062744 - moving ave loss 1.7999664539204279\n",
            "step 1962 - loss 1.7683812379837036 - moving ave loss 1.7968079323267556\n",
            "step 1963 - loss 1.7936521768569946 - moving ave loss 1.7964923567797795\n",
            "step 1964 - loss 1.4910554885864258 - moving ave loss 1.765948669960444\n",
            "step 1965 - loss 1.685293436050415 - moving ave loss 1.7578831465694411\n",
            "step 1966 - loss 1.4179855585098267 - moving ave loss 1.7238933877634797\n",
            "step 1967 - loss 1.9630523920059204 - moving ave loss 1.7478092881877236\n",
            "step 1968 - loss 1.967421293258667 - moving ave loss 1.769770488694818\n",
            "step 1969 - loss 1.3588918447494507 - moving ave loss 1.7286826243002813\n",
            "step 1970 - loss 1.8023076057434082 - moving ave loss 1.7360451224445939\n",
            "step 1971 - loss 1.3617818355560303 - moving ave loss 1.6986187937557375\n",
            "step 1972 - loss 1.6813629865646362 - moving ave loss 1.6968932130366274\n",
            "step 1973 - loss 1.810436487197876 - moving ave loss 1.7082475404527524\n",
            "step 1974 - loss 1.2354978322982788 - moving ave loss 1.660972569637305\n",
            "step 1975 - loss 2.0350136756896973 - moving ave loss 1.6983766802425442\n",
            "step 1976 - loss 1.99751877784729 - moving ave loss 1.7282908900030187\n",
            "step 1977 - loss 2.1479403972625732 - moving ave loss 1.7702558407289741\n",
            "step 1978 - loss 1.9665112495422363 - moving ave loss 1.7898813816103005\n",
            "Finish 86 epoch(es)\n",
            "step 1979 - loss 1.3623831272125244 - moving ave loss 1.747131556170523\n",
            "step 1980 - loss 1.5560725927352905 - moving ave loss 1.7280256598269998\n",
            "step 1981 - loss 2.1375441551208496 - moving ave loss 1.7689775093563849\n",
            "step 1982 - loss 1.9250421524047852 - moving ave loss 1.784583973661225\n",
            "step 1983 - loss 1.698664903640747 - moving ave loss 1.7759920666591773\n",
            "step 1984 - loss 1.7152042388916016 - moving ave loss 1.7699132838824196\n",
            "step 1985 - loss 1.5538616180419922 - moving ave loss 1.748308117298377\n",
            "step 1986 - loss 2.3096280097961426 - moving ave loss 1.8044401065481537\n",
            "step 1987 - loss 1.851639986038208 - moving ave loss 1.8091600944971593\n",
            "step 1988 - loss 1.8079371452331543 - moving ave loss 1.8090377995707587\n",
            "step 1989 - loss 2.1131818294525146 - moving ave loss 1.8394522025589344\n",
            "step 1990 - loss 1.4382925033569336 - moving ave loss 1.7993362326387343\n",
            "step 1991 - loss 1.5592821836471558 - moving ave loss 1.7753308277395767\n",
            "step 1992 - loss 1.5497338771820068 - moving ave loss 1.7527711326838198\n",
            "step 1993 - loss 1.4882161617279053 - moving ave loss 1.7263156355882283\n",
            "step 1994 - loss 1.7803411483764648 - moving ave loss 1.7317181868670521\n",
            "step 1995 - loss 1.8157825469970703 - moving ave loss 1.740124622880054\n",
            "step 1996 - loss 1.8602397441864014 - moving ave loss 1.752136135010689\n",
            "step 1997 - loss 1.5965309143066406 - moving ave loss 1.736575612940284\n",
            "step 1998 - loss 1.6091885566711426 - moving ave loss 1.7238369073133701\n",
            "step 1999 - loss 1.3276691436767578 - moving ave loss 1.684220130949709\n",
            "step 2000 - loss 2.3764615058898926 - moving ave loss 1.7534442684437272\n",
            "Checkpoint at step 2000\n",
            "step 2001 - loss 1.8004988431930542 - moving ave loss 1.75814972591866\n",
            "Finish 87 epoch(es)\n",
            "step 2002 - loss 1.71103036403656 - moving ave loss 1.75343778973045\n",
            "step 2003 - loss 1.7365996837615967 - moving ave loss 1.7517539791335648\n",
            "step 2004 - loss 1.6223658323287964 - moving ave loss 1.738815164453088\n",
            "step 2005 - loss 1.98146390914917 - moving ave loss 1.7630800389226964\n",
            "step 2006 - loss 2.0157504081726074 - moving ave loss 1.7883470758476876\n",
            "step 2007 - loss 1.4411351680755615 - moving ave loss 1.7536258850704751\n",
            "step 2008 - loss 2.15704607963562 - moving ave loss 1.7939679045269896\n",
            "step 2009 - loss 2.051748514175415 - moving ave loss 1.8197459654918322\n",
            "step 2010 - loss 1.8310601711273193 - moving ave loss 1.8208773860553809\n",
            "step 2011 - loss 1.701332449913025 - moving ave loss 1.8089228924411453\n",
            "step 2012 - loss 2.063138008117676 - moving ave loss 1.8343444040087984\n",
            "step 2013 - loss 1.5674163103103638 - moving ave loss 1.8076515946389549\n",
            "step 2014 - loss 1.9162750244140625 - moving ave loss 1.8185139376164656\n",
            "step 2015 - loss 1.4138922691345215 - moving ave loss 1.7780517707682713\n",
            "step 2016 - loss 1.8804150819778442 - moving ave loss 1.7882881018892285\n",
            "step 2017 - loss 1.884347677230835 - moving ave loss 1.7978940594233892\n",
            "step 2018 - loss 1.794832468032837 - moving ave loss 1.797587900284334\n",
            "step 2019 - loss 2.254345417022705 - moving ave loss 1.843263651958171\n",
            "step 2020 - loss 1.8019217252731323 - moving ave loss 1.8391294592896672\n",
            "step 2021 - loss 1.2258353233337402 - moving ave loss 1.7778000456940743\n",
            "step 2022 - loss 1.543572187423706 - moving ave loss 1.7543772598670375\n",
            "step 2023 - loss 1.3695189952850342 - moving ave loss 1.7158914334088373\n",
            "step 2024 - loss 1.8113949298858643 - moving ave loss 1.72544178305654\n",
            "Finish 88 epoch(es)\n",
            "step 2025 - loss 1.7395575046539307 - moving ave loss 1.726853355216279\n",
            "step 2026 - loss 1.8286707401275635 - moving ave loss 1.7370350937074077\n",
            "step 2027 - loss 1.8398184776306152 - moving ave loss 1.7473134320997286\n",
            "step 2028 - loss 2.1038155555725098 - moving ave loss 1.7829636444470067\n",
            "step 2029 - loss 1.4624534845352173 - moving ave loss 1.7509126284558278\n",
            "step 2030 - loss 1.7778633832931519 - moving ave loss 1.7536077039395603\n",
            "step 2031 - loss 1.4148263931274414 - moving ave loss 1.7197295728583486\n",
            "step 2032 - loss 1.5565177202224731 - moving ave loss 1.7034083875947612\n",
            "step 2033 - loss 1.658470869064331 - moving ave loss 1.6989146357417182\n",
            "step 2034 - loss 1.7604999542236328 - moving ave loss 1.7050731675899098\n",
            "step 2035 - loss 1.6943895816802979 - moving ave loss 1.7040048089989486\n",
            "step 2036 - loss 2.057081699371338 - moving ave loss 1.7393124980361874\n",
            "step 2037 - loss 1.4637691974639893 - moving ave loss 1.7117581679789675\n",
            "step 2038 - loss 1.9935367107391357 - moving ave loss 1.7399360222549844\n",
            "step 2039 - loss 1.7200353145599365 - moving ave loss 1.7379459514854796\n",
            "step 2040 - loss 2.0508036613464355 - moving ave loss 1.7692317224715752\n",
            "step 2041 - loss 1.8705942630767822 - moving ave loss 1.7793679765320958\n",
            "step 2042 - loss 1.6308894157409668 - moving ave loss 1.7645201204529828\n",
            "step 2043 - loss 2.049222469329834 - moving ave loss 1.792990355340668\n",
            "step 2044 - loss 1.7128649950027466 - moving ave loss 1.7849778193068762\n",
            "step 2045 - loss 2.1132781505584717 - moving ave loss 1.8178078524320358\n",
            "step 2046 - loss 1.5167086124420166 - moving ave loss 1.7876979284330339\n",
            "step 2047 - loss 1.9573943614959717 - moving ave loss 1.8046675717393277\n",
            "Finish 89 epoch(es)\n",
            "step 2048 - loss 1.9423151016235352 - moving ave loss 1.8184323247277485\n",
            "step 2049 - loss 1.7118656635284424 - moving ave loss 1.807775658607818\n",
            "step 2050 - loss 1.638116717338562 - moving ave loss 1.7908097644808925\n",
            "step 2051 - loss 1.459023118019104 - moving ave loss 1.7576310998347138\n",
            "step 2052 - loss 2.115030288696289 - moving ave loss 1.7933710187208713\n",
            "step 2053 - loss 1.8823282718658447 - moving ave loss 1.8022667440353688\n",
            "step 2054 - loss 2.062251567840576 - moving ave loss 1.8282652264158896\n",
            "step 2055 - loss 1.643242359161377 - moving ave loss 1.8097629396904384\n",
            "step 2056 - loss 1.4421013593673706 - moving ave loss 1.7729967816581318\n",
            "step 2057 - loss 2.0716347694396973 - moving ave loss 1.8028605804362883\n",
            "step 2058 - loss 1.7639200687408447 - moving ave loss 1.798966529266744\n",
            "step 2059 - loss 1.9681071043014526 - moving ave loss 1.815880586770215\n",
            "step 2060 - loss 1.7339260578155518 - moving ave loss 1.8076851338747486\n",
            "step 2061 - loss 1.2777118682861328 - moving ave loss 1.7546878073158871\n",
            "step 2062 - loss 1.3221158981323242 - moving ave loss 1.7114306163975308\n",
            "step 2063 - loss 1.451967716217041 - moving ave loss 1.6854843263794819\n",
            "step 2064 - loss 1.5209193229675293 - moving ave loss 1.6690278260382867\n",
            "step 2065 - loss 1.7869503498077393 - moving ave loss 1.6808200784152318\n",
            "step 2066 - loss 1.7873789072036743 - moving ave loss 1.6914759612940762\n",
            "step 2067 - loss 1.434114694595337 - moving ave loss 1.6657398346242023\n",
            "step 2068 - loss 2.161134958267212 - moving ave loss 1.7152793469885033\n",
            "step 2069 - loss 1.6275250911712646 - moving ave loss 1.7065039214067794\n",
            "step 2070 - loss 1.599172592163086 - moving ave loss 1.69577078848241\n",
            "Finish 90 epoch(es)\n",
            "step 2071 - loss 2.122626304626465 - moving ave loss 1.7384563400968156\n",
            "step 2072 - loss 1.4069180488586426 - moving ave loss 1.7053025109729985\n",
            "step 2073 - loss 1.5295639038085938 - moving ave loss 1.6877286502565578\n",
            "step 2074 - loss 1.8843355178833008 - moving ave loss 1.7073893370192321\n",
            "step 2075 - loss 1.6574867963790894 - moving ave loss 1.702399082955218\n",
            "step 2076 - loss 1.7926933765411377 - moving ave loss 1.7114285123138102\n",
            "step 2077 - loss 1.598850131034851 - moving ave loss 1.7001706741859142\n",
            "step 2078 - loss 2.2001571655273438 - moving ave loss 1.750169323320057\n",
            "step 2079 - loss 1.7306448221206665 - moving ave loss 1.748216873200118\n",
            "step 2080 - loss 1.4855618476867676 - moving ave loss 1.721951370648783\n",
            "step 2081 - loss 1.7356665134429932 - moving ave loss 1.723322884928204\n",
            "step 2082 - loss 1.6536039113998413 - moving ave loss 1.7163509875753677\n",
            "step 2083 - loss 1.8860974311828613 - moving ave loss 1.733325631936117\n",
            "step 2084 - loss 1.6310679912567139 - moving ave loss 1.7230998678681768\n",
            "step 2085 - loss 1.885246992111206 - moving ave loss 1.7393145802924799\n",
            "step 2086 - loss 1.7429399490356445 - moving ave loss 1.7396771171667964\n",
            "step 2087 - loss 1.9390971660614014 - moving ave loss 1.759619122056257\n",
            "step 2088 - loss 1.923858642578125 - moving ave loss 1.7760430741084436\n",
            "step 2089 - loss 1.3800413608551025 - moving ave loss 1.7364429027831094\n",
            "step 2090 - loss 1.4770690202713013 - moving ave loss 1.7105055145319288\n",
            "step 2091 - loss 1.7478001117706299 - moving ave loss 1.714234974255799\n",
            "step 2092 - loss 1.83771550655365 - moving ave loss 1.726583027485584\n",
            "step 2093 - loss 2.3812949657440186 - moving ave loss 1.7920542213114274\n",
            "Finish 91 epoch(es)\n",
            "step 2094 - loss 1.8192129135131836 - moving ave loss 1.794770090531603\n",
            "step 2095 - loss 1.8705074787139893 - moving ave loss 1.8023438293498417\n",
            "step 2096 - loss 2.3575592041015625 - moving ave loss 1.8578653668250138\n",
            "step 2097 - loss 1.9280611276626587 - moving ave loss 1.8648849429087784\n",
            "step 2098 - loss 1.697204351425171 - moving ave loss 1.8481168837604178\n",
            "step 2099 - loss 2.0760347843170166 - moving ave loss 1.8709086738160776\n",
            "step 2100 - loss 1.7821438312530518 - moving ave loss 1.8620321895597751\n",
            "step 2101 - loss 2.2271461486816406 - moving ave loss 1.8985435854719617\n",
            "step 2102 - loss 1.411924123764038 - moving ave loss 1.8498816393011694\n",
            "step 2103 - loss 1.5919142961502075 - moving ave loss 1.8240849049860732\n",
            "step 2104 - loss 1.9888087511062622 - moving ave loss 1.8405572895980922\n",
            "step 2105 - loss 1.878740906715393 - moving ave loss 1.8443756513098222\n",
            "step 2106 - loss 1.7650508880615234 - moving ave loss 1.8364431749849923\n",
            "step 2107 - loss 2.038400650024414 - moving ave loss 1.8566389224889346\n",
            "step 2108 - loss 1.7934150695800781 - moving ave loss 1.850316537198049\n",
            "step 2109 - loss 1.4677314758300781 - moving ave loss 1.812058031061252\n",
            "step 2110 - loss 1.8855888843536377 - moving ave loss 1.8194111163904905\n",
            "step 2111 - loss 1.5636513233184814 - moving ave loss 1.7938351370832897\n",
            "step 2112 - loss 1.970535159111023 - moving ave loss 1.811505139286063\n",
            "step 2113 - loss 1.7806977033615112 - moving ave loss 1.808424395693608\n",
            "step 2114 - loss 1.383800745010376 - moving ave loss 1.7659620306252846\n",
            "step 2115 - loss 1.8112823963165283 - moving ave loss 1.7704940671944092\n",
            "step 2116 - loss 1.5874085426330566 - moving ave loss 1.752185514738274\n",
            "Finish 92 epoch(es)\n",
            "step 2117 - loss 1.7871965169906616 - moving ave loss 1.7556866149635129\n",
            "step 2118 - loss 2.0147488117218018 - moving ave loss 1.781592834639342\n",
            "step 2119 - loss 1.570300579071045 - moving ave loss 1.7604636090825123\n",
            "step 2120 - loss 1.4867665767669678 - moving ave loss 1.7330939058509578\n",
            "step 2121 - loss 1.7012112140655518 - moving ave loss 1.7299056366724173\n",
            "step 2122 - loss 1.6107549667358398 - moving ave loss 1.7179905696787596\n",
            "step 2123 - loss 1.3548431396484375 - moving ave loss 1.6816758266757275\n",
            "step 2124 - loss 1.5936013460159302 - moving ave loss 1.6728683786097478\n",
            "step 2125 - loss 1.4587938785552979 - moving ave loss 1.651460928604303\n",
            "Checkpoint at step 2125\n",
            "step 2126 - loss 1.427692174911499 - moving ave loss 1.6290840532350226\n",
            "step 2127 - loss 1.9146676063537598 - moving ave loss 1.6576424085468964\n",
            "step 2128 - loss 1.8932443857192993 - moving ave loss 1.6812026062641368\n",
            "step 2129 - loss 1.1961946487426758 - moving ave loss 1.6327018105119908\n",
            "step 2130 - loss 2.0262386798858643 - moving ave loss 1.6720554974493784\n",
            "step 2131 - loss 1.837002158164978 - moving ave loss 1.6885501635209383\n",
            "step 2132 - loss 1.6905059814453125 - moving ave loss 1.6887457453133758\n",
            "step 2133 - loss 1.608424425125122 - moving ave loss 1.6807136132945506\n",
            "step 2134 - loss 1.7524664402008057 - moving ave loss 1.687888895985176\n",
            "step 2135 - loss 2.008678913116455 - moving ave loss 1.719967897698304\n",
            "step 2136 - loss 1.687786340713501 - moving ave loss 1.7167497419998237\n",
            "step 2137 - loss 1.994626760482788 - moving ave loss 1.7445374438481203\n",
            "step 2138 - loss 1.6746498346328735 - moving ave loss 1.7375486829265956\n",
            "step 2139 - loss 1.9089897871017456 - moving ave loss 1.7546927933441108\n",
            "Finish 93 epoch(es)\n",
            "step 2140 - loss 1.8017290830612183 - moving ave loss 1.7593964223158216\n",
            "step 2141 - loss 1.6340210437774658 - moving ave loss 1.746858884461986\n",
            "step 2142 - loss 1.4685328006744385 - moving ave loss 1.719026276083231\n",
            "step 2143 - loss 1.6252117156982422 - moving ave loss 1.7096448200447323\n",
            "step 2144 - loss 1.398456335067749 - moving ave loss 1.678525971547034\n",
            "step 2145 - loss 1.441723346710205 - moving ave loss 1.654845709063351\n",
            "step 2146 - loss 2.212782859802246 - moving ave loss 1.7106394241372407\n",
            "step 2147 - loss 2.0455150604248047 - moving ave loss 1.744126987765997\n",
            "step 2148 - loss 1.7472844123840332 - moving ave loss 1.7444427302278007\n",
            "step 2149 - loss 1.6547337770462036 - moving ave loss 1.7354718349096412\n",
            "step 2150 - loss 1.8823602199554443 - moving ave loss 1.7501606734142214\n",
            "step 2151 - loss 1.3899004459381104 - moving ave loss 1.7141346506666104\n",
            "step 2152 - loss 1.790468454360962 - moving ave loss 1.7217680310360457\n",
            "step 2153 - loss 1.5730743408203125 - moving ave loss 1.7068986620144724\n",
            "step 2154 - loss 1.6589986085891724 - moving ave loss 1.7021086566719426\n",
            "step 2155 - loss 2.1177425384521484 - moving ave loss 1.7436720448499634\n",
            "step 2156 - loss 1.4268717765808105 - moving ave loss 1.7119920180230483\n",
            "step 2157 - loss 1.5776195526123047 - moving ave loss 1.698554771481974\n",
            "step 2158 - loss 2.20080828666687 - moving ave loss 1.7487801230004638\n",
            "step 2159 - loss 1.5370148420333862 - moving ave loss 1.727603594903756\n",
            "step 2160 - loss 1.5922152996063232 - moving ave loss 1.7140647653740129\n",
            "step 2161 - loss 1.7820837497711182 - moving ave loss 1.7208666638137236\n",
            "step 2162 - loss 1.554584264755249 - moving ave loss 1.7042384239078763\n",
            "Finish 94 epoch(es)\n",
            "step 2163 - loss 1.5275275707244873 - moving ave loss 1.6865673385895374\n",
            "step 2164 - loss 2.1757283210754395 - moving ave loss 1.7354834368381278\n",
            "step 2165 - loss 1.2848833799362183 - moving ave loss 1.690423431147937\n",
            "step 2166 - loss 1.4016982316970825 - moving ave loss 1.6615509112028515\n",
            "step 2167 - loss 1.6087725162506104 - moving ave loss 1.6562730717076275\n",
            "step 2168 - loss 1.8754137754440308 - moving ave loss 1.6781871420812677\n",
            "step 2169 - loss 1.6383352279663086 - moving ave loss 1.6742019506697718\n",
            "step 2170 - loss 2.1206839084625244 - moving ave loss 1.718850146449047\n",
            "step 2171 - loss 1.919786810874939 - moving ave loss 1.7389438128916364\n",
            "step 2172 - loss 1.7423396110534668 - moving ave loss 1.7392833927078195\n",
            "step 2173 - loss 1.4334073066711426 - moving ave loss 1.7086957841041517\n",
            "step 2174 - loss 1.8728892803192139 - moving ave loss 1.725115133725658\n",
            "step 2175 - loss 1.8852550983428955 - moving ave loss 1.7411291301873817\n",
            "step 2176 - loss 1.4257187843322754 - moving ave loss 1.709588095601871\n",
            "step 2177 - loss 1.833634614944458 - moving ave loss 1.7219927475361296\n",
            "step 2178 - loss 1.3947581052780151 - moving ave loss 1.6892692833103182\n",
            "step 2179 - loss 1.9876112937927246 - moving ave loss 1.719103484358559\n",
            "step 2180 - loss 2.0547852516174316 - moving ave loss 1.7526716610844464\n",
            "step 2181 - loss 1.622715950012207 - moving ave loss 1.7396760899772226\n",
            "step 2182 - loss 1.4697608947753906 - moving ave loss 1.7126845704570395\n",
            "step 2183 - loss 1.8054120540618896 - moving ave loss 1.7219573188175246\n",
            "step 2184 - loss 1.872082233428955 - moving ave loss 1.7369698102786677\n",
            "step 2185 - loss 1.8149681091308594 - moving ave loss 1.7447696401638868\n",
            "Finish 95 epoch(es)\n",
            "step 2186 - loss 2.584340810775757 - moving ave loss 1.8287267572250738\n",
            "step 2187 - loss 1.5588217973709106 - moving ave loss 1.8017362612396575\n",
            "step 2188 - loss 1.7764203548431396 - moving ave loss 1.7992046706000058\n",
            "step 2189 - loss 1.816333293914795 - moving ave loss 1.8009175329314848\n",
            "step 2190 - loss 1.5105360746383667 - moving ave loss 1.771879387102173\n",
            "step 2191 - loss 1.7098133563995361 - moving ave loss 1.7656727840319093\n",
            "step 2192 - loss 1.6681833267211914 - moving ave loss 1.7559238383008375\n",
            "step 2193 - loss 1.4758546352386475 - moving ave loss 1.7279169179946186\n",
            "step 2194 - loss 1.9810009002685547 - moving ave loss 1.7532253162220122\n",
            "step 2195 - loss 1.7120285034179688 - moving ave loss 1.7491056349416079\n",
            "step 2196 - loss 1.6333284378051758 - moving ave loss 1.7375279152279646\n",
            "step 2197 - loss 1.8818761110305786 - moving ave loss 1.751962734808226\n",
            "step 2198 - loss 1.9258075952529907 - moving ave loss 1.7693472208527026\n",
            "step 2199 - loss 1.8104811906814575 - moving ave loss 1.7734606178355783\n",
            "step 2200 - loss 1.9105041027069092 - moving ave loss 1.7871649663227114\n",
            "step 2201 - loss 1.8340531587600708 - moving ave loss 1.7918537855664474\n",
            "step 2202 - loss 1.5884788036346436 - moving ave loss 1.771516287373267\n",
            "step 2203 - loss 1.6306496858596802 - moving ave loss 1.7574296272219083\n",
            "step 2204 - loss 1.8173179626464844 - moving ave loss 1.7634184607643661\n",
            "step 2205 - loss 1.636011004447937 - moving ave loss 1.7506777151327233\n",
            "step 2206 - loss 1.4436047077178955 - moving ave loss 1.7199704143912407\n",
            "step 2207 - loss 1.754143476486206 - moving ave loss 1.723387720600737\n",
            "step 2208 - loss 1.4483129978179932 - moving ave loss 1.6958802483224626\n",
            "Finish 96 epoch(es)\n",
            "step 2209 - loss 1.8799539804458618 - moving ave loss 1.7142876215348024\n",
            "step 2210 - loss 2.1620726585388184 - moving ave loss 1.759066125235204\n",
            "step 2211 - loss 1.5641025304794312 - moving ave loss 1.7395697657596267\n",
            "step 2212 - loss 1.4488389492034912 - moving ave loss 1.7104966841040132\n",
            "step 2213 - loss 1.544370174407959 - moving ave loss 1.6938840331344078\n",
            "step 2214 - loss 1.5626373291015625 - moving ave loss 1.6807593627311235\n",
            "step 2215 - loss 1.8615880012512207 - moving ave loss 1.698842226583133\n",
            "step 2216 - loss 1.7726613283157349 - moving ave loss 1.7062241367563933\n",
            "step 2217 - loss 1.9163764715194702 - moving ave loss 1.727239370232701\n",
            "step 2218 - loss 1.614173412322998 - moving ave loss 1.7159327744417308\n",
            "step 2219 - loss 1.6888704299926758 - moving ave loss 1.7132265399968252\n",
            "step 2220 - loss 1.8897464275360107 - moving ave loss 1.7308785287507438\n",
            "step 2221 - loss 1.3376981019973755 - moving ave loss 1.691560486075407\n",
            "step 2222 - loss 2.3205246925354004 - moving ave loss 1.7544569067214062\n",
            "step 2223 - loss 1.7004176378250122 - moving ave loss 1.7490529798317667\n",
            "step 2224 - loss 1.6594732999801636 - moving ave loss 1.7400950118466063\n",
            "step 2225 - loss 1.1917608976364136 - moving ave loss 1.6852616004255871\n",
            "step 2226 - loss 1.535629391670227 - moving ave loss 1.6702983795500512\n",
            "step 2227 - loss 1.7281702756881714 - moving ave loss 1.6760855691638634\n",
            "step 2228 - loss 1.7465144395828247 - moving ave loss 1.6831284562057596\n",
            "step 2229 - loss 1.8575636148452759 - moving ave loss 1.7005719720697112\n",
            "step 2230 - loss 2.178997278213501 - moving ave loss 1.7484145026840903\n",
            "step 2231 - loss 1.6169581413269043 - moving ave loss 1.7352688665483718\n",
            "Finish 97 epoch(es)\n",
            "step 2232 - loss 1.5167521238327026 - moving ave loss 1.713417192276805\n",
            "step 2233 - loss 1.7177722454071045 - moving ave loss 1.713852697589835\n",
            "step 2234 - loss 1.9848098754882812 - moving ave loss 1.7409484153796795\n",
            "step 2235 - loss 2.0091772079467773 - moving ave loss 1.7677712946363893\n",
            "step 2236 - loss 1.6560426950454712 - moving ave loss 1.7565984346772976\n",
            "step 2237 - loss 1.8729462623596191 - moving ave loss 1.76823321744553\n",
            "step 2238 - loss 1.3615713119506836 - moving ave loss 1.7275670268960455\n",
            "step 2239 - loss 1.8530662059783936 - moving ave loss 1.7401169448042804\n",
            "step 2240 - loss 1.3394497632980347 - moving ave loss 1.700050226653656\n",
            "step 2241 - loss 1.6237168312072754 - moving ave loss 1.692416887109018\n",
            "step 2242 - loss 1.6166859865188599 - moving ave loss 1.684843797050002\n",
            "step 2243 - loss 1.378322958946228 - moving ave loss 1.6541917132396247\n",
            "step 2244 - loss 2.001779079437256 - moving ave loss 1.688950449859388\n",
            "step 2245 - loss 1.581886887550354 - moving ave loss 1.6782440936284846\n",
            "step 2246 - loss 1.6423181295394897 - moving ave loss 1.6746514972195852\n",
            "step 2247 - loss 1.5923385620117188 - moving ave loss 1.6664202036987985\n",
            "step 2248 - loss 1.7352025508880615 - moving ave loss 1.673298438417725\n",
            "step 2249 - loss 2.1661534309387207 - moving ave loss 1.7225839376698246\n",
            "step 2250 - loss 1.4767816066741943 - moving ave loss 1.6980037045702616\n",
            "Checkpoint at step 2250\n",
            "step 2251 - loss 2.14643931388855 - moving ave loss 1.7428472655020906\n",
            "step 2252 - loss 1.4470770359039307 - moving ave loss 1.7132702425422746\n",
            "step 2253 - loss 1.5294060707092285 - moving ave loss 1.69488382535897\n",
            "step 2254 - loss 2.0609912872314453 - moving ave loss 1.7314945715462176\n",
            "Finish 98 epoch(es)\n",
            "step 2255 - loss 2.0094950199127197 - moving ave loss 1.7592946163828678\n",
            "step 2256 - loss 1.4265975952148438 - moving ave loss 1.7260249142660655\n",
            "step 2257 - loss 2.077049732208252 - moving ave loss 1.7611273960602842\n",
            "step 2258 - loss 1.6145515441894531 - moving ave loss 1.746469810873201\n",
            "step 2259 - loss 1.6633193492889404 - moving ave loss 1.738154764714775\n",
            "step 2260 - loss 1.222868800163269 - moving ave loss 1.6866261682596244\n",
            "step 2261 - loss 1.5764989852905273 - moving ave loss 1.6756134499627149\n",
            "step 2262 - loss 1.6925380229949951 - moving ave loss 1.677305907265943\n",
            "step 2263 - loss 1.8972201347351074 - moving ave loss 1.6992973300128595\n",
            "step 2264 - loss 2.051206111907959 - moving ave loss 1.7344882082023696\n",
            "step 2265 - loss 1.7359122037887573 - moving ave loss 1.7346306077610083\n",
            "step 2266 - loss 1.8889265060424805 - moving ave loss 1.7500601975891557\n",
            "step 2267 - loss 1.6857352256774902 - moving ave loss 1.743627700397989\n",
            "step 2268 - loss 1.5331217050552368 - moving ave loss 1.7225771008637136\n",
            "step 2269 - loss 1.6014540195465088 - moving ave loss 1.7104647927319931\n",
            "step 2270 - loss 1.773328423500061 - moving ave loss 1.7167511558088\n",
            "step 2271 - loss 1.4021332263946533 - moving ave loss 1.6852893628673853\n",
            "step 2272 - loss 1.1213116645812988 - moving ave loss 1.6288915930387766\n",
            "step 2273 - loss 1.9644310474395752 - moving ave loss 1.6624455384788566\n",
            "step 2274 - loss 1.5740762948989868 - moving ave loss 1.6536086141208697\n",
            "step 2275 - loss 1.3806785345077515 - moving ave loss 1.6263156061595578\n",
            "step 2276 - loss 1.5107808113098145 - moving ave loss 1.6147621266745835\n",
            "step 2277 - loss 1.7436424493789673 - moving ave loss 1.627650158945022\n",
            "Finish 99 epoch(es)\n",
            "step 2278 - loss 1.5645751953125 - moving ave loss 1.62134266258177\n",
            "step 2279 - loss 1.8791509866714478 - moving ave loss 1.6471234949907376\n",
            "step 2280 - loss 1.8281089067459106 - moving ave loss 1.665222036166255\n",
            "step 2281 - loss 1.7583343982696533 - moving ave loss 1.674533272376595\n",
            "step 2282 - loss 1.4390463829040527 - moving ave loss 1.6509845834293408\n",
            "step 2283 - loss 1.9415372610092163 - moving ave loss 1.6800398511873285\n",
            "step 2284 - loss 1.6261696815490723 - moving ave loss 1.674652834223503\n",
            "step 2285 - loss 1.8931183815002441 - moving ave loss 1.6964993889511772\n",
            "step 2286 - loss 1.4742546081542969 - moving ave loss 1.6742749108714894\n",
            "step 2287 - loss 1.826616644859314 - moving ave loss 1.6895090842702718\n",
            "step 2288 - loss 1.343165397644043 - moving ave loss 1.654874715607649\n",
            "step 2289 - loss 2.0227460861206055 - moving ave loss 1.6916618526589449\n",
            "step 2290 - loss 1.7450206279754639 - moving ave loss 1.6969977301905967\n",
            "step 2291 - loss 1.3360546827316284 - moving ave loss 1.6609034254447\n",
            "step 2292 - loss 1.8798129558563232 - moving ave loss 1.6827943784858623\n",
            "step 2293 - loss 1.5950884819030762 - moving ave loss 1.6740237888275837\n",
            "step 2294 - loss 1.5822304487228394 - moving ave loss 1.6648444548171093\n",
            "step 2295 - loss 1.820544719696045 - moving ave loss 1.6804144813050028\n",
            "step 2296 - loss 1.617455244064331 - moving ave loss 1.6741185575809359\n",
            "step 2297 - loss 1.769637107849121 - moving ave loss 1.6836704126077544\n",
            "step 2298 - loss 1.5699080228805542 - moving ave loss 1.6722941736350345\n",
            "step 2299 - loss 1.3282575607299805 - moving ave loss 1.637890512344529\n",
            "step 2300 - loss 2.1567506790161133 - moving ave loss 1.6897765290116875\n",
            "Finish 100 epoch(es)\n",
            "step 2301 - loss 1.9370813369750977 - moving ave loss 1.7145070098080286\n",
            "step 2302 - loss 1.6996376514434814 - moving ave loss 1.713020073971574\n",
            "step 2303 - loss 1.6970088481903076 - moving ave loss 1.7114189513934472\n",
            "step 2304 - loss 1.5426357984542847 - moving ave loss 1.6945406360995312\n",
            "step 2305 - loss 1.3772441148757935 - moving ave loss 1.6628109839771574\n",
            "step 2306 - loss 1.9545979499816895 - moving ave loss 1.6919896805776107\n",
            "step 2307 - loss 1.7134352922439575 - moving ave loss 1.6941342417442455\n",
            "step 2308 - loss 1.6806371212005615 - moving ave loss 1.6927845296898771\n",
            "step 2309 - loss 1.617737054824829 - moving ave loss 1.6852797822033723\n",
            "step 2310 - loss 1.828310251235962 - moving ave loss 1.6995828291066313\n",
            "step 2311 - loss 1.7640777826309204 - moving ave loss 1.70603232445906\n",
            "step 2312 - loss 1.502180576324463 - moving ave loss 1.6856471496456005\n",
            "step 2313 - loss 1.8856568336486816 - moving ave loss 1.7056481180459087\n",
            "step 2314 - loss 1.8960113525390625 - moving ave loss 1.724684441495224\n",
            "step 2315 - loss 1.4280142784118652 - moving ave loss 1.6950174251868881\n",
            "step 2316 - loss 1.6944143772125244 - moving ave loss 1.6949571203894518\n",
            "step 2317 - loss 1.5556731224060059 - moving ave loss 1.6810287205911072\n",
            "step 2318 - loss 1.300069808959961 - moving ave loss 1.6429328294279926\n",
            "step 2319 - loss 2.0648488998413086 - moving ave loss 1.6851244364693243\n",
            "step 2320 - loss 1.6447327136993408 - moving ave loss 1.6810852641923262\n",
            "step 2321 - loss 1.6119779348373413 - moving ave loss 1.6741745312568277\n",
            "step 2322 - loss 1.8391284942626953 - moving ave loss 1.6906699275574146\n",
            "step 2323 - loss 1.2461330890655518 - moving ave loss 1.6462162437082284\n",
            "Finish 101 epoch(es)\n",
            "step 2324 - loss 1.3542523384094238 - moving ave loss 1.617019853178348\n",
            "step 2325 - loss 1.706520676612854 - moving ave loss 1.6259699355217987\n",
            "step 2326 - loss 2.015312910079956 - moving ave loss 1.6649042329776145\n",
            "step 2327 - loss 1.7037405967712402 - moving ave loss 1.668787869356977\n",
            "step 2328 - loss 2.255070686340332 - moving ave loss 1.7274161510553128\n",
            "step 2329 - loss 1.6722962856292725 - moving ave loss 1.721904164512709\n",
            "step 2330 - loss 1.7392970323562622 - moving ave loss 1.7236434512970642\n",
            "step 2331 - loss 1.7680217027664185 - moving ave loss 1.7280812764439997\n",
            "step 2332 - loss 1.9165982007980347 - moving ave loss 1.746932968879403\n",
            "step 2333 - loss 1.767981767654419 - moving ave loss 1.7490378487569047\n",
            "step 2334 - loss 1.5642650127410889 - moving ave loss 1.730560565155323\n",
            "step 2335 - loss 1.2566099166870117 - moving ave loss 1.683165500308492\n",
            "step 2336 - loss 1.6623684167861938 - moving ave loss 1.6810857919562623\n",
            "step 2337 - loss 1.854199767112732 - moving ave loss 1.6983971894719092\n",
            "step 2338 - loss 1.7335004806518555 - moving ave loss 1.7019075185899037\n",
            "step 2339 - loss 1.463006615638733 - moving ave loss 1.6780174282947868\n",
            "step 2340 - loss 1.358108401298523 - moving ave loss 1.6460265255951605\n",
            "step 2341 - loss 1.1607649326324463 - moving ave loss 1.597500366298889\n",
            "step 2342 - loss 1.458808183670044 - moving ave loss 1.5836311480360044\n",
            "step 2343 - loss 1.7075705528259277 - moving ave loss 1.5960250885149967\n",
            "step 2344 - loss 1.6449006795883179 - moving ave loss 1.6009126476223288\n",
            "step 2345 - loss 1.5452179908752441 - moving ave loss 1.5953431819476205\n",
            "step 2346 - loss 1.6947929859161377 - moving ave loss 1.6052881623444724\n",
            "Finish 102 epoch(es)\n",
            "step 2347 - loss 1.8112722635269165 - moving ave loss 1.625886572462717\n",
            "step 2348 - loss 1.8320709466934204 - moving ave loss 1.6465050098857874\n",
            "step 2349 - loss 1.50558602809906 - moving ave loss 1.6324131117071146\n",
            "step 2350 - loss 1.599329948425293 - moving ave loss 1.6291047953789324\n",
            "step 2351 - loss 1.7827692031860352 - moving ave loss 1.6444712361596427\n",
            "step 2352 - loss 1.9589262008666992 - moving ave loss 1.6759167326303486\n",
            "step 2353 - loss 1.9403865337371826 - moving ave loss 1.702363712741032\n",
            "step 2354 - loss 1.4538943767547607 - moving ave loss 1.677516779142405\n",
            "step 2355 - loss 1.5538806915283203 - moving ave loss 1.6651531703809965\n",
            "step 2356 - loss 1.6281914710998535 - moving ave loss 1.6614570004528824\n",
            "step 2357 - loss 1.528141736984253 - moving ave loss 1.6481254741060196\n",
            "step 2358 - loss 1.5074992179870605 - moving ave loss 1.6340628484941238\n",
            "step 2359 - loss 1.7016992568969727 - moving ave loss 1.6408264893344087\n",
            "step 2360 - loss 1.6004852056503296 - moving ave loss 1.636792360966001\n",
            "step 2361 - loss 1.6431934833526611 - moving ave loss 1.637432473204667\n",
            "step 2362 - loss 1.5677831172943115 - moving ave loss 1.6304675376136315\n",
            "step 2363 - loss 1.7304612398147583 - moving ave loss 1.6404669078337442\n",
            "step 2364 - loss 1.740302562713623 - moving ave loss 1.650450473321732\n",
            "step 2365 - loss 1.4487792253494263 - moving ave loss 1.6302833485245014\n",
            "step 2366 - loss 1.475780725479126 - moving ave loss 1.6148330862199638\n",
            "step 2367 - loss 1.819130539894104 - moving ave loss 1.6352628315873778\n",
            "step 2368 - loss 1.7434227466583252 - moving ave loss 1.6460788230944725\n",
            "step 2369 - loss 2.101987361907959 - moving ave loss 1.6916696769758213\n",
            "Finish 103 epoch(es)\n",
            "step 2370 - loss 1.5885274410247803 - moving ave loss 1.6813554533807173\n",
            "step 2371 - loss 1.977102279663086 - moving ave loss 1.7109301360089542\n",
            "step 2372 - loss 1.5653356313705444 - moving ave loss 1.696370685545113\n",
            "step 2373 - loss 1.717384934425354 - moving ave loss 1.6984721104331373\n",
            "step 2374 - loss 1.6284966468811035 - moving ave loss 1.6914745640779338\n",
            "step 2375 - loss 1.5255855321884155 - moving ave loss 1.6748856608889822\n",
            "Checkpoint at step 2375\n",
            "step 2376 - loss 1.7933590412139893 - moving ave loss 1.6867329989214828\n",
            "step 2377 - loss 1.814380168914795 - moving ave loss 1.699497715920814\n",
            "step 2378 - loss 1.6975308656692505 - moving ave loss 1.6993010308956575\n",
            "step 2379 - loss 1.8874592781066895 - moving ave loss 1.7181168556167608\n",
            "step 2380 - loss 1.557313084602356 - moving ave loss 1.7020364785153204\n",
            "step 2381 - loss 1.563726782798767 - moving ave loss 1.688205508943665\n",
            "step 2382 - loss 1.4456017017364502 - moving ave loss 1.6639451282229434\n",
            "step 2383 - loss 1.6879651546478271 - moving ave loss 1.6663471308654318\n",
            "step 2384 - loss 2.2085494995117188 - moving ave loss 1.7205673677300606\n",
            "step 2385 - loss 1.2046785354614258 - moving ave loss 1.6689784845031972\n",
            "step 2386 - loss 1.6879692077636719 - moving ave loss 1.6708775568292449\n",
            "step 2387 - loss 1.434793472290039 - moving ave loss 1.6472691483753243\n",
            "step 2388 - loss 1.9020187854766846 - moving ave loss 1.6727441120854605\n",
            "step 2389 - loss 1.5386196374893188 - moving ave loss 1.6593316646258462\n",
            "step 2390 - loss 1.6593875885009766 - moving ave loss 1.6593372570133593\n",
            "step 2391 - loss 1.5925869941711426 - moving ave loss 1.6526622307291379\n",
            "step 2392 - loss 1.7372589111328125 - moving ave loss 1.6611218987695053\n",
            "Finish 104 epoch(es)\n",
            "step 2393 - loss 2.060258150100708 - moving ave loss 1.7010355239026258\n",
            "step 2394 - loss 1.6399869918823242 - moving ave loss 1.6949306707005958\n",
            "step 2395 - loss 1.6927950382232666 - moving ave loss 1.6947171074528629\n",
            "step 2396 - loss 1.4686306715011597 - moving ave loss 1.6721084638576924\n",
            "step 2397 - loss 1.415614128112793 - moving ave loss 1.6464590302832025\n",
            "step 2398 - loss 1.5747102499008179 - moving ave loss 1.6392841522449642\n",
            "step 2399 - loss 1.8177244663238525 - moving ave loss 1.657128183652853\n",
            "step 2400 - loss 1.4943313598632812 - moving ave loss 1.640848501273896\n",
            "step 2401 - loss 1.9615440368652344 - moving ave loss 1.67291805483303\n",
            "step 2402 - loss 1.6895842552185059 - moving ave loss 1.6745846748715776\n",
            "step 2403 - loss 1.3764281272888184 - moving ave loss 1.6447690201133016\n",
            "step 2404 - loss 1.8473358154296875 - moving ave loss 1.6650256996449402\n",
            "step 2405 - loss 1.8626856803894043 - moving ave loss 1.6847916977193866\n",
            "step 2406 - loss 1.2800319194793701 - moving ave loss 1.644315719895385\n",
            "step 2407 - loss 1.7802860736846924 - moving ave loss 1.657912755274316\n",
            "step 2408 - loss 1.648357629776001 - moving ave loss 1.6569572427244845\n",
            "step 2409 - loss 1.6964333057403564 - moving ave loss 1.6609048490260718\n",
            "step 2410 - loss 1.8457974195480347 - moving ave loss 1.679394106078268\n",
            "step 2411 - loss 1.6230008602142334 - moving ave loss 1.6737547814918647\n",
            "step 2412 - loss 1.4122403860092163 - moving ave loss 1.6476033419435998\n",
            "step 2413 - loss 1.6536332368850708 - moving ave loss 1.6482063314377469\n",
            "step 2414 - loss 1.5823264122009277 - moving ave loss 1.6416183395140649\n",
            "step 2415 - loss 1.9599360227584839 - moving ave loss 1.673450107838507\n",
            "Finish 105 epoch(es)\n",
            "step 2416 - loss 1.7724910974502563 - moving ave loss 1.683354206799682\n",
            "step 2417 - loss 1.6134727001190186 - moving ave loss 1.6763660561316158\n",
            "step 2418 - loss 2.0189380645751953 - moving ave loss 1.7106232569759738\n",
            "step 2419 - loss 1.48524010181427 - moving ave loss 1.6880849414598034\n",
            "step 2420 - loss 1.851049780845642 - moving ave loss 1.7043814253983873\n",
            "step 2421 - loss 1.4783955812454224 - moving ave loss 1.6817828409830908\n",
            "step 2422 - loss 1.7631442546844482 - moving ave loss 1.6899189823532266\n",
            "step 2423 - loss 1.4020659923553467 - moving ave loss 1.6611336833534385\n",
            "step 2424 - loss 1.3581252098083496 - moving ave loss 1.6308328359989297\n",
            "step 2425 - loss 1.6996574401855469 - moving ave loss 1.6377152964175914\n",
            "step 2426 - loss 1.1803855895996094 - moving ave loss 1.5919823257357932\n",
            "step 2427 - loss 1.7355705499649048 - moving ave loss 1.6063411481587044\n",
            "step 2428 - loss 1.422750473022461 - moving ave loss 1.5879820806450802\n",
            "step 2429 - loss 1.4037256240844727 - moving ave loss 1.5695564349890194\n",
            "step 2430 - loss 1.7354356050491333 - moving ave loss 1.586144351995031\n",
            "step 2431 - loss 1.57493257522583 - moving ave loss 1.5850231743181107\n",
            "step 2432 - loss 1.4449431896209717 - moving ave loss 1.5710151758483968\n",
            "step 2433 - loss 1.910770297050476 - moving ave loss 1.604990687968605\n",
            "step 2434 - loss 1.4602885246276855 - moving ave loss 1.5905204716345132\n",
            "step 2435 - loss 2.2825515270233154 - moving ave loss 1.6597235771733934\n",
            "step 2436 - loss 1.6578327417373657 - moving ave loss 1.6595344936297907\n",
            "step 2437 - loss 1.589045524597168 - moving ave loss 1.6524855967265284\n",
            "step 2438 - loss 1.8566923141479492 - moving ave loss 1.6729062684686706\n",
            "Finish 106 epoch(es)\n",
            "step 2439 - loss 1.851383924484253 - moving ave loss 1.690754034070229\n",
            "step 2440 - loss 1.5447564125061035 - moving ave loss 1.6761542719138165\n",
            "step 2441 - loss 2.138868808746338 - moving ave loss 1.7224257255970685\n",
            "step 2442 - loss 1.362290859222412 - moving ave loss 1.686412238959603\n",
            "step 2443 - loss 1.5912367105484009 - moving ave loss 1.6768946861184828\n",
            "step 2444 - loss 1.5777512788772583 - moving ave loss 1.6669803453943604\n",
            "step 2445 - loss 1.4452415704727173 - moving ave loss 1.644806467902196\n",
            "step 2446 - loss 1.0209717750549316 - moving ave loss 1.5824229986174696\n",
            "step 2447 - loss 1.7992292642593384 - moving ave loss 1.6041036251816565\n",
            "step 2448 - loss 2.0022590160369873 - moving ave loss 1.6439191642671895\n",
            "step 2449 - loss 1.5671749114990234 - moving ave loss 1.636244738990373\n",
            "step 2450 - loss 1.530381202697754 - moving ave loss 1.625658385361111\n",
            "step 2451 - loss 1.78216552734375 - moving ave loss 1.641309099559375\n",
            "step 2452 - loss 2.156276226043701 - moving ave loss 1.6928058122078078\n",
            "step 2453 - loss 1.646984338760376 - moving ave loss 1.6882236648630646\n",
            "step 2454 - loss 1.1611090898513794 - moving ave loss 1.6355122073618962\n",
            "step 2455 - loss 2.0791025161743164 - moving ave loss 1.6798712382431382\n",
            "step 2456 - loss 1.6534533500671387 - moving ave loss 1.6772294494255382\n",
            "step 2457 - loss 1.3952906131744385 - moving ave loss 1.6490355658004283\n",
            "step 2458 - loss 2.0008578300476074 - moving ave loss 1.6842177922251462\n",
            "step 2459 - loss 1.33674955368042 - moving ave loss 1.6494709683706736\n",
            "step 2460 - loss 1.617892861366272 - moving ave loss 1.6463131576702335\n",
            "step 2461 - loss 1.5626028776168823 - moving ave loss 1.6379421296648984\n",
            "Finish 107 epoch(es)\n",
            "step 2462 - loss 1.5991170406341553 - moving ave loss 1.634059620761824\n",
            "step 2463 - loss 1.5881469249725342 - moving ave loss 1.6294683511828953\n",
            "step 2464 - loss 1.981935977935791 - moving ave loss 1.664715113858185\n",
            "step 2465 - loss 1.4960707426071167 - moving ave loss 1.6478506767330783\n",
            "step 2466 - loss 1.6480841636657715 - moving ave loss 1.6478740254263475\n",
            "step 2467 - loss 1.5175013542175293 - moving ave loss 1.6348367583054657\n",
            "step 2468 - loss 1.5047602653503418 - moving ave loss 1.6218291090099535\n",
            "step 2469 - loss 1.7474554777145386 - moving ave loss 1.634391745880412\n",
            "step 2470 - loss 1.4704747200012207 - moving ave loss 1.6180000432924928\n",
            "step 2471 - loss 1.7958927154541016 - moving ave loss 1.6357893105086536\n",
            "step 2472 - loss 1.6203569173812866 - moving ave loss 1.634246071195917\n",
            "step 2473 - loss 1.5141398906707764 - moving ave loss 1.622235453143403\n",
            "step 2474 - loss 1.5847183465957642 - moving ave loss 1.618483742488639\n",
            "step 2475 - loss 1.295249342918396 - moving ave loss 1.5861603025316147\n",
            "step 2476 - loss 1.5960683822631836 - moving ave loss 1.5871511105047715\n",
            "step 2477 - loss 1.4326914548873901 - moving ave loss 1.5717051449430333\n",
            "step 2478 - loss 2.0501317977905273 - moving ave loss 1.619547810227783\n",
            "step 2479 - loss 1.8203178644180298 - moving ave loss 1.6396248156468078\n",
            "step 2480 - loss 1.3441684246063232 - moving ave loss 1.6100791765427593\n",
            "step 2481 - loss 1.912551999092102 - moving ave loss 1.6403264587976938\n",
            "step 2482 - loss 1.368119239807129 - moving ave loss 1.6131057368986372\n",
            "step 2483 - loss 1.792066216468811 - moving ave loss 1.6310017848556546\n",
            "step 2484 - loss 1.489607334136963 - moving ave loss 1.6168623397837854\n",
            "Finish 108 epoch(es)\n",
            "step 2485 - loss 1.3667266368865967 - moving ave loss 1.5918487694940666\n",
            "step 2486 - loss 1.9461835622787476 - moving ave loss 1.6272822487725347\n",
            "step 2487 - loss 1.5850343704223633 - moving ave loss 1.6230574609375177\n",
            "step 2488 - loss 1.7936112880706787 - moving ave loss 1.6401128436508339\n",
            "step 2489 - loss 1.5943068265914917 - moving ave loss 1.6355322419448997\n",
            "step 2490 - loss 1.7224034070968628 - moving ave loss 1.644219358460096\n",
            "step 2491 - loss 1.3948479890823364 - moving ave loss 1.6192822215223202\n",
            "step 2492 - loss 1.9626386165618896 - moving ave loss 1.653617861026277\n",
            "step 2493 - loss 1.5296958684921265 - moving ave loss 1.641225661772862\n",
            "step 2494 - loss 1.596900224685669 - moving ave loss 1.6367931180641426\n",
            "step 2495 - loss 1.6035921573638916 - moving ave loss 1.6334730219941176\n",
            "step 2496 - loss 1.4936962127685547 - moving ave loss 1.6194953410715613\n",
            "step 2497 - loss 1.9112685918807983 - moving ave loss 1.648672666152485\n",
            "step 2498 - loss 1.7935844659805298 - moving ave loss 1.6631638461352898\n",
            "step 2499 - loss 1.8231496810913086 - moving ave loss 1.6791624296308918\n",
            "step 2500 - loss 1.7354698181152344 - moving ave loss 1.684793168479326\n",
            "Checkpoint at step 2500\n",
            "step 2501 - loss 2.1649138927459717 - moving ave loss 1.7328052409059906\n",
            "step 2502 - loss 1.3791840076446533 - moving ave loss 1.697443117579857\n",
            "step 2503 - loss 1.704527497291565 - moving ave loss 1.6981515555510278\n",
            "step 2504 - loss 1.6677569150924683 - moving ave loss 1.6951120915051718\n",
            "step 2505 - loss 1.4148534536361694 - moving ave loss 1.6670862277182716\n",
            "step 2506 - loss 1.8805794715881348 - moving ave loss 1.688435552105258\n",
            "step 2507 - loss 1.6608576774597168 - moving ave loss 1.685677764640704\n",
            "Finish 109 epoch(es)\n",
            "step 2508 - loss 1.4041416645050049 - moving ave loss 1.6575241546271342\n",
            "step 2509 - loss 1.5631911754608154 - moving ave loss 1.6480908567105024\n",
            "step 2510 - loss 1.3681893348693848 - moving ave loss 1.6201007045263907\n",
            "step 2511 - loss 1.324995994567871 - moving ave loss 1.5905902335305386\n",
            "step 2512 - loss 1.6455166339874268 - moving ave loss 1.5960828735762276\n",
            "step 2513 - loss 1.8953039646148682 - moving ave loss 1.6260049826800915\n",
            "step 2514 - loss 1.5821387767791748 - moving ave loss 1.62161836209\n",
            "step 2515 - loss 1.361810326576233 - moving ave loss 1.5956375585386233\n",
            "step 2516 - loss 1.3926585912704468 - moving ave loss 1.5753396618118056\n",
            "step 2517 - loss 1.5224230289459229 - moving ave loss 1.5700479985252174\n",
            "step 2518 - loss 1.6306450366973877 - moving ave loss 1.5761077023424344\n",
            "step 2519 - loss 1.6042835712432861 - moving ave loss 1.5789252892325196\n",
            "step 2520 - loss 1.4771174192428589 - moving ave loss 1.5687445022335536\n",
            "step 2521 - loss 2.045328378677368 - moving ave loss 1.6164028898779352\n",
            "step 2522 - loss 1.7169820070266724 - moving ave loss 1.626460801592809\n",
            "step 2523 - loss 1.4085075855255127 - moving ave loss 1.6046654799860796\n",
            "step 2524 - loss 1.849732756614685 - moving ave loss 1.6291722076489403\n",
            "step 2525 - loss 1.8099865913391113 - moving ave loss 1.6472536460179577\n",
            "step 2526 - loss 1.2402632236480713 - moving ave loss 1.606554603780969\n",
            "step 2527 - loss 1.8571873903274536 - moving ave loss 1.6316178824356173\n",
            "step 2528 - loss 1.8024823665618896 - moving ave loss 1.6487043308482447\n",
            "step 2529 - loss 1.3876242637634277 - moving ave loss 1.622596324139763\n",
            "step 2530 - loss 1.3159960508346558 - moving ave loss 1.5919362968092523\n",
            "Finish 110 epoch(es)\n",
            "step 2531 - loss 1.4086647033691406 - moving ave loss 1.5736091374652412\n",
            "step 2532 - loss 1.4424307346343994 - moving ave loss 1.560491297182157\n",
            "step 2533 - loss 1.3961076736450195 - moving ave loss 1.5440529348284433\n",
            "step 2534 - loss 1.6455802917480469 - moving ave loss 1.5542056705204037\n",
            "step 2535 - loss 1.7562077045440674 - moving ave loss 1.5744058739227702\n",
            "step 2536 - loss 1.3399455547332764 - moving ave loss 1.5509598420038209\n",
            "step 2537 - loss 1.5582140684127808 - moving ave loss 1.5516852646447168\n",
            "step 2538 - loss 1.5732812881469727 - moving ave loss 1.5538448669949425\n",
            "step 2539 - loss 1.3891541957855225 - moving ave loss 1.5373757998740005\n",
            "step 2540 - loss 2.2126996517181396 - moving ave loss 1.6049081850584144\n",
            "step 2541 - loss 1.8715894222259521 - moving ave loss 1.6315763087751682\n",
            "step 2542 - loss 1.7698646783828735 - moving ave loss 1.6454051457359387\n",
            "step 2543 - loss 1.6986470222473145 - moving ave loss 1.6507293333870763\n",
            "step 2544 - loss 1.9000329971313477 - moving ave loss 1.6756596997615034\n",
            "step 2545 - loss 1.9342492818832397 - moving ave loss 1.7015186579736772\n",
            "step 2546 - loss 1.643506646156311 - moving ave loss 1.6957174567919406\n",
            "step 2547 - loss 1.6051735877990723 - moving ave loss 1.6866630698926537\n",
            "step 2548 - loss 1.328171968460083 - moving ave loss 1.6508139597493967\n",
            "step 2549 - loss 1.6493123769760132 - moving ave loss 1.6506638014720585\n",
            "step 2550 - loss 1.5095309019088745 - moving ave loss 1.6365505115157402\n",
            "step 2551 - loss 2.0575785636901855 - moving ave loss 1.6786533167331847\n",
            "step 2552 - loss 1.2031972408294678 - moving ave loss 1.631107709142813\n",
            "step 2553 - loss 1.8446815013885498 - moving ave loss 1.6524650883673868\n",
            "Finish 111 epoch(es)\n",
            "step 2554 - loss 1.7439240217208862 - moving ave loss 1.6616109817027367\n",
            "step 2555 - loss 1.8926081657409668 - moving ave loss 1.6847107001065598\n",
            "step 2556 - loss 1.72226881980896 - moving ave loss 1.6884665120767999\n",
            "step 2557 - loss 1.566938042640686 - moving ave loss 1.6763136651331885\n",
            "step 2558 - loss 1.6760437488555908 - moving ave loss 1.6762866735054287\n",
            "step 2559 - loss 1.4609068632125854 - moving ave loss 1.6547486924761443\n",
            "step 2560 - loss 2.0099990367889404 - moving ave loss 1.690273726907424\n",
            "step 2561 - loss 1.5175158977508545 - moving ave loss 1.672997943991767\n",
            "step 2562 - loss 1.5310378074645996 - moving ave loss 1.6588019303390502\n",
            "step 2563 - loss 1.4244064092636108 - moving ave loss 1.6353623782315063\n",
            "step 2564 - loss 1.1544950008392334 - moving ave loss 1.587275640492279\n",
            "step 2565 - loss 1.7086797952651978 - moving ave loss 1.599416055969571\n",
            "step 2566 - loss 1.7154881954193115 - moving ave loss 1.611023269914545\n",
            "step 2567 - loss 1.7927908897399902 - moving ave loss 1.6292000318970896\n",
            "step 2568 - loss 1.4276831150054932 - moving ave loss 1.60904834020793\n",
            "step 2569 - loss 1.6519033908843994 - moving ave loss 1.613333845275577\n",
            "step 2570 - loss 2.0835447311401367 - moving ave loss 1.660354933862033\n",
            "step 2571 - loss 1.6769286394119263 - moving ave loss 1.6620123044170225\n",
            "step 2572 - loss 1.9406238794326782 - moving ave loss 1.6898734619185882\n",
            "step 2573 - loss 2.0586841106414795 - moving ave loss 1.7267545267908775\n",
            "step 2574 - loss 1.4573204517364502 - moving ave loss 1.6998111192854348\n",
            "step 2575 - loss 1.454526424407959 - moving ave loss 1.6752826497976872\n",
            "step 2576 - loss 1.627570390701294 - moving ave loss 1.6705114238880479\n",
            "Finish 112 epoch(es)\n",
            "step 2577 - loss 1.6729618310928345 - moving ave loss 1.6707564646085264\n",
            "step 2578 - loss 1.5341224670410156 - moving ave loss 1.6570930648517752\n",
            "step 2579 - loss 1.7417621612548828 - moving ave loss 1.665559974492086\n",
            "step 2580 - loss 1.396597146987915 - moving ave loss 1.6386636917416688\n",
            "step 2581 - loss 1.289931297302246 - moving ave loss 1.6037904522977267\n",
            "step 2582 - loss 1.6731101274490356 - moving ave loss 1.6107224198128576\n",
            "step 2583 - loss 2.169466018676758 - moving ave loss 1.6665967796992476\n",
            "step 2584 - loss 1.667041540145874 - moving ave loss 1.6666412557439103\n",
            "step 2585 - loss 1.5541903972625732 - moving ave loss 1.6553961698957766\n",
            "step 2586 - loss 1.6481778621673584 - moving ave loss 1.654674339122935\n",
            "step 2587 - loss 1.7937519550323486 - moving ave loss 1.6685821007138766\n",
            "step 2588 - loss 1.4068737030029297 - moving ave loss 1.642411260942782\n",
            "step 2589 - loss 1.8339866399765015 - moving ave loss 1.6615687988461538\n",
            "step 2590 - loss 1.5481042861938477 - moving ave loss 1.6502223475809232\n",
            "step 2591 - loss 1.6089074611663818 - moving ave loss 1.6460908589394692\n",
            "step 2592 - loss 2.2314743995666504 - moving ave loss 1.7046292130021874\n",
            "step 2593 - loss 1.4445502758026123 - moving ave loss 1.67862131928223\n",
            "step 2594 - loss 1.3661999702453613 - moving ave loss 1.647379184378543\n",
            "step 2595 - loss 1.7606289386749268 - moving ave loss 1.6587041598081813\n",
            "step 2596 - loss 1.3921419382095337 - moving ave loss 1.6320479376483166\n",
            "step 2597 - loss 1.8757555484771729 - moving ave loss 1.6564186987312024\n",
            "step 2598 - loss 1.6141588687896729 - moving ave loss 1.6521927157370495\n",
            "step 2599 - loss 1.6367708444595337 - moving ave loss 1.650650528609298\n",
            "Finish 113 epoch(es)\n",
            "step 2600 - loss 1.7960337400436401 - moving ave loss 1.6651888497527323\n",
            "step 2601 - loss 1.7002320289611816 - moving ave loss 1.6686931676735772\n",
            "step 2602 - loss 1.5474393367767334 - moving ave loss 1.6565677845838929\n",
            "step 2603 - loss 1.3778924942016602 - moving ave loss 1.6287002555456696\n",
            "step 2604 - loss 1.9231173992156982 - moving ave loss 1.6581419699126725\n",
            "step 2605 - loss 1.7470442056655884 - moving ave loss 1.6670321934879642\n",
            "step 2606 - loss 1.7004203796386719 - moving ave loss 1.670371012103035\n",
            "step 2607 - loss 1.3495101928710938 - moving ave loss 1.6382849301798408\n",
            "step 2608 - loss 1.6526963710784912 - moving ave loss 1.639726074269706\n",
            "step 2609 - loss 1.597433090209961 - moving ave loss 1.6354967758637315\n",
            "step 2610 - loss 1.6582717895507812 - moving ave loss 1.6377742772324364\n",
            "step 2611 - loss 1.9959161281585693 - moving ave loss 1.6735884623250499\n",
            "step 2612 - loss 1.585997462272644 - moving ave loss 1.6648293623198094\n",
            "step 2613 - loss 1.2193199396133423 - moving ave loss 1.6202784200491627\n",
            "step 2614 - loss 1.7155492305755615 - moving ave loss 1.6298055011018027\n",
            "step 2615 - loss 1.9793932437896729 - moving ave loss 1.6647642753705898\n",
            "step 2616 - loss 1.7815319299697876 - moving ave loss 1.6764410408305097\n",
            "step 2617 - loss 1.379520058631897 - moving ave loss 1.6467489426106485\n",
            "step 2618 - loss 1.6853086948394775 - moving ave loss 1.6506049178335316\n",
            "step 2619 - loss 1.5455366373062134 - moving ave loss 1.6400980897807997\n",
            "step 2620 - loss 2.145188331604004 - moving ave loss 1.6906071139631202\n",
            "step 2621 - loss 1.7547104358673096 - moving ave loss 1.6970174461535392\n",
            "step 2622 - loss 1.6478122472763062 - moving ave loss 1.6920969262658159\n",
            "Finish 114 epoch(es)\n",
            "step 2623 - loss 1.5537575483322144 - moving ave loss 1.6782629884724558\n",
            "step 2624 - loss 1.835278034210205 - moving ave loss 1.6939644930462308\n",
            "step 2625 - loss 1.8655314445495605 - moving ave loss 1.711121188196564\n",
            "Checkpoint at step 2625\n",
            "W0614 01:22:40.862443 140264209213312 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:960: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to delete files with this prefix.\n",
            "step 2626 - loss 1.4614903926849365 - moving ave loss 1.6861581086454014\n",
            "step 2627 - loss 1.5399670600891113 - moving ave loss 1.6715390037897724\n",
            "step 2628 - loss 1.5651471614837646 - moving ave loss 1.6608998195591715\n",
            "step 2629 - loss 1.089455246925354 - moving ave loss 1.6037553622957899\n",
            "step 2630 - loss 1.6014130115509033 - moving ave loss 1.6035211272213012\n",
            "step 2631 - loss 1.6071908473968506 - moving ave loss 1.6038880992388562\n",
            "step 2632 - loss 1.6292028427124023 - moving ave loss 1.606419573586211\n",
            "step 2633 - loss 1.424243688583374 - moving ave loss 1.5882019850859272\n",
            "step 2634 - loss 1.951368808746338 - moving ave loss 1.6245186674519685\n",
            "step 2635 - loss 1.646668791770935 - moving ave loss 1.6267336798838652\n",
            "step 2636 - loss 1.493696928024292 - moving ave loss 1.613430004697908\n",
            "step 2637 - loss 1.6376831531524658 - moving ave loss 1.615855319543364\n",
            "step 2638 - loss 1.5724108219146729 - moving ave loss 1.611510869780495\n",
            "step 2639 - loss 1.8664060831069946 - moving ave loss 1.637000391113145\n",
            "step 2640 - loss 1.7808301448822021 - moving ave loss 1.6513833664900508\n",
            "step 2641 - loss 1.8275623321533203 - moving ave loss 1.6690012630563777\n",
            "step 2642 - loss 1.8094803094863892 - moving ave loss 1.6830491676993788\n",
            "step 2643 - loss 1.5912506580352783 - moving ave loss 1.673869316732969\n",
            "step 2644 - loss 1.418287754058838 - moving ave loss 1.648311160465556\n",
            "step 2645 - loss 1.417762041091919 - moving ave loss 1.6252562485281923\n",
            "Finish 115 epoch(es)\n",
            "step 2646 - loss 1.9126510620117188 - moving ave loss 1.6539957298765449\n",
            "step 2647 - loss 1.5584796667099 - moving ave loss 1.6444441235598803\n",
            "step 2648 - loss 1.530684471130371 - moving ave loss 1.6330681583169295\n",
            "step 2649 - loss 1.1842058897018433 - moving ave loss 1.588181931455421\n",
            "step 2650 - loss 1.5146714448928833 - moving ave loss 1.5808308827991673\n",
            "step 2651 - loss 1.5228965282440186 - moving ave loss 1.5750374473436524\n",
            "step 2652 - loss 1.2309715747833252 - moving ave loss 1.5406308600876197\n",
            "step 2653 - loss 1.7529420852661133 - moving ave loss 1.5618619826054692\n",
            "step 2654 - loss 1.5108022689819336 - moving ave loss 1.5567560112431158\n",
            "step 2655 - loss 1.5785918235778809 - moving ave loss 1.5589395924765925\n",
            "step 2656 - loss 1.4502480030059814 - moving ave loss 1.5480704335295314\n",
            "step 2657 - loss 1.4397480487823486 - moving ave loss 1.537238195054813\n",
            "step 2658 - loss 1.4445998668670654 - moving ave loss 1.5279743622360382\n",
            "step 2659 - loss 1.792609453201294 - moving ave loss 1.554437871332564\n",
            "step 2660 - loss 1.7981863021850586 - moving ave loss 1.5788127144178135\n",
            "step 2661 - loss 1.702772617340088 - moving ave loss 1.591208704710041\n",
            "step 2662 - loss 2.0778133869171143 - moving ave loss 1.6398691729307484\n",
            "step 2663 - loss 2.0281615257263184 - moving ave loss 1.6786984082103054\n",
            "step 2664 - loss 1.6665741205215454 - moving ave loss 1.6774859794414296\n",
            "step 2665 - loss 1.4386303424835205 - moving ave loss 1.6536004157456388\n",
            "step 2666 - loss 1.8038944005966187 - moving ave loss 1.668629814230737\n",
            "step 2667 - loss 1.4985911846160889 - moving ave loss 1.6516259512692721\n",
            "step 2668 - loss 1.4315485954284668 - moving ave loss 1.6296182156851917\n",
            "Finish 116 epoch(es)\n",
            "step 2669 - loss 1.8405040502548218 - moving ave loss 1.6507067991421547\n",
            "step 2670 - loss 1.3994908332824707 - moving ave loss 1.6255852025561863\n",
            "step 2671 - loss 1.4968373775482178 - moving ave loss 1.6127104200553894\n",
            "step 2672 - loss 2.2306623458862305 - moving ave loss 1.6745056126384736\n",
            "step 2673 - loss 1.4487268924713135 - moving ave loss 1.6519277406217576\n",
            "step 2674 - loss 1.681287169456482 - moving ave loss 1.6548636835052302\n",
            "step 2675 - loss 1.8921878337860107 - moving ave loss 1.6785960985333084\n",
            "step 2676 - loss 1.4892925024032593 - moving ave loss 1.6596657389203036\n",
            "step 2677 - loss 1.6526660919189453 - moving ave loss 1.6589657742201678\n",
            "step 2678 - loss 1.494415283203125 - moving ave loss 1.6425107251184636\n",
            "step 2679 - loss 1.3197969198226929 - moving ave loss 1.6102393445888867\n",
            "step 2680 - loss 1.67472243309021 - moving ave loss 1.616687653439019\n",
            "step 2681 - loss 1.6900041103363037 - moving ave loss 1.6240192991287477\n",
            "step 2682 - loss 1.7142260074615479 - moving ave loss 1.6330399699620277\n",
            "step 2683 - loss 1.5703201293945312 - moving ave loss 1.6267679859052782\n",
            "step 2684 - loss 1.563633680343628 - moving ave loss 1.6204545553491132\n",
            "step 2685 - loss 2.180269241333008 - moving ave loss 1.6764360239475027\n",
            "step 2686 - loss 1.3805572986602783 - moving ave loss 1.6468481514187803\n",
            "step 2687 - loss 1.6550298929214478 - moving ave loss 1.647666325569047\n",
            "step 2688 - loss 1.8621902465820312 - moving ave loss 1.6691187176703455\n",
            "step 2689 - loss 1.5932793617248535 - moving ave loss 1.6615347820757962\n",
            "step 2690 - loss 1.7390685081481934 - moving ave loss 1.669288154683036\n",
            "step 2691 - loss 1.41294264793396 - moving ave loss 1.6436536040081284\n",
            "Finish 117 epoch(es)\n",
            "step 2692 - loss 1.47926926612854 - moving ave loss 1.6272151702201696\n",
            "step 2693 - loss 1.5127978324890137 - moving ave loss 1.6157734364470542\n",
            "step 2694 - loss 1.7516374588012695 - moving ave loss 1.6293598386824757\n",
            "step 2695 - loss 1.6338787078857422 - moving ave loss 1.6298117256028024\n",
            "step 2696 - loss 1.7516791820526123 - moving ave loss 1.6419984712477835\n",
            "step 2697 - loss 1.3971585035324097 - moving ave loss 1.6175144744762462\n",
            "step 2698 - loss 1.2726147174835205 - moving ave loss 1.5830244987769737\n",
            "step 2699 - loss 1.8542962074279785 - moving ave loss 1.6101516696420743\n",
            "step 2700 - loss 1.5953152179718018 - moving ave loss 1.608668024475047\n",
            "step 2701 - loss 1.646968126296997 - moving ave loss 1.6124980346572422\n",
            "step 2702 - loss 1.6785281896591187 - moving ave loss 1.6191010501574299\n",
            "step 2703 - loss 1.2966339588165283 - moving ave loss 1.5868543410233398\n",
            "step 2704 - loss 1.4087793827056885 - moving ave loss 1.5690468451915747\n",
            "step 2705 - loss 1.5188835859298706 - moving ave loss 1.5640305192654043\n",
            "step 2706 - loss 1.5042829513549805 - moving ave loss 1.5580557624743618\n",
            "step 2707 - loss 2.0543744564056396 - moving ave loss 1.6076876318674898\n",
            "step 2708 - loss 1.6591500043869019 - moving ave loss 1.612833869119431\n",
            "step 2709 - loss 1.4801568984985352 - moving ave loss 1.5995661720573415\n",
            "step 2710 - loss 1.2293832302093506 - moving ave loss 1.5625478778725426\n",
            "step 2711 - loss 1.7499916553497314 - moving ave loss 1.5812922556202615\n",
            "step 2712 - loss 1.4249553680419922 - moving ave loss 1.5656585668624345\n",
            "step 2713 - loss 1.2303805351257324 - moving ave loss 1.5321307636887644\n",
            "step 2714 - loss 1.3811008930206299 - moving ave loss 1.517027776621951\n",
            "Finish 118 epoch(es)\n",
            "step 2715 - loss 1.6200008392333984 - moving ave loss 1.5273250828830958\n",
            "step 2716 - loss 1.8791980743408203 - moving ave loss 1.5625123820288682\n",
            "step 2717 - loss 1.717217206954956 - moving ave loss 1.577982864521477\n",
            "step 2718 - loss 1.518911361694336 - moving ave loss 1.5720757142387631\n",
            "step 2719 - loss 1.5946145057678223 - moving ave loss 1.574329593391669\n",
            "step 2720 - loss 1.9435675144195557 - moving ave loss 1.6112533854944577\n",
            "step 2721 - loss 1.7086002826690674 - moving ave loss 1.6209880752119188\n",
            "step 2722 - loss 1.6741619110107422 - moving ave loss 1.6263054587918013\n",
            "step 2723 - loss 1.5433871746063232 - moving ave loss 1.6180136303732535\n",
            "step 2724 - loss 1.770585060119629 - moving ave loss 1.633270773347891\n",
            "step 2725 - loss 1.6763191223144531 - moving ave loss 1.6375756082445474\n",
            "step 2726 - loss 1.4581968784332275 - moving ave loss 1.6196377352634153\n",
            "step 2727 - loss 1.5432308912277222 - moving ave loss 1.611997050859846\n",
            "step 2728 - loss 1.64832603931427 - moving ave loss 1.6156299497052884\n",
            "step 2729 - loss 1.4855504035949707 - moving ave loss 1.6026219950942566\n",
            "step 2730 - loss 1.4949672222137451 - moving ave loss 1.5918565178062056\n",
            "step 2731 - loss 2.121476173400879 - moving ave loss 1.6448184833656732\n",
            "step 2732 - loss 1.5430517196655273 - moving ave loss 1.6346418069956585\n",
            "step 2733 - loss 1.615174412727356 - moving ave loss 1.6326950675688283\n",
            "step 2734 - loss 1.2448126077651978 - moving ave loss 1.5939068215884653\n",
            "step 2735 - loss 1.5661439895629883 - moving ave loss 1.5911305383859178\n",
            "step 2736 - loss 1.3012456893920898 - moving ave loss 1.5621420534865351\n",
            "step 2737 - loss 1.671207070350647 - moving ave loss 1.5730485551729463\n",
            "Finish 119 epoch(es)\n",
            "step 2738 - loss 1.984605312347412 - moving ave loss 1.6142042308903928\n",
            "step 2739 - loss 1.4423577785491943 - moving ave loss 1.597019585656273\n",
            "step 2740 - loss 2.2357072830200195 - moving ave loss 1.6608883553926477\n",
            "step 2741 - loss 1.8567378520965576 - moving ave loss 1.6804733050630387\n",
            "step 2742 - loss 1.5707372426986694 - moving ave loss 1.6694996988266018\n",
            "step 2743 - loss 1.629638910293579 - moving ave loss 1.6655136199732996\n",
            "step 2744 - loss 1.6299854516983032 - moving ave loss 1.6619608031458\n",
            "step 2745 - loss 1.7147048711776733 - moving ave loss 1.6672352099489873\n",
            "step 2746 - loss 1.527567744255066 - moving ave loss 1.6532684633795953\n",
            "step 2747 - loss 1.7420110702514648 - moving ave loss 1.6621427240667823\n",
            "step 2748 - loss 1.2928426265716553 - moving ave loss 1.6252127143172697\n",
            "step 2749 - loss 1.664769172668457 - moving ave loss 1.6291683601523883\n",
            "step 2750 - loss 1.1371455192565918 - moving ave loss 1.5799660760628087\n",
            "Checkpoint at step 2750\n",
            "step 2751 - loss 1.5522178411483765 - moving ave loss 1.5771912525713656\n",
            "step 2752 - loss 1.3087714910507202 - moving ave loss 1.550349276419301\n",
            "step 2753 - loss 1.2577680349349976 - moving ave loss 1.5210911522708708\n",
            "step 2754 - loss 1.2840327024459839 - moving ave loss 1.497385307288382\n",
            "step 2755 - loss 1.7494386434555054 - moving ave loss 1.5225906409050944\n",
            "step 2756 - loss 1.5215623378753662 - moving ave loss 1.5224878106021216\n",
            "step 2757 - loss 1.514277458190918 - moving ave loss 1.5216667753610011\n",
            "step 2758 - loss 1.54581618309021 - moving ave loss 1.524081716133922\n",
            "step 2759 - loss 1.2165025472640991 - moving ave loss 1.4933237992469397\n",
            "step 2760 - loss 1.6389789581298828 - moving ave loss 1.507889315135234\n",
            "Finish 120 epoch(es)\n",
            "step 2761 - loss 1.4111971855163574 - moving ave loss 1.4982201021733463\n",
            "step 2762 - loss 1.525980830192566 - moving ave loss 1.5009961749752683\n",
            "step 2763 - loss 1.7409460544586182 - moving ave loss 1.5249911629236033\n",
            "step 2764 - loss 1.935969591140747 - moving ave loss 1.5660890057453176\n",
            "step 2765 - loss 1.5080630779266357 - moving ave loss 1.5602864129634495\n",
            "step 2766 - loss 1.3941683769226074 - moving ave loss 1.5436746093593652\n",
            "step 2767 - loss 1.2371087074279785 - moving ave loss 1.5130180191662266\n",
            "step 2768 - loss 1.7252193689346313 - moving ave loss 1.5342381541430672\n",
            "step 2769 - loss 1.6265772581100464 - moving ave loss 1.5434720645397653\n",
            "step 2770 - loss 1.530576229095459 - moving ave loss 1.5421824809953346\n",
            "step 2771 - loss 1.7467848062515259 - moving ave loss 1.562642713520954\n",
            "step 2772 - loss 1.8463797569274902 - moving ave loss 1.5910164178616075\n",
            "step 2773 - loss 2.0847489833831787 - moving ave loss 1.6403896744137647\n",
            "step 2774 - loss 1.8596618175506592 - moving ave loss 1.6623168887274542\n",
            "step 2775 - loss 1.9587525129318237 - moving ave loss 1.6919604511478914\n",
            "step 2776 - loss 1.6264355182647705 - moving ave loss 1.6854079578595793\n",
            "step 2777 - loss 1.1963473558425903 - moving ave loss 1.6365018976578805\n",
            "step 2778 - loss 1.870446801185608 - moving ave loss 1.6598963880106532\n",
            "step 2779 - loss 1.0658880472183228 - moving ave loss 1.60049555393142\n",
            "step 2780 - loss 1.647296667098999 - moving ave loss 1.605175665248178\n",
            "step 2781 - loss 1.5922387838363647 - moving ave loss 1.6038819771069965\n",
            "step 2782 - loss 1.2725719213485718 - moving ave loss 1.570750971531154\n",
            "step 2783 - loss 1.6227731704711914 - moving ave loss 1.5759531914251579\n",
            "Finish 121 epoch(es)\n",
            "step 2784 - loss 1.4898293018341064 - moving ave loss 1.567340802466053\n",
            "step 2785 - loss 1.484401822090149 - moving ave loss 1.5590469044284625\n",
            "step 2786 - loss 1.6152055263519287 - moving ave loss 1.564662766620809\n",
            "step 2787 - loss 1.6658974885940552 - moving ave loss 1.5747862388181337\n",
            "step 2788 - loss 1.2773467302322388 - moving ave loss 1.5450422879595442\n",
            "step 2789 - loss 1.6830779314041138 - moving ave loss 1.5588458523040012\n",
            "step 2790 - loss 1.5836114883422852 - moving ave loss 1.5613224159078296\n",
            "step 2791 - loss 1.7472708225250244 - moving ave loss 1.5799172565695492\n",
            "step 2792 - loss 1.6628899574279785 - moving ave loss 1.588214526655392\n",
            "step 2793 - loss 1.0935524702072144 - moving ave loss 1.5387483210105743\n",
            "step 2794 - loss 1.484154224395752 - moving ave loss 1.5332889113490922\n",
            "step 2795 - loss 1.4353933334350586 - moving ave loss 1.5234993535576888\n",
            "step 2796 - loss 1.988673448562622 - moving ave loss 1.5700167630581823\n",
            "step 2797 - loss 1.5781391859054565 - moving ave loss 1.5708290053429097\n",
            "step 2798 - loss 1.5715583562850952 - moving ave loss 1.5709019404371283\n",
            "step 2799 - loss 1.6848597526550293 - moving ave loss 1.5822977216589185\n",
            "step 2800 - loss 1.6344118118286133 - moving ave loss 1.587509130675888\n",
            "step 2801 - loss 1.6403454542160034 - moving ave loss 1.5927927630298995\n",
            "step 2802 - loss 1.8366367816925049 - moving ave loss 1.61717716489616\n",
            "step 2803 - loss 1.3282010555267334 - moving ave loss 1.5882795539592174\n",
            "step 2804 - loss 1.4499053955078125 - moving ave loss 1.574442138114077\n",
            "step 2805 - loss 1.9795291423797607 - moving ave loss 1.6149508385406455\n",
            "step 2806 - loss 1.5852351188659668 - moving ave loss 1.6119792665731776\n",
            "Finish 122 epoch(es)\n",
            "step 2807 - loss 1.9223430156707764 - moving ave loss 1.6430156414829375\n",
            "step 2808 - loss 1.6794465780258179 - moving ave loss 1.6466587351372257\n",
            "step 2809 - loss 1.3715674877166748 - moving ave loss 1.6191496103951706\n",
            "step 2810 - loss 1.5171124935150146 - moving ave loss 1.6089458987071552\n",
            "step 2811 - loss 1.8495160341262817 - moving ave loss 1.633002912249068\n",
            "step 2812 - loss 1.7928041219711304 - moving ave loss 1.648983033221274\n",
            "step 2813 - loss 1.555617094039917 - moving ave loss 1.6396464393031382\n",
            "step 2814 - loss 1.2302203178405762 - moving ave loss 1.598703827156882\n",
            "step 2815 - loss 1.5889782905578613 - moving ave loss 1.59773127349698\n",
            "step 2816 - loss 1.457948088645935 - moving ave loss 1.5837529550118754\n",
            "step 2817 - loss 1.5095839500427246 - moving ave loss 1.5763360545149605\n",
            "step 2818 - loss 1.6782697439193726 - moving ave loss 1.586529423455402\n",
            "step 2819 - loss 1.2589285373687744 - moving ave loss 1.5537693348467394\n",
            "step 2820 - loss 1.5008877515792847 - moving ave loss 1.548481176519994\n",
            "step 2821 - loss 1.4024944305419922 - moving ave loss 1.5338825019221938\n",
            "step 2822 - loss 1.5803112983703613 - moving ave loss 1.5385253815670108\n",
            "step 2823 - loss 1.4677743911743164 - moving ave loss 1.5314502825277414\n",
            "step 2824 - loss 1.8111565113067627 - moving ave loss 1.5594209054056436\n",
            "step 2825 - loss 1.8826041221618652 - moving ave loss 1.5917392270812658\n",
            "step 2826 - loss 1.7024638652801514 - moving ave loss 1.6028116909011545\n",
            "step 2827 - loss 1.5036275386810303 - moving ave loss 1.5928932756791423\n",
            "step 2828 - loss 1.3547860383987427 - moving ave loss 1.5690825519511025\n",
            "step 2829 - loss 1.947177767753601 - moving ave loss 1.6068920735313523\n",
            "Finish 123 epoch(es)\n",
            "step 2830 - loss 1.6306371688842773 - moving ave loss 1.6092665830666448\n",
            "step 2831 - loss 1.5261340141296387 - moving ave loss 1.6009533261729443\n",
            "step 2832 - loss 1.846341848373413 - moving ave loss 1.6254921783929912\n",
            "step 2833 - loss 1.8222442865371704 - moving ave loss 1.6451673892074092\n",
            "step 2834 - loss 1.3810452222824097 - moving ave loss 1.6187551725149092\n",
            "step 2835 - loss 1.177696704864502 - moving ave loss 1.5746493257498684\n",
            "step 2836 - loss 1.7001688480377197 - moving ave loss 1.5872012779786535\n",
            "step 2837 - loss 1.4738516807556152 - moving ave loss 1.5758663182563497\n",
            "step 2838 - loss 1.6211729049682617 - moving ave loss 1.580396976927541\n",
            "step 2839 - loss 1.2291008234024048 - moving ave loss 1.5452673615750274\n",
            "step 2840 - loss 1.7774206399917603 - moving ave loss 1.5684826894167008\n",
            "step 2841 - loss 1.4157048463821411 - moving ave loss 1.5532049051132448\n",
            "step 2842 - loss 1.306917428970337 - moving ave loss 1.5285761574989543\n",
            "step 2843 - loss 1.7273112535476685 - moving ave loss 1.5484496671038257\n",
            "step 2844 - loss 1.5544452667236328 - moving ave loss 1.5490492270658063\n",
            "step 2845 - loss 1.7897324562072754 - moving ave loss 1.5731175499799532\n",
            "step 2846 - loss 1.3499257564544678 - moving ave loss 1.5507983706274049\n",
            "step 2847 - loss 1.5333023071289062 - moving ave loss 1.549048764277555\n",
            "step 2848 - loss 2.031804084777832 - moving ave loss 1.5973242963275829\n",
            "step 2849 - loss 1.6312459707260132 - moving ave loss 1.600716463767426\n",
            "step 2850 - loss 1.5541579723358154 - moving ave loss 1.5960606146242649\n",
            "step 2851 - loss 1.3272284269332886 - moving ave loss 1.5691773958551674\n",
            "step 2852 - loss 1.817874550819397 - moving ave loss 1.5940471113515904\n",
            "Finish 124 epoch(es)\n",
            "step 2853 - loss 1.6392390727996826 - moving ave loss 1.5985663074963996\n",
            "step 2854 - loss 1.6900444030761719 - moving ave loss 1.607714117054377\n",
            "step 2855 - loss 1.1407856941223145 - moving ave loss 1.5610212747611707\n",
            "step 2856 - loss 1.6210534572601318 - moving ave loss 1.567024493011067\n",
            "step 2857 - loss 1.2961807250976562 - moving ave loss 1.539940116219726\n",
            "step 2858 - loss 1.7199796438217163 - moving ave loss 1.557944068979925\n",
            "step 2859 - loss 1.3429327011108398 - moving ave loss 1.5364429321930166\n",
            "step 2860 - loss 1.8089210987091064 - moving ave loss 1.5636907488446257\n",
            "step 2861 - loss 1.6716599464416504 - moving ave loss 1.574487668604328\n",
            "step 2862 - loss 1.758044719696045 - moving ave loss 1.5928433737135\n",
            "step 2863 - loss 1.2738170623779297 - moving ave loss 1.5609407425799429\n",
            "step 2864 - loss 1.1841429471969604 - moving ave loss 1.5232609630416447\n",
            "step 2865 - loss 1.5861725807189941 - moving ave loss 1.5295521248093797\n",
            "step 2866 - loss 1.595626711845398 - moving ave loss 1.5361595835129815\n",
            "step 2867 - loss 1.2865245342254639 - moving ave loss 1.5111960785842298\n",
            "step 2868 - loss 1.8520538806915283 - moving ave loss 1.5452818587949597\n",
            "step 2869 - loss 1.3851890563964844 - moving ave loss 1.5292725785551122\n",
            "step 2870 - loss 1.7685216665267944 - moving ave loss 1.5531974873522805\n",
            "step 2871 - loss 1.6448336839675903 - moving ave loss 1.5623611070138115\n",
            "step 2872 - loss 1.569187045097351 - moving ave loss 1.5630437008221654\n",
            "step 2873 - loss 1.412292718887329 - moving ave loss 1.5479686026286819\n",
            "step 2874 - loss 1.4329599142074585 - moving ave loss 1.5364677337865595\n",
            "step 2875 - loss 1.19254732131958 - moving ave loss 1.5020756925398615\n",
            "Checkpoint at step 2875\n",
            "Finish 125 epoch(es)\n",
            "step 2876 - loss 1.6558375358581543 - moving ave loss 1.5174518768716907\n",
            "step 2877 - loss 1.8127316236495972 - moving ave loss 1.5469798515494815\n",
            "step 2878 - loss 2.269641399383545 - moving ave loss 1.619246006332888\n",
            "step 2879 - loss 1.5897761583328247 - moving ave loss 1.6162990215328816\n",
            "step 2880 - loss 1.5146098136901855 - moving ave loss 1.6061301007486122\n",
            "step 2881 - loss 1.3022077083587646 - moving ave loss 1.5757378615096274\n",
            "step 2882 - loss 1.4716852903366089 - moving ave loss 1.5653326043923255\n",
            "step 2883 - loss 1.7837345600128174 - moving ave loss 1.5871727999543748\n",
            "step 2884 - loss 1.0999630689620972 - moving ave loss 1.538451826855147\n",
            "step 2885 - loss 1.2969276905059814 - moving ave loss 1.5142994132202305\n",
            "step 2886 - loss 1.8022468090057373 - moving ave loss 1.5430941527987811\n",
            "step 2887 - loss 1.5787123441696167 - moving ave loss 1.5466559719358648\n",
            "step 2888 - loss 1.7723426818847656 - moving ave loss 1.569224642930755\n",
            "step 2889 - loss 1.8342475891113281 - moving ave loss 1.5957269375488123\n",
            "step 2890 - loss 1.8581199645996094 - moving ave loss 1.6219662402538921\n",
            "step 2891 - loss 1.7647944688796997 - moving ave loss 1.636249063116473\n",
            "step 2892 - loss 2.3900442123413086 - moving ave loss 1.7116285780389566\n",
            "step 2893 - loss 1.3757648468017578 - moving ave loss 1.6780422049152366\n",
            "step 2894 - loss 1.2210522890090942 - moving ave loss 1.6323432133246225\n",
            "step 2895 - loss 1.6181360483169556 - moving ave loss 1.6309224968238558\n",
            "step 2896 - loss 1.281144142150879 - moving ave loss 1.5959446613565582\n",
            "step 2897 - loss 1.5138270854949951 - moving ave loss 1.587732903770402\n",
            "step 2898 - loss 1.2990586757659912 - moving ave loss 1.558865480969961\n",
            "Finish 126 epoch(es)\n",
            "step 2899 - loss 1.8377265930175781 - moving ave loss 1.5867515921747228\n",
            "step 2900 - loss 1.6490782499313354 - moving ave loss 1.5929842579503841\n",
            "step 2901 - loss 1.367668628692627 - moving ave loss 1.5704526950246085\n",
            "step 2902 - loss 1.5432476997375488 - moving ave loss 1.5677321954959025\n",
            "step 2903 - loss 1.6785905361175537 - moving ave loss 1.5788180295580678\n",
            "step 2904 - loss 1.481613278388977 - moving ave loss 1.569097554441159\n",
            "step 2905 - loss 1.3392949104309082 - moving ave loss 1.5461172900401339\n",
            "step 2906 - loss 1.4872815608978271 - moving ave loss 1.540233717125903\n",
            "step 2907 - loss 1.76123046875 - moving ave loss 1.5623333922883127\n",
            "step 2908 - loss 2.0974831581115723 - moving ave loss 1.6158483688706387\n",
            "step 2909 - loss 1.550208330154419 - moving ave loss 1.6092843649990167\n",
            "step 2910 - loss 1.6031274795532227 - moving ave loss 1.6086686764544373\n",
            "step 2911 - loss 1.379356861114502 - moving ave loss 1.585737494920444\n",
            "step 2912 - loss 1.2509424686431885 - moving ave loss 1.5522579922927184\n",
            "step 2913 - loss 1.6031804084777832 - moving ave loss 1.557350233911225\n",
            "step 2914 - loss 1.3025445938110352 - moving ave loss 1.531869669901206\n",
            "step 2915 - loss 1.5624078512191772 - moving ave loss 1.5349234880330034\n",
            "step 2916 - loss 1.6341934204101562 - moving ave loss 1.544850481270719\n",
            "step 2917 - loss 1.475801944732666 - moving ave loss 1.5379456276169137\n",
            "step 2918 - loss 1.3793143033981323 - moving ave loss 1.5220824951950356\n",
            "step 2919 - loss 1.6846492290496826 - moving ave loss 1.5383391685805003\n",
            "step 2920 - loss 1.5413507223129272 - moving ave loss 1.538640323953743\n",
            "step 2921 - loss 1.6280425786972046 - moving ave loss 1.5475805494280892\n",
            "Finish 127 epoch(es)\n",
            "step 2922 - loss 1.7492091655731201 - moving ave loss 1.5677434110425923\n",
            "step 2923 - loss 2.167586326599121 - moving ave loss 1.627727702598245\n",
            "step 2924 - loss 1.3499114513397217 - moving ave loss 1.5999460774723928\n",
            "step 2925 - loss 1.1738977432250977 - moving ave loss 1.5573412440476635\n",
            "step 2926 - loss 1.0669384002685547 - moving ave loss 1.5083009596697527\n",
            "step 2927 - loss 1.4980884790420532 - moving ave loss 1.5072797116069827\n",
            "step 2928 - loss 1.739932894706726 - moving ave loss 1.5305450299169572\n",
            "step 2929 - loss 1.4883545637130737 - moving ave loss 1.526325983296569\n",
            "step 2930 - loss 2.0158982276916504 - moving ave loss 1.5752832077360772\n",
            "step 2931 - loss 1.3713278770446777 - moving ave loss 1.5548876746669371\n",
            "step 2932 - loss 1.7218499183654785 - moving ave loss 1.5715838990367914\n",
            "step 2933 - loss 1.2549751996994019 - moving ave loss 1.5399230291030526\n",
            "step 2934 - loss 1.610552430152893 - moving ave loss 1.5469859692080368\n",
            "step 2935 - loss 1.5823343992233276 - moving ave loss 1.550520812209566\n",
            "step 2936 - loss 1.5686897039413452 - moving ave loss 1.552337701382744\n",
            "step 2937 - loss 1.62992525100708 - moving ave loss 1.5600964563451776\n",
            "step 2938 - loss 1.8083722591400146 - moving ave loss 1.5849240366246613\n",
            "step 2939 - loss 1.915178656578064 - moving ave loss 1.6179494986200016\n",
            "step 2940 - loss 1.980499029159546 - moving ave loss 1.654204451673956\n",
            "step 2941 - loss 1.5512628555297852 - moving ave loss 1.643910292059539\n",
            "step 2942 - loss 1.9159867763519287 - moving ave loss 1.6711179404887782\n",
            "step 2943 - loss 1.4764878749847412 - moving ave loss 1.6516549339383746\n",
            "step 2944 - loss 1.5428321361541748 - moving ave loss 1.6407726541599548\n",
            "Finish 128 epoch(es)\n",
            "step 2945 - loss 1.6093069314956665 - moving ave loss 1.637626081893526\n",
            "step 2946 - loss 1.7550904750823975 - moving ave loss 1.649372521212413\n",
            "step 2947 - loss 1.4682374000549316 - moving ave loss 1.631259009096665\n",
            "step 2948 - loss 1.242204189300537 - moving ave loss 1.5923535271170524\n",
            "step 2949 - loss 1.380354642868042 - moving ave loss 1.5711536386921514\n",
            "step 2950 - loss 1.4707763195037842 - moving ave loss 1.5611159067733147\n",
            "step 2951 - loss 1.971186637878418 - moving ave loss 1.6021229798838252\n",
            "step 2952 - loss 1.597294569015503 - moving ave loss 1.6016401387969932\n",
            "step 2953 - loss 1.2325656414031982 - moving ave loss 1.5647326890576136\n",
            "step 2954 - loss 1.3392254114151 - moving ave loss 1.5421819612933623\n",
            "step 2955 - loss 1.3903284072875977 - moving ave loss 1.5269966058927857\n",
            "step 2956 - loss 1.1648043394088745 - moving ave loss 1.4907773792443946\n",
            "step 2957 - loss 1.7589812278747559 - moving ave loss 1.5175977641074307\n",
            "step 2958 - loss 1.9823510646820068 - moving ave loss 1.5640730941648884\n",
            "step 2959 - loss 1.638493537902832 - moving ave loss 1.5715151385386827\n",
            "step 2960 - loss 1.4660946130752563 - moving ave loss 1.5609730859923403\n",
            "step 2961 - loss 1.6359801292419434 - moving ave loss 1.5684737903173005\n",
            "step 2962 - loss 1.655444622039795 - moving ave loss 1.57717087348955\n",
            "step 2963 - loss 1.681267261505127 - moving ave loss 1.5875805122911077\n",
            "step 2964 - loss 1.5893223285675049 - moving ave loss 1.5877546939187477\n",
            "step 2965 - loss 1.7132195234298706 - moving ave loss 1.60030117686986\n",
            "step 2966 - loss 1.5274431705474854 - moving ave loss 1.5930153762376227\n",
            "step 2967 - loss 1.2454785108566284 - moving ave loss 1.5582616896995232\n",
            "Finish 129 epoch(es)\n",
            "step 2968 - loss 1.2179715633392334 - moving ave loss 1.5242326770634942\n",
            "step 2969 - loss 1.5966476202011108 - moving ave loss 1.531474171377256\n",
            "step 2970 - loss 1.658050775527954 - moving ave loss 1.5441318317923258\n",
            "step 2971 - loss 1.3978493213653564 - moving ave loss 1.5295035807496289\n",
            "step 2972 - loss 1.7817389965057373 - moving ave loss 1.5547271223252397\n",
            "step 2973 - loss 1.5242760181427002 - moving ave loss 1.5516820119069858\n",
            "step 2974 - loss 1.5264867544174194 - moving ave loss 1.5491624861580293\n",
            "step 2975 - loss 1.522300362586975 - moving ave loss 1.546476273800924\n",
            "step 2976 - loss 1.2064244747161865 - moving ave loss 1.5124710938924502\n",
            "step 2977 - loss 1.719017505645752 - moving ave loss 1.5331257350677805\n",
            "step 2978 - loss 1.236574411392212 - moving ave loss 1.5034706027002238\n",
            "step 2979 - loss 1.7177822589874268 - moving ave loss 1.5249017683289443\n",
            "step 2980 - loss 1.800532341003418 - moving ave loss 1.5524648255963918\n",
            "step 2981 - loss 1.8430286645889282 - moving ave loss 1.5815212094956455\n",
            "step 2982 - loss 1.4862077236175537 - moving ave loss 1.5719898609078364\n",
            "step 2983 - loss 1.4090481996536255 - moving ave loss 1.5556956947824154\n",
            "step 2984 - loss 1.7811999320983887 - moving ave loss 1.5782461185140129\n",
            "step 2985 - loss 1.3737101554870605 - moving ave loss 1.5577925222113176\n",
            "step 2986 - loss 1.528430700302124 - moving ave loss 1.5548563400203983\n",
            "step 2987 - loss 1.6935722827911377 - moving ave loss 1.5687279342974723\n",
            "step 2988 - loss 1.4548203945159912 - moving ave loss 1.5573371803193243\n",
            "step 2989 - loss 1.3902966976165771 - moving ave loss 1.5406331320490496\n",
            "step 2990 - loss 1.2734066247940063 - moving ave loss 1.5139104813235453\n",
            "Finish 130 epoch(es)\n",
            "step 2991 - loss 1.3397012948989868 - moving ave loss 1.4964895626810895\n",
            "step 2992 - loss 1.4486721754074097 - moving ave loss 1.4917078239537216\n",
            "step 2993 - loss 1.7024747133255005 - moving ave loss 1.5127845128908997\n",
            "step 2994 - loss 1.2825398445129395 - moving ave loss 1.4897600460531037\n",
            "step 2995 - loss 1.2119182348251343 - moving ave loss 1.4619758649303067\n",
            "step 2996 - loss 1.5976645946502686 - moving ave loss 1.475544737902303\n",
            "step 2997 - loss 1.5036866664886475 - moving ave loss 1.4783589307609375\n",
            "step 2998 - loss 1.757256269454956 - moving ave loss 1.5062486646303395\n",
            "step 2999 - loss 1.7882158756256104 - moving ave loss 1.5344453857298666\n",
            "step 3000 - loss 1.1571491956710815 - moving ave loss 1.496715766723988\n",
            "Checkpoint at step 3000\n",
            "step 3001 - loss 1.2366864681243896 - moving ave loss 1.4707128368640283\n",
            "step 3002 - loss 1.4767611026763916 - moving ave loss 1.4713176634452647\n",
            "step 3003 - loss 1.4599392414093018 - moving ave loss 1.4701798212416686\n",
            "step 3004 - loss 1.5295801162719727 - moving ave loss 1.476119850744699\n",
            "step 3005 - loss 1.8747490644454956 - moving ave loss 1.5159827721147787\n",
            "step 3006 - loss 0.9648600816726685 - moving ave loss 1.4608705030705678\n",
            "step 3007 - loss 1.7347179651260376 - moving ave loss 1.4882552492761147\n",
            "step 3008 - loss 1.660220742225647 - moving ave loss 1.505451798571068\n",
            "step 3009 - loss 1.7564235925674438 - moving ave loss 1.5305489779707058\n",
            "step 3010 - loss 1.6442153453826904 - moving ave loss 1.5419156147119044\n",
            "step 3011 - loss 1.4128451347351074 - moving ave loss 1.5290085667142248\n",
            "step 3012 - loss 1.5090839862823486 - moving ave loss 1.5270161086710372\n",
            "step 3013 - loss 1.3056583404541016 - moving ave loss 1.5048803318493436\n",
            "Finish 131 epoch(es)\n",
            "step 3014 - loss 1.672537088394165 - moving ave loss 1.5216460075038258\n",
            "step 3015 - loss 1.615230679512024 - moving ave loss 1.5310044747046456\n",
            "step 3016 - loss 1.390843391418457 - moving ave loss 1.5169883663760269\n",
            "step 3017 - loss 1.4775819778442383 - moving ave loss 1.513047727522848\n",
            "step 3018 - loss 1.320148229598999 - moving ave loss 1.4937577777304631\n",
            "step 3019 - loss 1.762970209121704 - moving ave loss 1.5206790208695873\n",
            "step 3020 - loss 1.268129825592041 - moving ave loss 1.4954241013418328\n",
            "step 3021 - loss 1.6995837688446045 - moving ave loss 1.51584006809211\n",
            "step 3022 - loss 1.59584379196167 - moving ave loss 1.5238404404790662\n",
            "step 3023 - loss 1.8569594621658325 - moving ave loss 1.557152342647743\n",
            "step 3024 - loss 1.3196312189102173 - moving ave loss 1.5334002302739904\n",
            "step 3025 - loss 1.751484751701355 - moving ave loss 1.555208682416727\n",
            "step 3026 - loss 1.760094404220581 - moving ave loss 1.5756972545971126\n",
            "step 3027 - loss 1.801931381225586 - moving ave loss 1.59832066725996\n",
            "step 3028 - loss 1.6626250743865967 - moving ave loss 1.6047511079726235\n",
            "step 3029 - loss 1.5828596353530884 - moving ave loss 1.60256196071067\n",
            "step 3030 - loss 1.114746332168579 - moving ave loss 1.553780397856461\n",
            "step 3031 - loss 1.4693952798843384 - moving ave loss 1.5453418860592487\n",
            "step 3032 - loss 2.054802656173706 - moving ave loss 1.5962879630706945\n",
            "step 3033 - loss 1.7234218120574951 - moving ave loss 1.6090013479693746\n",
            "step 3034 - loss 1.514626145362854 - moving ave loss 1.5995638277087225\n",
            "step 3035 - loss 1.3389204740524292 - moving ave loss 1.5734994923430932\n",
            "step 3036 - loss 1.6017401218414307 - moving ave loss 1.576323555292927\n",
            "Finish 132 epoch(es)\n",
            "step 3037 - loss 1.5056827068328857 - moving ave loss 1.569259470446923\n",
            "step 3038 - loss 1.6560890674591064 - moving ave loss 1.5779424301481413\n",
            "step 3039 - loss 1.652033805847168 - moving ave loss 1.585351567718044\n",
            "step 3040 - loss 1.608224630355835 - moving ave loss 1.5876388739818232\n",
            "step 3041 - loss 1.538387656211853 - moving ave loss 1.5827137522048262\n",
            "step 3042 - loss 1.6697306632995605 - moving ave loss 1.5914154433142997\n",
            "step 3043 - loss 1.3430869579315186 - moving ave loss 1.5665825947760217\n",
            "step 3044 - loss 1.4011659622192383 - moving ave loss 1.5500409315203434\n",
            "step 3045 - loss 1.7291133403778076 - moving ave loss 1.56794817240609\n",
            "step 3046 - loss 1.5875672101974487 - moving ave loss 1.5699100761852258\n",
            "step 3047 - loss 1.512614369392395 - moving ave loss 1.5641805055059428\n",
            "step 3048 - loss 1.7079617977142334 - moving ave loss 1.5785586347267717\n",
            "step 3049 - loss 1.869450330734253 - moving ave loss 1.6076478043275197\n",
            "step 3050 - loss 1.5219409465789795 - moving ave loss 1.5990771185526658\n",
            "step 3051 - loss 1.7161478996276855 - moving ave loss 1.610784196660168\n",
            "step 3052 - loss 1.371222972869873 - moving ave loss 1.5868280742811385\n",
            "step 3053 - loss 1.3917076587677002 - moving ave loss 1.5673160327297946\n",
            "step 3054 - loss 1.4731535911560059 - moving ave loss 1.5578997885724157\n",
            "step 3055 - loss 1.5361568927764893 - moving ave loss 1.5557254989928233\n",
            "step 3056 - loss 1.5027027130126953 - moving ave loss 1.5504232203948105\n",
            "step 3057 - loss 1.4420868158340454 - moving ave loss 1.539589579938734\n",
            "step 3058 - loss 1.4695039987564087 - moving ave loss 1.5325810218205016\n",
            "step 3059 - loss 1.8860173225402832 - moving ave loss 1.5679246518924799\n",
            "Finish 133 epoch(es)\n",
            "step 3060 - loss 1.8349404335021973 - moving ave loss 1.5946262300534515\n",
            "step 3061 - loss 1.8846802711486816 - moving ave loss 1.6236316341629746\n",
            "step 3062 - loss 1.1434437036514282 - moving ave loss 1.57561284111182\n",
            "step 3063 - loss 1.4748455286026 - moving ave loss 1.565536109860898\n",
            "step 3064 - loss 1.7763340473175049 - moving ave loss 1.5866159036065588\n",
            "step 3065 - loss 1.2566839456558228 - moving ave loss 1.5536227078114853\n",
            "step 3066 - loss 1.4800533056259155 - moving ave loss 1.5462657675929283\n",
            "step 3067 - loss 1.105262279510498 - moving ave loss 1.5021654187846853\n",
            "step 3068 - loss 1.6494661569595337 - moving ave loss 1.5168954926021703\n",
            "step 3069 - loss 1.1358013153076172 - moving ave loss 1.4787860748727149\n",
            "step 3070 - loss 1.3391401767730713 - moving ave loss 1.4648214850627503\n",
            "step 3071 - loss 1.2505977153778076 - moving ave loss 1.443399108094256\n",
            "step 3072 - loss 1.4905681610107422 - moving ave loss 1.4481160133859046\n",
            "step 3073 - loss 1.7542130947113037 - moving ave loss 1.4787257215184444\n",
            "step 3074 - loss 1.3533968925476074 - moving ave loss 1.4661928386213607\n",
            "step 3075 - loss 1.6437968015670776 - moving ave loss 1.4839532349159326\n",
            "step 3076 - loss 1.712244987487793 - moving ave loss 1.5067824101731189\n",
            "step 3077 - loss 1.2990812063217163 - moving ave loss 1.4860122897879788\n",
            "step 3078 - loss 1.5851049423217773 - moving ave loss 1.4959215550413587\n",
            "step 3079 - loss 1.5370934009552002 - moving ave loss 1.500038739632743\n",
            "step 3080 - loss 1.9217941761016846 - moving ave loss 1.5422142832796373\n",
            "step 3081 - loss 1.5631523132324219 - moving ave loss 1.5443080862749157\n",
            "step 3082 - loss 1.492272138595581 - moving ave loss 1.5391044915069823\n",
            "Finish 134 epoch(es)\n",
            "step 3083 - loss 1.7826040983200073 - moving ave loss 1.5634544521882847\n",
            "step 3084 - loss 1.6938295364379883 - moving ave loss 1.576491960613255\n",
            "step 3085 - loss 1.504705786705017 - moving ave loss 1.569313343222431\n",
            "step 3086 - loss 1.6337476968765259 - moving ave loss 1.5757567785878406\n",
            "step 3087 - loss 1.3990812301635742 - moving ave loss 1.558089223745414\n",
            "step 3088 - loss 1.4779324531555176 - moving ave loss 1.5500735466864244\n",
            "step 3089 - loss 1.8708252906799316 - moving ave loss 1.5821487210857752\n",
            "step 3090 - loss 1.4715502262115479 - moving ave loss 1.5710888715983526\n",
            "step 3091 - loss 1.9903485774993896 - moving ave loss 1.6130148421884565\n",
            "step 3092 - loss 1.4285695552825928 - moving ave loss 1.5945703134978702\n",
            "step 3093 - loss 1.4816999435424805 - moving ave loss 1.5832832765023312\n",
            "step 3094 - loss 1.3091583251953125 - moving ave loss 1.5558707813716293\n",
            "step 3095 - loss 1.5640358924865723 - moving ave loss 1.5566872924831237\n",
            "step 3096 - loss 1.6197333335876465 - moving ave loss 1.562991896593576\n",
            "step 3097 - loss 1.4788209199905396 - moving ave loss 1.5545747989332726\n",
            "step 3098 - loss 1.233954668045044 - moving ave loss 1.5225127858444496\n",
            "step 3099 - loss 1.3767116069793701 - moving ave loss 1.5079326679579417\n",
            "step 3100 - loss 1.4737088680267334 - moving ave loss 1.504510287964821\n",
            "step 3101 - loss 1.3514950275421143 - moving ave loss 1.4892087619225503\n",
            "step 3102 - loss 1.6913741827011108 - moving ave loss 1.5094253040004064\n",
            "step 3103 - loss 1.6841074228286743 - moving ave loss 1.5268935158832333\n",
            "step 3104 - loss 1.2822757959365845 - moving ave loss 1.5024317438885686\n",
            "step 3105 - loss 1.2740440368652344 - moving ave loss 1.4795929731862352\n",
            "Finish 135 epoch(es)\n",
            "step 3106 - loss 1.9347984790802002 - moving ave loss 1.5251135237756317\n",
            "step 3107 - loss 1.372227430343628 - moving ave loss 1.5098249144324314\n",
            "step 3108 - loss 1.6675894260406494 - moving ave loss 1.5256013655932532\n",
            "step 3109 - loss 1.7990847826004028 - moving ave loss 1.5529497072939682\n",
            "step 3110 - loss 1.144927978515625 - moving ave loss 1.512147534416134\n",
            "step 3111 - loss 1.8417551517486572 - moving ave loss 1.5451082961493863\n",
            "step 3112 - loss 1.5959547758102417 - moving ave loss 1.5501929441154718\n",
            "step 3113 - loss 1.4624147415161133 - moving ave loss 1.541415123855536\n",
            "step 3114 - loss 1.4217947721481323 - moving ave loss 1.5294530886847957\n",
            "step 3115 - loss 1.5896472930908203 - moving ave loss 1.5354725091253982\n",
            "step 3116 - loss 1.3027317523956299 - moving ave loss 1.5121984334524214\n",
            "step 3117 - loss 1.5902379751205444 - moving ave loss 1.520002387619234\n",
            "step 3118 - loss 1.7574050426483154 - moving ave loss 1.543742653122142\n",
            "step 3119 - loss 1.6941044330596924 - moving ave loss 1.5587788311158972\n",
            "step 3120 - loss 1.5601896047592163 - moving ave loss 1.5589199084802292\n",
            "step 3121 - loss 1.1259069442749023 - moving ave loss 1.5156186120596964\n",
            "step 3122 - loss 1.2630219459533691 - moving ave loss 1.4903589454490636\n",
            "step 3123 - loss 1.3863123655319214 - moving ave loss 1.4799542874573492\n",
            "step 3124 - loss 1.540527582168579 - moving ave loss 1.4860116169284723\n",
            "step 3125 - loss 1.3719629049301147 - moving ave loss 1.4746067457286367\n",
            "Checkpoint at step 3125\n",
            "step 3126 - loss 1.6650115251541138 - moving ave loss 1.4936472236711844\n",
            "step 3127 - loss 1.8227782249450684 - moving ave loss 1.5265603237985728\n",
            "step 3128 - loss 1.9038376808166504 - moving ave loss 1.5642880595003807\n",
            "Finish 136 epoch(es)\n",
            "step 3129 - loss 1.8164644241333008 - moving ave loss 1.5895056959636729\n",
            "step 3130 - loss 1.856600046157837 - moving ave loss 1.6162151309830892\n",
            "step 3131 - loss 1.462622880935669 - moving ave loss 1.6008559059783471\n",
            "step 3132 - loss 1.2927905321121216 - moving ave loss 1.5700493685917247\n",
            "step 3133 - loss 1.5877444744110107 - moving ave loss 1.5718188791736534\n",
            "step 3134 - loss 1.6709644794464111 - moving ave loss 1.5817334392009292\n",
            "step 3135 - loss 1.366398811340332 - moving ave loss 1.5601999764148695\n",
            "step 3136 - loss 1.6369811296463013 - moving ave loss 1.5678780917380128\n",
            "step 3137 - loss 1.015963077545166 - moving ave loss 1.512686590318728\n",
            "step 3138 - loss 1.3531367778778076 - moving ave loss 1.496731609074636\n",
            "step 3139 - loss 1.192896842956543 - moving ave loss 1.466348132462827\n",
            "step 3140 - loss 1.8785552978515625 - moving ave loss 1.5075688490017005\n",
            "step 3141 - loss 2.0067954063415527 - moving ave loss 1.5574915047356859\n",
            "step 3142 - loss 1.7879674434661865 - moving ave loss 1.580539098608736\n",
            "step 3143 - loss 1.3277413845062256 - moving ave loss 1.555259327198485\n",
            "step 3144 - loss 1.5639086961746216 - moving ave loss 1.5561242640960988\n",
            "step 3145 - loss 1.5871350765228271 - moving ave loss 1.5592253453387717\n",
            "step 3146 - loss 1.5296322107315063 - moving ave loss 1.5562660318780452\n",
            "step 3147 - loss 1.5685266256332397 - moving ave loss 1.5574920912535648\n",
            "step 3148 - loss 1.5075126886367798 - moving ave loss 1.5524941509918864\n",
            "step 3149 - loss 1.5739738941192627 - moving ave loss 1.554642125304624\n",
            "step 3150 - loss 1.4198813438415527 - moving ave loss 1.541166047158317\n",
            "step 3151 - loss 1.7190566062927246 - moving ave loss 1.5589551030717579\n",
            "Finish 137 epoch(es)\n",
            "step 3152 - loss 1.5074182748794556 - moving ave loss 1.5538014202525277\n",
            "step 3153 - loss 1.4621769189834595 - moving ave loss 1.544638970125621\n",
            "step 3154 - loss 1.3466949462890625 - moving ave loss 1.524844567741965\n",
            "step 3155 - loss 1.5150196552276611 - moving ave loss 1.5238620764905346\n",
            "step 3156 - loss 1.7257410287857056 - moving ave loss 1.5440499717200518\n",
            "step 3157 - loss 1.2207896709442139 - moving ave loss 1.511723941642468\n",
            "step 3158 - loss 1.4419450759887695 - moving ave loss 1.5047460550770981\n",
            "step 3159 - loss 1.8404834270477295 - moving ave loss 1.5383197922741614\n",
            "step 3160 - loss 1.398855447769165 - moving ave loss 1.5243733578236618\n",
            "step 3161 - loss 1.4824130535125732 - moving ave loss 1.520177327392553\n",
            "step 3162 - loss 1.5190786123275757 - moving ave loss 1.5200674558860552\n",
            "step 3163 - loss 1.4790574312210083 - moving ave loss 1.5159664534195507\n",
            "step 3164 - loss 1.5820635557174683 - moving ave loss 1.5225761636493425\n",
            "step 3165 - loss 1.846618890762329 - moving ave loss 1.554980436360641\n",
            "step 3166 - loss 1.4340170621871948 - moving ave loss 1.5428840989432964\n",
            "step 3167 - loss 1.425363302230835 - moving ave loss 1.5311320192720503\n",
            "step 3168 - loss 1.375871181488037 - moving ave loss 1.515605935493649\n",
            "step 3169 - loss 1.4330183267593384 - moving ave loss 1.507347174620218\n",
            "step 3170 - loss 1.6043421030044556 - moving ave loss 1.5170466674586418\n",
            "step 3171 - loss 1.6205756664276123 - moving ave loss 1.527399567355539\n",
            "step 3172 - loss 1.2702003717422485 - moving ave loss 1.50167964779421\n",
            "step 3173 - loss 1.985046148300171 - moving ave loss 1.5500162978448062\n",
            "step 3174 - loss 1.3596887588500977 - moving ave loss 1.5309835439453354\n",
            "Finish 138 epoch(es)\n",
            "step 3175 - loss 1.4195556640625 - moving ave loss 1.519840755957052\n",
            "step 3176 - loss 1.6118792295455933 - moving ave loss 1.5290446033159062\n",
            "step 3177 - loss 1.407494306564331 - moving ave loss 1.5168895736407488\n",
            "step 3178 - loss 1.5255554914474487 - moving ave loss 1.5177561654214187\n",
            "step 3179 - loss 1.5049854516983032 - moving ave loss 1.5164790940491073\n",
            "step 3180 - loss 1.3425374031066895 - moving ave loss 1.4990849249548654\n",
            "step 3181 - loss 1.5203723907470703 - moving ave loss 1.501213671534086\n",
            "step 3182 - loss 1.5851619243621826 - moving ave loss 1.5096084968168957\n",
            "step 3183 - loss 1.7114189863204956 - moving ave loss 1.529789545767256\n",
            "step 3184 - loss 1.37017023563385 - moving ave loss 1.5138276147539154\n",
            "step 3185 - loss 1.4063575267791748 - moving ave loss 1.5030806059564412\n",
            "step 3186 - loss 1.6206927299499512 - moving ave loss 1.5148418183557921\n",
            "step 3187 - loss 1.4169930219650269 - moving ave loss 1.5050569387167156\n",
            "step 3188 - loss 1.498542070388794 - moving ave loss 1.5044054518839234\n",
            "step 3189 - loss 1.0996806621551514 - moving ave loss 1.4639329729110462\n",
            "step 3190 - loss 1.7626147270202637 - moving ave loss 1.493801148321968\n",
            "step 3191 - loss 1.3403170108795166 - moving ave loss 1.478452734577723\n",
            "step 3192 - loss 1.5162385702133179 - moving ave loss 1.4822313181412825\n",
            "step 3193 - loss 1.4931864738464355 - moving ave loss 1.4833268337117977\n",
            "step 3194 - loss 1.7633318901062012 - moving ave loss 1.511327339351238\n",
            "step 3195 - loss 1.4862233400344849 - moving ave loss 1.5088169394195627\n",
            "step 3196 - loss 1.7485442161560059 - moving ave loss 1.532789667093207\n",
            "step 3197 - loss 1.7435483932495117 - moving ave loss 1.5538655397088377\n",
            "Finish 139 epoch(es)\n",
            "step 3198 - loss 1.6560230255126953 - moving ave loss 1.5640812882892237\n",
            "step 3199 - loss 1.5973540544509888 - moving ave loss 1.5674085649054001\n",
            "step 3200 - loss 1.293654441833496 - moving ave loss 1.5400331525982098\n",
            "step 3201 - loss 1.503882646560669 - moving ave loss 1.5364181019944558\n",
            "step 3202 - loss 1.4364275932312012 - moving ave loss 1.5264190511181306\n",
            "step 3203 - loss 1.4463584423065186 - moving ave loss 1.5184129902369694\n",
            "step 3204 - loss 1.4962958097457886 - moving ave loss 1.5162012721878515\n",
            "step 3205 - loss 1.4725366830825806 - moving ave loss 1.5118348132773245\n",
            "step 3206 - loss 1.6735000610351562 - moving ave loss 1.5280013380531077\n",
            "step 3207 - loss 1.2664363384246826 - moving ave loss 1.5018448380902654\n",
            "step 3208 - loss 1.438556432723999 - moving ave loss 1.4955159975536387\n",
            "step 3209 - loss 1.1242363452911377 - moving ave loss 1.4583880323273888\n",
            "step 3210 - loss 1.4652099609375 - moving ave loss 1.4590702251883998\n",
            "step 3211 - loss 1.4062306880950928 - moving ave loss 1.4537862714790692\n",
            "step 3212 - loss 1.7632901668548584 - moving ave loss 1.4847366610166481\n",
            "step 3213 - loss 1.498185396194458 - moving ave loss 1.4860815345344291\n",
            "step 3214 - loss 1.7156659364700317 - moving ave loss 1.5090399747279895\n",
            "step 3215 - loss 1.603471040725708 - moving ave loss 1.5184830813277614\n",
            "step 3216 - loss 1.667091727256775 - moving ave loss 1.5333439459206628\n",
            "step 3217 - loss 1.3848028182983398 - moving ave loss 1.5184898331584307\n",
            "step 3218 - loss 1.4659605026245117 - moving ave loss 1.513236900105039\n",
            "step 3219 - loss 1.3794033527374268 - moving ave loss 1.4998535453682778\n",
            "step 3220 - loss 1.3364858627319336 - moving ave loss 1.4835167771046434\n",
            "Finish 140 epoch(es)\n",
            "step 3221 - loss 1.52497398853302 - moving ave loss 1.487662498247481\n",
            "step 3222 - loss 1.3806631565093994 - moving ave loss 1.4769625640736728\n",
            "step 3223 - loss 1.606863260269165 - moving ave loss 1.489952633693222\n",
            "step 3224 - loss 1.3600958585739136 - moving ave loss 1.4769669561812913\n",
            "step 3225 - loss 1.3419877290725708 - moving ave loss 1.4634690334704192\n",
            "step 3226 - loss 1.4890940189361572 - moving ave loss 1.466031532016993\n",
            "step 3227 - loss 1.6238751411437988 - moving ave loss 1.4818158929296734\n",
            "step 3228 - loss 1.3148956298828125 - moving ave loss 1.4651238666249875\n",
            "step 3229 - loss 1.4887418746948242 - moving ave loss 1.4674856674319712\n",
            "step 3230 - loss 1.8130531311035156 - moving ave loss 1.5020424137991255\n",
            "step 3231 - loss 1.5019794702529907 - moving ave loss 1.502036119444512\n",
            "step 3232 - loss 1.6014949083328247 - moving ave loss 1.5119819983333433\n",
            "step 3233 - loss 1.3433339595794678 - moving ave loss 1.4951171944579558\n",
            "step 3234 - loss 1.7355412244796753 - moving ave loss 1.5191595974601277\n",
            "step 3235 - loss 1.3919037580490112 - moving ave loss 1.5064340135190162\n",
            "step 3236 - loss 1.085343599319458 - moving ave loss 1.4643249720990603\n",
            "step 3237 - loss 1.5778861045837402 - moving ave loss 1.4756810853475284\n",
            "step 3238 - loss 1.7267959117889404 - moving ave loss 1.5007925679916696\n",
            "step 3239 - loss 1.5312765836715698 - moving ave loss 1.5038409695596597\n",
            "step 3240 - loss 1.4643776416778564 - moving ave loss 1.4998946367714794\n",
            "step 3241 - loss 1.2461638450622559 - moving ave loss 1.474521557600557\n",
            "step 3242 - loss 1.7997492551803589 - moving ave loss 1.5070443273585372\n",
            "step 3243 - loss 1.5194346904754639 - moving ave loss 1.50828336367023\n",
            "Finish 141 epoch(es)\n",
            "step 3244 - loss 1.5748357772827148 - moving ave loss 1.5149386050314786\n",
            "step 3245 - loss 1.315112590789795 - moving ave loss 1.4949560036073104\n",
            "step 3246 - loss 1.9294636249542236 - moving ave loss 1.5384067657420017\n",
            "step 3247 - loss 1.566333532333374 - moving ave loss 1.541199442401139\n",
            "step 3248 - loss 1.6521645784378052 - moving ave loss 1.5522959560048057\n",
            "step 3249 - loss 1.30904221534729 - moving ave loss 1.527970581939054\n",
            "step 3250 - loss 1.4032373428344727 - moving ave loss 1.5154972580285961\n",
            "Checkpoint at step 3250\n",
            "step 3251 - loss 1.6021815538406372 - moving ave loss 1.5241656876098004\n",
            "step 3252 - loss 1.215618371963501 - moving ave loss 1.4933109560451705\n",
            "step 3253 - loss 1.5376794338226318 - moving ave loss 1.4977478038229166\n",
            "step 3254 - loss 1.1998565196990967 - moving ave loss 1.4679586754105347\n",
            "step 3255 - loss 1.2136186361312866 - moving ave loss 1.4425246714826099\n",
            "step 3256 - loss 1.8431589603424072 - moving ave loss 1.4825881003685897\n",
            "step 3257 - loss 1.4666481018066406 - moving ave loss 1.4809941005123948\n",
            "step 3258 - loss 1.3948304653167725 - moving ave loss 1.4723777369928326\n",
            "step 3259 - loss 1.3817464113235474 - moving ave loss 1.4633146044259042\n",
            "step 3260 - loss 1.6401509046554565 - moving ave loss 1.4809982344488595\n",
            "step 3261 - loss 1.8399072885513306 - moving ave loss 1.5168891398591067\n",
            "step 3262 - loss 1.7276380062103271 - moving ave loss 1.537964026494229\n",
            "step 3263 - loss 1.5457831621170044 - moving ave loss 1.5387459400565064\n",
            "step 3264 - loss 1.413346290588379 - moving ave loss 1.526205975109694\n",
            "step 3265 - loss 1.5410048961639404 - moving ave loss 1.5276858672151186\n",
            "step 3266 - loss 1.1843078136444092 - moving ave loss 1.4933480618580477\n",
            "Finish 142 epoch(es)\n",
            "step 3267 - loss 1.4156105518341064 - moving ave loss 1.4855743108556536\n",
            "step 3268 - loss 1.4917572736740112 - moving ave loss 1.4861926071374894\n",
            "step 3269 - loss 1.2807170152664185 - moving ave loss 1.4656450479503824\n",
            "step 3270 - loss 1.512923240661621 - moving ave loss 1.4703728672215064\n",
            "step 3271 - loss 1.097120761871338 - moving ave loss 1.4330476566864896\n",
            "step 3272 - loss 1.2293386459350586 - moving ave loss 1.4126767556113464\n",
            "step 3273 - loss 1.9726946353912354 - moving ave loss 1.4686785435893355\n",
            "step 3274 - loss 1.6015560626983643 - moving ave loss 1.4819662955002384\n",
            "step 3275 - loss 1.2642351388931274 - moving ave loss 1.4601931798395273\n",
            "step 3276 - loss 1.2444448471069336 - moving ave loss 1.438618346566268\n",
            "step 3277 - loss 1.4478085041046143 - moving ave loss 1.4395373623201029\n",
            "step 3278 - loss 1.6293834447860718 - moving ave loss 1.4585219705666999\n",
            "step 3279 - loss 1.348525047302246 - moving ave loss 1.4475222782402546\n",
            "step 3280 - loss 1.3158433437347412 - moving ave loss 1.4343543847897033\n",
            "step 3281 - loss 1.7121529579162598 - moving ave loss 1.462134242102359\n",
            "step 3282 - loss 1.9644662141799927 - moving ave loss 1.5123674393101223\n",
            "step 3283 - loss 1.1552454233169556 - moving ave loss 1.4766552377108055\n",
            "step 3284 - loss 1.4761693477630615 - moving ave loss 1.4766066487160312\n",
            "step 3285 - loss 1.5099812746047974 - moving ave loss 1.4799441113049079\n",
            "step 3286 - loss 1.6001979112625122 - moving ave loss 1.4919694913006685\n",
            "step 3287 - loss 1.3483598232269287 - moving ave loss 1.4776085244932944\n",
            "step 3288 - loss 1.070785641670227 - moving ave loss 1.4369262362109876\n",
            "step 3289 - loss 1.7629969120025635 - moving ave loss 1.4695333037901452\n",
            "Finish 143 epoch(es)\n",
            "step 3290 - loss 1.2506049871444702 - moving ave loss 1.4476404721255778\n",
            "step 3291 - loss 1.407653570175171 - moving ave loss 1.4436417819305372\n",
            "step 3292 - loss 1.723846197128296 - moving ave loss 1.4716622234503132\n",
            "step 3293 - loss 1.7686164379119873 - moving ave loss 1.5013576448964807\n",
            "step 3294 - loss 1.6427290439605713 - moving ave loss 1.51549478480289\n",
            "step 3295 - loss 1.390820026397705 - moving ave loss 1.5030273089623716\n",
            "step 3296 - loss 1.421081304550171 - moving ave loss 1.4948327085211517\n",
            "step 3297 - loss 1.7476568222045898 - moving ave loss 1.5201151198894955\n",
            "step 3298 - loss 1.849892020225525 - moving ave loss 1.5530928099230985\n",
            "step 3299 - loss 1.43223237991333 - moving ave loss 1.5410067669221217\n",
            "step 3300 - loss 1.4330763816833496 - moving ave loss 1.5302137283982444\n",
            "step 3301 - loss 1.2799391746520996 - moving ave loss 1.50518627302363\n",
            "step 3302 - loss 1.1738948822021484 - moving ave loss 1.472057133941482\n",
            "step 3303 - loss 1.6788537502288818 - moving ave loss 1.492736795570222\n",
            "step 3304 - loss 1.4246222972869873 - moving ave loss 1.4859253457418986\n",
            "step 3305 - loss 1.3577580451965332 - moving ave loss 1.473108615687362\n",
            "step 3306 - loss 1.632909893989563 - moving ave loss 1.4890887435175821\n",
            "step 3307 - loss 1.3153313398361206 - moving ave loss 1.471713003149436\n",
            "step 3308 - loss 1.6822327375411987 - moving ave loss 1.4927649765886124\n",
            "step 3309 - loss 1.5587739944458008 - moving ave loss 1.4993658783743313\n",
            "step 3310 - loss 1.4343522787094116 - moving ave loss 1.4928645184078395\n",
            "step 3311 - loss 1.6636366844177246 - moving ave loss 1.509941735008828\n",
            "step 3312 - loss 1.40993332862854 - moving ave loss 1.4999408943707992\n",
            "Finish 144 epoch(es)\n",
            "step 3313 - loss 1.9867794513702393 - moving ave loss 1.5486247500707433\n",
            "step 3314 - loss 1.6223340034484863 - moving ave loss 1.5559956754085176\n",
            "step 3315 - loss 1.6008967161178589 - moving ave loss 1.5604857794794518\n",
            "step 3316 - loss 1.9867956638336182 - moving ave loss 1.6031167679148683\n",
            "step 3317 - loss 1.3537781238555908 - moving ave loss 1.5781829035089407\n",
            "step 3318 - loss 1.5039242506027222 - moving ave loss 1.570757038218319\n",
            "step 3319 - loss 1.200697660446167 - moving ave loss 1.5337511004411037\n",
            "step 3320 - loss 1.6052162647247314 - moving ave loss 1.5408976168694666\n",
            "step 3321 - loss 1.4508147239685059 - moving ave loss 1.5318893275793706\n",
            "step 3322 - loss 1.0068707466125488 - moving ave loss 1.4793874694826885\n",
            "step 3323 - loss 1.4668514728546143 - moving ave loss 1.478133869819881\n",
            "step 3324 - loss 1.59540593624115 - moving ave loss 1.489861076462008\n",
            "step 3325 - loss 1.56163489818573 - moving ave loss 1.4970384586343803\n",
            "step 3326 - loss 1.5563983917236328 - moving ave loss 1.5029744519433055\n",
            "step 3327 - loss 1.5918992757797241 - moving ave loss 1.5118669343269475\n",
            "step 3328 - loss 1.500631332397461 - moving ave loss 1.510743374133999\n",
            "step 3329 - loss 1.3601725101470947 - moving ave loss 1.4956862877353088\n",
            "step 3330 - loss 1.6972275972366333 - moving ave loss 1.5158404186854413\n",
            "step 3331 - loss 1.2608603239059448 - moving ave loss 1.4903424092074917\n",
            "step 3332 - loss 1.5102280378341675 - moving ave loss 1.4923309720701592\n",
            "step 3333 - loss 1.6420865058898926 - moving ave loss 1.5073065254521325\n",
            "step 3334 - loss 1.3144104480743408 - moving ave loss 1.4880169177143534\n",
            "step 3335 - loss 1.8449779748916626 - moving ave loss 1.5237130234320846\n",
            "Finish 145 epoch(es)\n",
            "step 3336 - loss 1.2737538814544678 - moving ave loss 1.4987171092343228\n",
            "step 3337 - loss 1.5804672241210938 - moving ave loss 1.506892120723\n",
            "step 3338 - loss 1.5869430303573608 - moving ave loss 1.514897211686436\n",
            "step 3339 - loss 1.4516472816467285 - moving ave loss 1.5085722186824653\n",
            "step 3340 - loss 1.2315590381622314 - moving ave loss 1.480870900630442\n",
            "step 3341 - loss 1.5544930696487427 - moving ave loss 1.488233117532272\n",
            "step 3342 - loss 1.6227796077728271 - moving ave loss 1.5016877665563277\n",
            "step 3343 - loss 1.3641066551208496 - moving ave loss 1.4879296554127799\n",
            "step 3344 - loss 1.654471516609192 - moving ave loss 1.5045838415324213\n",
            "step 3345 - loss 1.4492706060409546 - moving ave loss 1.4990525179832748\n",
            "step 3346 - loss 1.1764793395996094 - moving ave loss 1.4667952001449083\n",
            "step 3347 - loss 1.1963202953338623 - moving ave loss 1.4397477096638036\n",
            "step 3348 - loss 2.057053565979004 - moving ave loss 1.5014782952953236\n",
            "step 3349 - loss 1.430781602859497 - moving ave loss 1.4944086260517409\n",
            "step 3350 - loss 1.9008514881134033 - moving ave loss 1.5350529122579073\n",
            "step 3351 - loss 1.6207275390625 - moving ave loss 1.5436203749383668\n",
            "step 3352 - loss 1.1691100597381592 - moving ave loss 1.506169343418346\n",
            "step 3353 - loss 1.6321160793304443 - moving ave loss 1.518764017009556\n",
            "step 3354 - loss 1.62796950340271 - moving ave loss 1.5296845656488716\n",
            "step 3355 - loss 1.6736066341400146 - moving ave loss 1.5440767724979858\n",
            "step 3356 - loss 1.7794615030288696 - moving ave loss 1.5676152455510741\n",
            "step 3357 - loss 1.8624721765518188 - moving ave loss 1.5971009386511488\n",
            "step 3358 - loss 1.7629951238632202 - moving ave loss 1.613690357172356\n",
            "Finish 146 epoch(es)\n",
            "step 3359 - loss 1.5085492134094238 - moving ave loss 1.603176242796063\n",
            "step 3360 - loss 1.5014674663543701 - moving ave loss 1.5930053651518938\n",
            "step 3361 - loss 1.4695643186569214 - moving ave loss 1.5806612605023966\n",
            "step 3362 - loss 1.7662137746810913 - moving ave loss 1.5992165119202661\n",
            "step 3363 - loss 1.4820877313613892 - moving ave loss 1.5875036338643786\n",
            "step 3364 - loss 1.5596933364868164 - moving ave loss 1.5847226041266225\n",
            "step 3365 - loss 1.0152028799057007 - moving ave loss 1.5277706317045303\n",
            "step 3366 - loss 1.615987777709961 - moving ave loss 1.5365923463050735\n",
            "step 3367 - loss 1.344361662864685 - moving ave loss 1.5173692779610348\n",
            "step 3368 - loss 1.393888235092163 - moving ave loss 1.5050211736741477\n",
            "step 3369 - loss 1.428471326828003 - moving ave loss 1.497366188989533\n",
            "step 3370 - loss 1.356086254119873 - moving ave loss 1.483238195502567\n",
            "step 3371 - loss 1.532627820968628 - moving ave loss 1.4881771580491732\n",
            "step 3372 - loss 1.4450172185897827 - moving ave loss 1.4838611641032342\n",
            "step 3373 - loss 1.181436538696289 - moving ave loss 1.45361870156254\n",
            "step 3374 - loss 1.6298942565917969 - moving ave loss 1.4712462570654656\n",
            "step 3375 - loss 1.7598090171813965 - moving ave loss 1.5001025330770588\n",
            "Checkpoint at step 3375\n",
            "step 3376 - loss 1.6372432708740234 - moving ave loss 1.5138166068567553\n",
            "step 3377 - loss 1.5254032611846924 - moving ave loss 1.514975272289549\n",
            "step 3378 - loss 1.2734599113464355 - moving ave loss 1.4908237361952377\n",
            "step 3379 - loss 1.8035802841186523 - moving ave loss 1.5220993909875793\n",
            "step 3380 - loss 1.430239200592041 - moving ave loss 1.5129133719480254\n",
            "step 3381 - loss 1.6147546768188477 - moving ave loss 1.5230975024351077\n",
            "Finish 147 epoch(es)\n",
            "step 3382 - loss 1.1983683109283447 - moving ave loss 1.4906245832844314\n",
            "step 3383 - loss 1.0544002056121826 - moving ave loss 1.4470021455172066\n",
            "step 3384 - loss 1.6037909984588623 - moving ave loss 1.4626810308113722\n",
            "step 3385 - loss 1.8106770515441895 - moving ave loss 1.497480632884654\n",
            "step 3386 - loss 1.1474467515945435 - moving ave loss 1.462477244755643\n",
            "step 3387 - loss 1.3053381443023682 - moving ave loss 1.4467633347103155\n",
            "step 3388 - loss 1.4950839281082153 - moving ave loss 1.4515953940501056\n",
            "step 3389 - loss 1.364461898803711 - moving ave loss 1.4428820445254662\n",
            "step 3390 - loss 1.855289101600647 - moving ave loss 1.4841227502329843\n",
            "step 3391 - loss 1.6288968324661255 - moving ave loss 1.4986001584562985\n",
            "step 3392 - loss 1.5520411729812622 - moving ave loss 1.5039442599087949\n",
            "step 3393 - loss 1.6621277332305908 - moving ave loss 1.5197626072409747\n",
            "step 3394 - loss 1.226959466934204 - moving ave loss 1.4904822932102975\n",
            "step 3395 - loss 1.513272762298584 - moving ave loss 1.492761340119126\n",
            "step 3396 - loss 1.3750169277191162 - moving ave loss 1.480986898879125\n",
            "step 3397 - loss 1.8055795431137085 - moving ave loss 1.5134461633025835\n",
            "step 3398 - loss 1.527804970741272 - moving ave loss 1.5148820440464523\n",
            "step 3399 - loss 1.2792658805847168 - moving ave loss 1.4913204277002787\n",
            "step 3400 - loss 1.3909832239151 - moving ave loss 1.481286707321761\n",
            "step 3401 - loss 1.3795897960662842 - moving ave loss 1.4711170161962135\n",
            "step 3402 - loss 1.5002919435501099 - moving ave loss 1.4740345089316031\n",
            "step 3403 - loss 1.2699236869812012 - moving ave loss 1.4536234267365629\n",
            "step 3404 - loss 1.7594897747039795 - moving ave loss 1.4842100615333045\n",
            "Finish 148 epoch(es)\n",
            "step 3405 - loss 1.1094316244125366 - moving ave loss 1.4467322178212276\n",
            "step 3406 - loss 1.4437133073806763 - moving ave loss 1.4464303267771725\n",
            "step 3407 - loss 1.6732484102249146 - moving ave loss 1.4691121351219467\n",
            "step 3408 - loss 1.7108044624328613 - moving ave loss 1.4932813678530383\n",
            "step 3409 - loss 1.8557548522949219 - moving ave loss 1.5295287162972266\n",
            "step 3410 - loss 1.5996936559677124 - moving ave loss 1.5365452102642754\n",
            "step 3411 - loss 1.4348654747009277 - moving ave loss 1.5263772367079407\n",
            "step 3412 - loss 1.5122246742248535 - moving ave loss 1.524961980459632\n",
            "step 3413 - loss 1.6449693441390991 - moving ave loss 1.5369627168275788\n",
            "step 3414 - loss 1.3692164421081543 - moving ave loss 1.5201880893556363\n",
            "step 3415 - loss 1.225792646408081 - moving ave loss 1.4907485450608808\n",
            "step 3416 - loss 1.253342628479004 - moving ave loss 1.4670079534026932\n",
            "step 3417 - loss 1.2892985343933105 - moving ave loss 1.449237011501755\n",
            "step 3418 - loss 1.4901446104049683 - moving ave loss 1.4533277713920765\n",
            "step 3419 - loss 1.5868070125579834 - moving ave loss 1.466675695508667\n",
            "step 3420 - loss 1.808852195739746 - moving ave loss 1.5008933455317752\n",
            "step 3421 - loss 1.517469882965088 - moving ave loss 1.5025509992751065\n",
            "step 3422 - loss 1.74481999874115 - moving ave loss 1.526777899221711\n",
            "step 3423 - loss 1.3460001945495605 - moving ave loss 1.508700128754496\n",
            "step 3424 - loss 1.626407265663147 - moving ave loss 1.5204708424453612\n",
            "step 3425 - loss 1.2772736549377441 - moving ave loss 1.4961511236945997\n",
            "step 3426 - loss 1.2723686695098877 - moving ave loss 1.4737728782761284\n",
            "step 3427 - loss 1.646864891052246 - moving ave loss 1.4910820795537403\n",
            "Finish 149 epoch(es)\n",
            "step 3428 - loss 1.5456814765930176 - moving ave loss 1.496542019257668\n",
            "step 3429 - loss 1.5500028133392334 - moving ave loss 1.5018880986658245\n",
            "step 3430 - loss 1.5851995944976807 - moving ave loss 1.5102192482490102\n",
            "step 3431 - loss 1.5727789402008057 - moving ave loss 1.5164752174441898\n",
            "step 3432 - loss 1.7134087085723877 - moving ave loss 1.5361685665570097\n",
            "step 3433 - loss 1.5418086051940918 - moving ave loss 1.536732570420718\n",
            "step 3434 - loss 1.2290364503860474 - moving ave loss 1.505962958417251\n",
            "step 3435 - loss 1.4423563480377197 - moving ave loss 1.4996022973792977\n",
            "step 3436 - loss 1.9288932085037231 - moving ave loss 1.5425313884917404\n",
            "step 3437 - loss 1.6149835586547852 - moving ave loss 1.549776605508045\n",
            "step 3438 - loss 1.0815401077270508 - moving ave loss 1.5029529557299457\n",
            "step 3439 - loss 1.4835140705108643 - moving ave loss 1.5010090672080376\n",
            "step 3440 - loss 1.8266078233718872 - moving ave loss 1.5335689428244228\n",
            "step 3441 - loss 1.2923741340637207 - moving ave loss 1.5094494619483527\n",
            "step 3442 - loss 1.4437015056610107 - moving ave loss 1.5028746663196184\n",
            "step 3443 - loss 1.5421199798583984 - moving ave loss 1.5067991976734965\n",
            "step 3444 - loss 1.3468044996261597 - moving ave loss 1.4907997278687628\n",
            "step 3445 - loss 1.539475917816162 - moving ave loss 1.4956673468635027\n",
            "step 3446 - loss 1.5750296115875244 - moving ave loss 1.5036035733359048\n",
            "step 3447 - loss 1.6750285625457764 - moving ave loss 1.520746072256892\n",
            "step 3448 - loss 1.5401597023010254 - moving ave loss 1.5226874352613053\n",
            "step 3449 - loss 1.6853021383285522 - moving ave loss 1.53894890556803\n",
            "step 3450 - loss 1.5520541667938232 - moving ave loss 1.5402594316906095\n",
            "Finish 150 epoch(es)\n",
            "Checkpoint at step 3450\n",
            "Training finished, exit.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51jy4vB1zqRz",
        "colab_type": "text"
      },
      "source": [
        "**Import libraries to load the model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H9CWlFXzvQpM",
        "colab_type": "code",
        "outputId": "ecb2f512-fd83-46b4-96ee-daea0e10413c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        }
      },
      "source": [
        "import cv2\n",
        "from darkflow.net.build import TFNet\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0621 07:39:16.045018 139638569105280 deprecation_wrapper.py:119] From /content/darkflow-master/darkflow/net/build.py:15: The name tf.train.RMSPropOptimizer is deprecated. Please use tf.compat.v1.train.RMSPropOptimizer instead.\n",
            "\n",
            "W0621 07:39:16.046646 139638569105280 deprecation_wrapper.py:119] From /content/darkflow-master/darkflow/net/build.py:16: The name tf.train.AdadeltaOptimizer is deprecated. Please use tf.compat.v1.train.AdadeltaOptimizer instead.\n",
            "\n",
            "W0621 07:39:16.047628 139638569105280 deprecation_wrapper.py:119] From /content/darkflow-master/darkflow/net/build.py:17: The name tf.train.AdagradOptimizer is deprecated. Please use tf.compat.v1.train.AdagradOptimizer instead.\n",
            "\n",
            "W0621 07:39:16.052647 139638569105280 deprecation_wrapper.py:119] From /content/darkflow-master/darkflow/net/build.py:18: The name tf.train.AdagradDAOptimizer is deprecated. Please use tf.compat.v1.train.AdagradDAOptimizer instead.\n",
            "\n",
            "W0621 07:39:16.055907 139638569105280 deprecation_wrapper.py:119] From /content/darkflow-master/darkflow/net/build.py:19: The name tf.train.MomentumOptimizer is deprecated. Please use tf.compat.v1.train.MomentumOptimizer instead.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qRIEVkL3-kt6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#To show this as svg images\n",
        "%config InlineBackend.figure_format = 'svg'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wsObrTvuzvQp",
        "colab_type": "text"
      },
      "source": [
        "**Load the model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aa4s21Cy_byb",
        "colab_type": "code",
        "outputId": "42295ba0-717f-448c-818a-b860650c0237",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 784
        }
      },
      "source": [
        "options = {\n",
        "    'model': '/content/darkflow-master/cfg/tiny-yolo-voc1.cfg',\n",
        "    'load': 3450,                             # 3450 is the step number. Can be found in the ckpt folder\n",
        "    'threshold': 0.05,                       # this number can be higher if the performance is better\n",
        "    'gpu': 1.0                               # Dont use this if you have no gpu\n",
        "}\n",
        "\n",
        "tfnet = TFNet(options)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0621 07:46:54.555423 139638569105280 deprecation_wrapper.py:119] From /content/darkflow-master/darkflow/net/build.py:105: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0621 07:46:54.567155 139638569105280 deprecation_wrapper.py:119] From /content/darkflow-master/darkflow/net/ops/baseop.py:70: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "W0621 07:46:54.568251 139638569105280 deprecation_wrapper.py:119] From /content/darkflow-master/darkflow/net/ops/baseop.py:71: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
            "\n",
            "W0621 07:46:54.578965 139638569105280 deprecation_wrapper.py:119] From /content/darkflow-master/darkflow/net/ops/baseop.py:84: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "W0621 07:46:54.652727 139638569105280 deprecation_wrapper.py:119] From /content/darkflow-master/darkflow/net/ops/simple.py:106: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Parsing /content/darkflow-master/cfg/tiny-yolo-voc1.cfg\n",
            "Loading None ...\n",
            "Finished in 0.00010991096496582031s\n",
            "\n",
            "Building net ...\n",
            "Source | Train? | Layer description                | Output size\n",
            "-------+--------+----------------------------------+---------------\n",
            "       |        | input                            | (?, 416, 416, 3)\n",
            " Init  |  Yep!  | conv 3x3p1_1  +bnorm  leaky      | (?, 416, 416, 16)\n",
            " Load  |  Yep!  | maxp 2x2p0_2                     | (?, 208, 208, 16)\n",
            " Init  |  Yep!  | conv 3x3p1_1  +bnorm  leaky      | (?, 208, 208, 32)\n",
            " Load  |  Yep!  | maxp 2x2p0_2                     | (?, 104, 104, 32)\n",
            " Init  |  Yep!  | conv 3x3p1_1  +bnorm  leaky      | (?, 104, 104, 64)\n",
            " Load  |  Yep!  | maxp 2x2p0_2                     | (?, 52, 52, 64)\n",
            " Init  |  Yep!  | conv 3x3p1_1  +bnorm  leaky      | (?, 52, 52, 128)\n",
            " Load  |  Yep!  | maxp 2x2p0_2                     | (?, 26, 26, 128)\n",
            " Init  |  Yep!  | conv 3x3p1_1  +bnorm  leaky      | (?, 26, 26, 256)\n",
            " Load  |  Yep!  | maxp 2x2p0_2                     | (?, 13, 13, 256)\n",
            " Init  |  Yep!  | conv 3x3p1_1  +bnorm  leaky      | (?, 13, 13, 512)\n",
            " Load  |  Yep!  | maxp 2x2p0_1                     | (?, 13, 13, 512)\n",
            " Init  |  Yep!  | conv 3x3p1_1  +bnorm  leaky      | (?, 13, 13, 1024)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0621 07:46:56.404785 139638569105280 deprecation_wrapper.py:119] From /content/darkflow-master/darkflow/net/build.py:132: The name tf.GPUOptions is deprecated. Please use tf.compat.v1.GPUOptions instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " Init  |  Yep!  | conv 3x3p1_1  +bnorm  leaky      | (?, 13, 13, 1024)\n",
            " Init  |  Yep!  | conv 1x1p0_1    linear           | (?, 13, 13, 40)\n",
            "-------+--------+----------------------------------+---------------\n",
            "GPU mode with 1.0 usage\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0621 07:46:59.918082 139638569105280 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loading from ./ckpt/tiny-yolo-voc1-3450\n",
            "Finished in 5.818031072616577s\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZtzB5Gcz4zX",
        "colab_type": "text"
      },
      "source": [
        "**Predict the image**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7UzJrGf_AR7B",
        "colab_type": "code",
        "outputId": "afa3c430-aaee-43cb-b93d-b243d4bcd5a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "source": [
        "# read the color image and covert to RGB\n",
        "\n",
        "img = cv2.imread('/content/darkflow-master/test_images/ankle_boot2.jpg', cv2.IMREAD_COLOR)\n",
        "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "img.shape\n",
        "\n",
        "# use YOLO to predict the image\n",
        "result = tfnet.return_predict(img)\n",
        "result\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'bottomright': {'x': 200, 'y': 247},\n",
              "  'confidence': 0.104880616,\n",
              "  'label': 'Ankle boots',\n",
              "  'topleft': {'x': 25, 'y': 38}}]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3uhacC-k0DNT",
        "colab_type": "text"
      },
      "source": [
        "**Display the predicted output**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u1Z9qRnmA-h2",
        "colab_type": "code",
        "outputId": "7b9e72e2-6781-4060-e085-67a026382471",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "# pull out some info from the results\n",
        "\n",
        "tl = (result[0]['topleft']['x'], result[0]['topleft']['y'])\n",
        "br = (result[0]['bottomright']['x'], result[0]['bottomright']['y'])\n",
        "label = result[0]['label']\n",
        "\n",
        "\n",
        "# add the box and label and display it\n",
        "img = cv2.rectangle(img, tl, br, (255, 0, 0), 3) # draw a ractangle onto an image\n",
        "img = cv2.putText(img, label, tl, cv2.FONT_HERSHEY_COMPLEX   , 1, (0, 0, 0), 1) # add laebl name\n",
        "plt.imshow(img)\n",
        "plt.show()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"252.018125pt\" version=\"1.1\" viewBox=\"0 0 223.723362 252.018125\" width=\"223.723362pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <defs>\n  <style type=\"text/css\">\n*{stroke-linecap:butt;stroke-linejoin:round;}\n  </style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 252.018125 \nL 223.723362 252.018125 \nL 223.723362 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 33.2875 228.14 \nL 207.412759 228.14 \nL 207.412759 10.7 \nL 33.2875 10.7 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g clip-path=\"url(#pf1ec96a0a4)\">\n    <image height=\"218\" id=\"image2346c9ca3a\" transform=\"scale(1 -1)translate(0 -218)\" width=\"175\" x=\"33.2875\" xlink:href=\"data:image/png;base64,\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\" y=\"-10.14\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"m233cc6d8f9\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.720647\" xlink:href=\"#m233cc6d8f9\" y=\"228.14\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <defs>\n       <path d=\"M 31.78125 66.40625 \nQ 24.171875 66.40625 20.328125 58.90625 \nQ 16.5 51.421875 16.5 36.375 \nQ 16.5 21.390625 20.328125 13.890625 \nQ 24.171875 6.390625 31.78125 6.390625 \nQ 39.453125 6.390625 43.28125 13.890625 \nQ 47.125 21.390625 47.125 36.375 \nQ 47.125 51.421875 43.28125 58.90625 \nQ 39.453125 66.40625 31.78125 66.40625 \nz\nM 31.78125 74.21875 \nQ 44.046875 74.21875 50.515625 64.515625 \nQ 56.984375 54.828125 56.984375 36.375 \nQ 56.984375 17.96875 50.515625 8.265625 \nQ 44.046875 -1.421875 31.78125 -1.421875 \nQ 19.53125 -1.421875 13.0625 8.265625 \nQ 6.59375 17.96875 6.59375 36.375 \nQ 6.59375 54.828125 13.0625 64.515625 \nQ 19.53125 74.21875 31.78125 74.21875 \nz\n\" id=\"DejaVuSans-48\"/>\n      </defs>\n      <g transform=\"translate(30.539397 242.738437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"77.035388\" xlink:href=\"#m233cc6d8f9\" y=\"228.14\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 50 -->\n      <defs>\n       <path d=\"M 10.796875 72.90625 \nL 49.515625 72.90625 \nL 49.515625 64.59375 \nL 19.828125 64.59375 \nL 19.828125 46.734375 \nQ 21.96875 47.46875 24.109375 47.828125 \nQ 26.265625 48.1875 28.421875 48.1875 \nQ 40.625 48.1875 47.75 41.5 \nQ 54.890625 34.8125 54.890625 23.390625 \nQ 54.890625 11.625 47.5625 5.09375 \nQ 40.234375 -1.421875 26.90625 -1.421875 \nQ 22.3125 -1.421875 17.546875 -0.640625 \nQ 12.796875 0.140625 7.71875 1.703125 \nL 7.71875 11.625 \nQ 12.109375 9.234375 16.796875 8.0625 \nQ 21.484375 6.890625 26.703125 6.890625 \nQ 35.15625 6.890625 40.078125 11.328125 \nQ 45.015625 15.765625 45.015625 23.390625 \nQ 45.015625 31 40.078125 35.4375 \nQ 35.15625 39.890625 26.703125 39.890625 \nQ 22.75 39.890625 18.8125 39.015625 \nQ 14.890625 38.140625 10.796875 36.28125 \nz\n\" id=\"DejaVuSans-53\"/>\n      </defs>\n      <g transform=\"translate(70.672888 242.738437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"120.350129\" xlink:href=\"#m233cc6d8f9\" y=\"228.14\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 100 -->\n      <defs>\n       <path d=\"M 12.40625 8.296875 \nL 28.515625 8.296875 \nL 28.515625 63.921875 \nL 10.984375 60.40625 \nL 10.984375 69.390625 \nL 28.421875 72.90625 \nL 38.28125 72.90625 \nL 38.28125 8.296875 \nL 54.390625 8.296875 \nL 54.390625 0 \nL 12.40625 0 \nz\n\" id=\"DejaVuSans-49\"/>\n      </defs>\n      <g transform=\"translate(110.806379 242.738437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"163.664871\" xlink:href=\"#m233cc6d8f9\" y=\"228.14\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 150 -->\n      <g transform=\"translate(154.121121 242.738437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"206.979612\" xlink:href=\"#m233cc6d8f9\" y=\"228.14\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 200 -->\n      <defs>\n       <path d=\"M 19.1875 8.296875 \nL 53.609375 8.296875 \nL 53.609375 0 \nL 7.328125 0 \nL 7.328125 8.296875 \nQ 12.9375 14.109375 22.625 23.890625 \nQ 32.328125 33.6875 34.8125 36.53125 \nQ 39.546875 41.84375 41.421875 45.53125 \nQ 43.3125 49.21875 43.3125 52.78125 \nQ 43.3125 58.59375 39.234375 62.25 \nQ 35.15625 65.921875 28.609375 65.921875 \nQ 23.96875 65.921875 18.8125 64.3125 \nQ 13.671875 62.703125 7.8125 59.421875 \nL 7.8125 69.390625 \nQ 13.765625 71.78125 18.9375 73 \nQ 24.125 74.21875 28.421875 74.21875 \nQ 39.75 74.21875 46.484375 68.546875 \nQ 53.21875 62.890625 53.21875 53.421875 \nQ 53.21875 48.921875 51.53125 44.890625 \nQ 49.859375 40.875 45.40625 35.40625 \nQ 44.1875 33.984375 37.640625 27.21875 \nQ 31.109375 20.453125 19.1875 8.296875 \nz\n\" id=\"DejaVuSans-50\"/>\n      </defs>\n      <g transform=\"translate(197.435862 242.738437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_6\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"mfe0606d7b9\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#mfe0606d7b9\" y=\"11.133147\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 0 -->\n      <g transform=\"translate(19.925 14.932366)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_7\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#mfe0606d7b9\" y=\"54.447888\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 50 -->\n      <g transform=\"translate(13.5625 58.247107)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#mfe0606d7b9\" y=\"97.762629\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 100 -->\n      <g transform=\"translate(7.2 101.561848)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_9\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#mfe0606d7b9\" y=\"141.077371\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 150 -->\n      <g transform=\"translate(7.2 144.876589)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#mfe0606d7b9\" y=\"184.392112\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 200 -->\n      <g transform=\"translate(7.2 188.19133)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_11\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#mfe0606d7b9\" y=\"227.706853\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 250 -->\n      <g transform=\"translate(7.2 231.506071)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 33.2875 228.14 \nL 33.2875 10.7 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 207.412759 228.14 \nL 207.412759 10.7 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 33.2875 228.14 \nL 207.412759 228.14 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 33.2875 10.7 \nL 207.412759 10.7 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"pf1ec96a0a4\">\n   <rect height=\"217.44\" width=\"174.125259\" x=\"33.2875\" y=\"10.7\"/>\n  </clipPath>\n </defs>\n</svg>\n"
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D1Sk76yFc8ZJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#This command can be used for testing but it didn't work for me\n",
        "#!flow --imgdir content/darkflow-master/test_images/ankle_boot1.jpg --model cfg/tiny-yolo-voc1.cfg --load 3450"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4IFv4MWGY_Kp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}